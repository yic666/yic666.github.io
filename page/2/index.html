<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Yic">
<meta property="og:url" content="http://example.com/page/2/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yic-gdut">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Yic</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/" class="post-title-link" itemprop="url">视频理解相关论文</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-09-12 22:20:46" itemprop="dateCreated datePublished" datetime="2022-09-12T22:20:46+08:00">2022-09-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-13 12:26:18" itemprop="dateModified" datetime="2023-03-13T12:26:18+08:00">2023-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇博文参考李沐读论文系列中视频理解的相关内容 # 双流网络</p>
<h2 id="前言">前言</h2>
<p>视频本身是一个很好的数据来源，比2D的单个图像包含更多信息，比如物体移动的信息，以及长期的时序信息，包括音频信息。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913100303892.png"
alt="image-20220913100303892" />
<figcaption aria-hidden="true">image-20220913100303892</figcaption>
</figure>
<p>这篇论文是第一篇利用深度学习进行视频理解效果与之前的手工特征方法效果相当的方法。</p>
<p>双流网络顾名思义就是使用了两个卷积神经网络，如下图所示：</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913101016813.png"
alt="image-20220913101016813" />
<figcaption aria-hidden="true">image-20220913101016813</figcaption>
</figure>
<p>视频相当于连续的多张图片，因此早期的工作将视频的关键帧抽取出来，一个个输入卷积网络，最后以Early
Fasion或者Late
Fasion的方式合并结果，达到一种时空学习的效果，但效果不如手工设置特征。因此，双流网络的作者认为卷积网络适合学习局部特征，而不适合学习物体的移动信息（Motion-information)，于是作者使用了光流法来获取物体的移动信息，以学习光流和动作分类的映射。作者把双流网络分为空间流神经网络和时间流神经网络，空间流的输入为单帧图像，而时间流的输入为一系列光流图片。</p>
<h2 id="摘要和引言">摘要和引言</h2>
<p>在这篇论文中，作者研究了如何使用深度卷积网络实现视频的动作识别。主要问题的难点在于如何同时学习两种信息：一种是从静止的的图像上学到信息，比如物体的形状大小颜色，以及整体的场景信息；另一种是物体之间的移动信息，或者说是视频中的时序信息。这篇论文主要有三个贡献：双流网络、能取得很好效果的在少量数据上训练的光流网络、muliti-task
learning。</p>
<p>视频天生就能提供一种数据增强，因为在视频中，同一个物体会有多种形变以及遮挡和光照的改变，是一种非常自然的数据增强。</p>
<h2 id="双流网络结构">双流网络结构</h2>
<p>空间流学习空间特征，时间流学习运动特征，最后使用late
fusion合并结果。可以通过简单的加权平均，也可以训练一个多分类的线性SVM。</p>
<h3 id="空间流">空间流</h3>
<p>空间流神经网络的输入就是一帧帧的图像，其实就是个图像分类的任务。静止的信息本身就是个有用的信息，因为很多动作与识别出的物体是紧密相关的。因此单独空间流网络已经能达到很好的效果了，而且还能使用Imagenet去预训练。</p>
<h3 id="时间流">时间流</h3>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913104904429.png"
alt="image-20220913104904429" />
<figcaption aria-hidden="true">image-20220913104904429</figcaption>
</figure>
<p>（a）和（b）为视频中的前后两帧，从这两帧可以得到如（c）的光流，在数学上光流实际上就是一个在平面上的向量，因此得到分为（d）水平方向和（e）垂直方向两个方向的可视化结果。从维度上看，假如输入为<span
class="math inline">\(w*h\)</span>的RGB图像也就是输入维度是<span
class="math inline">\(w*h*3\)</span>，而得到的光流图维度则为<span
class="math inline">\(w*h*2\)</span>，2代表两个方向。</p>
<h3 id="如何叠加光流图">如何叠加光流图</h3>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913110726700.png"
alt="image-20220913110726700" />
<figcaption aria-hidden="true">image-20220913110726700</figcaption>
</figure>
<p>第一种方式就是简单直接地将得到的光流图堆叠在一起，每次叠加的都是同样位置的光流特征，这种方式不需要更多的预处理，但没有充分利用光流的信息。第二种则是根据光流的轨迹，在轨迹上进行数值的叠加。另外，作者也使用了双向光流的方法。</p>
<h3 id="实现细节">实现细节</h3>
<p>事实上，两个流网络都可以看作是Alexnet的变体都是五层卷积加两层全连接，训练部分都是标准的操作。</p>
<ol type="1">
<li><p>测试部分：</p>
<p>无论视频多长，都是等间距抽取25帧。对每一帧都做ten
crop，即这一帧及其翻转都取四个边角以及中间部分，也就是会有10个crop，即最后会有250帧。由于视频的测试方式多种多样，因此不同的测试方式的比较不公平。</p></li>
<li><p>光流处理：</p>
<p>使用在GPU上运行的算法去获得光流（0.06s)，耗时大。而光流的另一个问题是这种密集光流所占的空间大，而这篇论文把光流图resize，并以jpeg的形式存储，使得从1.5TB降到27GB。但即使如此，光流法所占的时间和空间成本都是相当高的。</p></li>
</ol>
<h2 id="结论和总结">结论和总结</h2>
<p>这篇论文提出了一个具有竞争力的深度视频分类模型，包含了基于卷积神经网络的独立空间和时间识别流，
是视频理解的开山之作。</p>
<p>这篇论文给予了一种使用先验知识作为网络输入的思路来解决单神经网络不能解决的问题，提供了一种多流网络的思路，另外双流网络可以当作是一种多模态学习的先例。</p>
<h1 id="i3d">I3D</h1>
<p>从论文标题可以看出这篇论文的两个重要贡献： I3D模型（Inflated 3D
ConvNet）、Kinetics数据集。I3D直译为扩散的3D卷积神经网络，扩散的意思就是把原有的2D模型“扩散”到3D。</p>
<h2 id="相关工作">相关工作</h2>
<p>在2D的图像分类领域已经有了主导的神经网络结构，如VGG和Resnet，而视频领域至今依旧没有主导的架构。当时主要有以下几种选择，要么是纯2D网络，后面再接上比如LSTM这样的操作，从而对时间进行建模，要么是加上光流使得网络具备对运动信息的建模能力，要么则是直接使用3D网络来学习Special
Temporal 的时空特征 。</p>
<p>而这篇论文提出的I3D（Two-Stream Inflated 3D
ConvNets），有两个关键词：双流和inflated。由于3D网络的参数量过于巨大，而且也没有合适足够的数据去训练，因此网络不能太深（C3D）。作者在使用了其提出来的inflated操作直接就能使用例如Inception、VGG、Resnet这样的网络，而且能使用这些网络预训练的参数作为初始化。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913140943884.png"
alt="image-20220913140943884" />
<figcaption aria-hidden="true">image-20220913140943884</figcaption>
</figure>
<p>图a的方法就是卷积神经网络跟一个LSTM，把这个视频问题当作是一个图像分类问题，一张一张图片输入到卷积神经网络去做特征提取，接着把这些特征输入到LSTM网络。</p>
<p>图b的方法就是非常暴力的3D卷积网络，直接将视频段当成一个Valume，输入到网络之中。这样的网络能进行时空学习，而且卷积核是三维的，因此参数量变得很大。</p>
<p>图c的方法就是上文介绍过的双流网络。</p>
<p>图d的方法是将b和c的方法结合在一起，与双流网络一样，分为空间流和时间流，而不同于双流网络使用的Late
Fasion去融合结果，而是使用Early
Fasion，在没出结果之前，就把两个特征先融合在一起，然后使用一个3D卷积神经网络去处理得到分类结果。</p>
<p>以上四种方法作者都在其提出的数据集上进行了测试，然后提出双流I3D网络，即图e。</p>
<h2 id="实现细节-1">实现细节</h2>
<h3 id="如何inflated">如何Inflated</h3>
<p>简单来说，就是直接暴力地把把一个2D网络转换为3D网络。就是简单的把2D的kernel变成3D的kernel，2D的pooling层变成3D的pooling层。这样就可以不用自己设置网络，直接使用之前的2D网络。比如最新的Timesformer也是从Vit从2D
inflated 到3D</p>
<h3 id="bootstrapping">Bootstrapping</h3>
<p>Bootstrapping字面意思就是引导，就是如何从以及训练好的2D模型出发去初始化一个3D模型，然后继续训练以得到更好的效果。如何用
2D CNN 的预训练权重来初始化一个 3D
CNN。用同样一张图片，反复复制粘贴，变成一个boring的视频，然后只要保证这个无聊的视频输入到3D
CNN的输出和图片输入到2D
CNN的输出一致，就能保证初始化是正确的。具体的做法就是把所有的2D
filters在时间维度上复制了n次，
得到一个n帧的视频，为了保持输出的一致，除了 2D
参数复制n次之外，还需将参数除以n。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913151055074.png"
alt="image-20220913151055074" />
<figcaption aria-hidden="true">image-20220913151055074</figcaption>
</figure>
<h2 id="总结">总结</h2>
<p>这篇论文通过一系列的实验证明了在大规模数据集作预训练后，再做迁移学习能得到很好的效果。另外，在网络结构的设计上，作者也认为本文并没有进行全面的探索，像
action tubes、attention mechanism
这些结构都没有进行尝试。这些都是未来研究中不错的跟进方向。</p>
<p>这篇论文从两个方面上解决了训练的问题，一方面是没有数据也能使用Inflated的操作转换得到3D网络，并且能使用预训练的2D网络的参数去初始化3D网络，如P3D、R(2+1)D以及TimesFormer等；而另一方面，如果从头设计一个3D网络，这篇论文提供了一个足够大的数据集k
400，而且不需要依赖于ImageNet预训练的参数，如slowfast、X3D、mvit等工作都是从头开始训练的。自此，视频理解领域的后续研究工作就可以从更多角度展开了。</p>
<h1 id="deep-video">Deep Video</h1>
<p>CVPR 2014</p>
<h2 id="方法">方法</h2>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917200215637.png"
alt="image-20220917200215637" />
<figcaption aria-hidden="true">image-20220917200215637</figcaption>
</figure>
<ol type="1">
<li>Single Frame
其实就是一个图片分类的任务，单帧输入到卷积神经网络，不包含时间信息和视频信息</li>
<li>Late Fusion 和 Early Fusion 上文介绍过</li>
<li>Slow Fusion 在网络的过程中，对特征进行合并</li>
</ol>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917201204252.png"
alt="image-20220917201204252" />
<figcaption aria-hidden="true">image-20220917201204252</figcaption>
</figure>
<p>由于效果不好，作者尝试了图像领域的多分辨率卷积，将输入分成两部分（原图和正中间的抠图）。这种网络可以理解成一种双流网络，但双流的权值是共享的；也可以理解说是一种早期的注意力机制，这样的操作使得网络更关注图片中心的区域。</p>
<h2 id="总结-1">总结</h2>
<p>本文的主要贡献并不是在于任务性能的提升（实际也并没有提升），而是在于进行了深度
CNN
网络在视频理解领域的初步探索。在本文之后，大量的使用深度神经网络处理视频数据的工作涌现出来，视频理解领域正式进入深度学习时代。</p>
<p># 双流网络系列</p>
<p>## 原始双流网络改进方向</p>
<ol type="1">
<li><p>Late fusion 可否更换为Early
Fusion，如果能先做空间和时间流的特征交互，应该效果会变好</p></li>
<li><p>原始双流网络使用的是一种基于Alexnet的变体，采用VGG、Inception
Net、ResNet等更好的backbone是否会得到更好的效果，如何在小数据集上去训练大模型，如何控制好过拟合问题？</p></li>
<li><p>原始双流网络是直接把图片通过卷积神经网络的特征直接用做分类，而众所周知，RNN或LSTM可以很好的处理时序数据，或许可以使用LSTM处理卷积神经网络抽取的特征，得到更强的特征。</p></li>
<li><p>如何做长时间的视频理解？原始双流网络处理的视频都是非常短的，远小于一般一个动作的时间。</p></li>
</ol>
<h2 id="beyond-short-snippets">Beyond Short Snippets</h2>
<p>CVPR 2015</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918150440864.png"
alt="image-20220918150440864" />
<figcaption aria-hidden="true">image-20220918150440864</figcaption>
</figure>
<p>这篇论文的题目中的Short
Snippets指的就是2-3s的视频段，基本方法如上图，如果按照双流网络的思想，原始的输入只有几帧，处理的视频是很短的，为了处理长的视频，得找到合适的方法对抽取的特征做pooling，比如简单的Max
pooling或Average pooling，这篇文章对这些pooling做了详细的研究，类似Deep
video做了late pooling等，最后的结果conv
pooling效果最好，也尝试了使用LSTM做特征融合，而最后的结果提升有限。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918151315556.png"
alt="image-20220918151315556" />
<figcaption aria-hidden="true">image-20220918151315556</figcaption>
</figure>
<p>上图c代表卷积神经网络对每个视频帧抽取的特征，且这些卷积神经网络权值共享，接着使用五层LSTM去处理抽取的视频特征，最后使用softmax做分类。</p>
<p>正常来说LSTM在处理时序信息应该是会得到更好的效果的，但在处理短视频时，视频在时间上整个画面几乎没有变化，比较连续，也就是说输入到LSTM的时序特征没有变化，LSTM也就学不到什么东西。</p>
<h2
id="convolutional-two-stream-network-fusion-for-video-action-recognition">Convolutional
Two-Stream Network Fusion for Video Action Recognition</h2>
<p>CVPR 2016</p>
<p>这篇论文详细的讲如何做双流网络的合并，如何在时间流和空间流之中做Early
Fusion。作者从三个角度展开了研究：一是如何进行空间维度的特征融合（Spatial
Fusion），二是如何进行时间维度的特征融合（Temporal
Fusion)，三是应该在哪一层进行特征融合。</p>
<h3 id="spatial-fusion">Spatial Fusion</h3>
<p>如何保证时间流和空间流的特征图在同样的位置产生的通道response可以联系起来，在特征图层面做合并，作者做了以下尝试：</p>
<ol type="1">
<li><p>Max
Fusion：对于a和b两个不同的特征图，只取最大值作为合并后的特征图</p></li>
<li><p>Concatenation fusion：将两个分支中对应位置的值拼接起来</p></li>
<li><p>Conv fusion：将两个分支的值送入到一个卷积，得到结果</p></li>
<li><p>Bilinear
fusion：在每个位置上计算两特征的矩阵外积在哪合并</p></li>
</ol>
<h3 id="在哪合并">在哪合并</h3>
<p>作者针对在哪一层合并，做了大量的消融实验，得到了两种效果比较好的方式，如下图。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918165944620.png"
alt="image-20220918165944620" />
<figcaption aria-hidden="true">image-20220918165944620</figcaption>
</figure>
<p>第一种是空间流和时间流分别做，然后到conv4之后，进行一次fusion，在模型中间已经二合一，成为一流的网络。而第二种则是单独做到conv5以后，把空间流和时间流的特征做一次合并，得到一个spatial
temporal 的特征，然后继续保持空间流的完整性，在最后做一次合并。</p>
<h3 id="temporal-fusion">Temporal fusion</h3>
<p>当有很多视频帧，每一帧都有抽取了特征，如何在时间轴上把这些特征合并起来，得到最后的特征。作者尝试了两种方式：3D
Pooling和3D Conv + 3D Pooling</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918201122804.png"
alt="image-20220918201122804" />
<figcaption aria-hidden="true">image-20220918201122804</figcaption>
</figure>
<p>上图是作者提出的整体框架，蓝色是空间流，绿色是时间流，同样先分别用这两个网络去抽取RGB和光流图像的特征，抽取好特征之后就使用上文所说的Fusion方法，使用3D
Conv + 3D
Pooling的方法聚合特征，通过一个FC层，最后计算Spatiotemporal的损失函数。由于在视频处理中，时序信息很重要，因此作者单独把时间流拿出来再做一个3D
pooling，最后做一个针对时间上的损失函数。
也就是最后的框架包含了两个分支，最后同样使用late
fusion的方式得到分类结果。</p>
<h2 id="tsn">TSN</h2>
<p>eccv 2016</p>
<p>这篇论文提出一种简单的方式处理比较长的视频，并且确定了很多好用的技巧。</p>
<h3 id="temporal-segment">Temporal Segment</h3>
<p>这篇论文处理长视频的思路非常简单，就是把视频分成多段。具体来说，假如把视频分为三段，每一段随机抽取一帧，并以这一帧作为起点去选几帧计算光流图像，输入双流网络，每一段都如此处理，但权值共享。把每一个段出来的logits做一个consensus，也就是融合（乘法，加法，max，average或者lstm）每个段出来的分类结果，最后把时间流和空间流的结果做late
fusion。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918210327820.png"
alt="image-20220918210327820" />
<figcaption aria-hidden="true">image-20220918210327820</figcaption>
</figure>
<p>Temporal Segment 的思想不仅可以用在这种短视频这种裁剪好的video
clip上，也可以运用在没裁剪过的长视频，就算分的段落的含义不一样，只要在consensus上不使用average的方式，比如使用lstm去模拟时间的走势。而且这种思想还可以用在弱监督和无监督的工作上。</p>
<h3 id="好用的技巧">好用的技巧</h3>
<ol type="1">
<li>Cross Modality Pre-training：</li>
</ol>
<p>Modality指的就是图像和光流。作者提出了也使用Imagenet预训练参数来对光流分支初始化，而问题就是RGB图片的通道数和光流的通道数不对应，无法直接参数初始化。而本文认为，直接把三通道的权重取平均，直接复制20份就行了。而实验结果证明了这样做的可行性。</p>
<ol start="2" type="1">
<li><p>Regularization Techniques：</p>
<p>BN是一种广泛运用的正则化方法，但由于视频数据集太小会导致过拟合。本文提出了partial
BN，只打开第一层的BN来适应新的输入，后面的全部冻住不动以避免过拟合。</p></li>
<li><p>Data Augmentation</p>
<p>为了防止过拟合，数据增强是一个必不可少的工具。作者提出了两个东西：corner
croppong和scale-jittering。前者强制对边角处的图像进行裁剪；后者是对图片随机裁剪，得到不同长宽比的图片。</p></li>
</ol>
<h1 id="d网络系列">3D网络系列</h1>
<p>参考<a
target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/126238074">深度学习时代的视频理解综述_Adenialzz的博客</a></p>
<h2 id="c3d">C3D</h2>
<p>ICCV-2015</p>
<p>本文是早期探索 3D CNN
在视频理解领域的工作之一。做法非常简单，就是简单的搭了一个深度的 3D
卷积神经网络。与再之前的工作的差异在于使用了大型的数据集（Sports
1M），并使用了更深的网络结构。</p>
<p>文章给出的模型结构图非常简单，就是几层卷积和池化的堆叠，只是其中卷积核是
3D 的： <span class="math inline">\(3\times 3\times3\)</span>
。值得一提的是，作者最终的做法是将 fc6 得到的 4096 维的特征直接用 SVM
来做分类，又快效果又好。本文被称为 C3D 也是指的这个 fc6 得到的 4096
维的特征，被称为 C3D 特征。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919193757565.png"
alt="image-20220919193757565" />
<figcaption aria-hidden="true">image-20220919193757565</figcaption>
</figure>
<p>之前提到，C3D
的性能在同期工作中并不突出，那为什么这篇工作有如此大的影响力呢？实际上，这篇工作的主要贡献有两点。一是实现深度
3D CNN 模型，并在大型数据集上进行训练，证明了在视频领域，3D CNN 比 2D
CNN
再融合（DeepVideo）的效果要更好。二是作者指明并提供了抽特征（而非微调）的研究方式。在当时的年代，想要用大型的深度
3D CNN
模型跑大型的数据集不是每一个实验室都能做到的，作者提供了一个接口，上传视频，返回
4096 维的 C3D 特征。这就方便后续的研究者们根据大型 3D CNN
提取的特征，再进行下游任务的研究。这无疑极大地推动了当时视频领域的研究进程。因此，除了方法上有令人眼前一亮的新意之外，如果能够提供一些资源（如大数据集或大预训练模型），能够推动整个领域的研究进程，也是值得被铭记的工作。</p>
<h2 id="non-local">Non-local</h2>
<p>CVPR-2017</p>
<p>之前在双流网络系列，我们介绍过使用 LSTM 来建模时序信息。在 2017
年，NLP 领域发生了一件大事，Transformer 横空出世。我们知道，Transformer
的核心就是 self-attention
自注意力模块，它能够对长距离的序列信息进行建模。在之后，基于 Transformer
的模型几乎横扫了 NLP 领域和近两年的 CV 领域。这篇 2018
年的工作，就是将自注意力机制的 Non-local 算子，引入到了 CNN
模型中。并且为了适配视频任务，本文的 Non-local
算子是时空（spacetime）维度的。除了视频任务之外，本文也测试了 Non-local
算子在检测/分割等其他视觉任务上的性能。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195350007.png"
alt="image-20220919195350007" />
<figcaption aria-hidden="true">image-20220919195350007</figcaption>
</figure>
<p>上图就是用于时空的non-local算子，这实际上就是注意力机制，下面三个类似QKV，QK相乘得到注意力矩阵，再于V相乘，并有个残差操作。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195832134.png"
alt="image-20220919195832134" />
<figcaption aria-hidden="true">image-20220919195832134</figcaption>
</figure>
<p>消融实验体现了作者的设计思路。第一步是注意力怎么算，第二步是non-local用在哪一层，第三步是加多少个non-local，第四个就是证明spacetime上的自注意力机制的重要性。表格g探索了
non-local 在长视频（128帧）上的性能，可以看到提升也是很显著的。</p>
<h2 id="r21d">R(2+1)D</h2>
<p>CVPR 2018</p>
<p>这篇文章对时空的卷积做了详尽的实验，对各种2D+3D的卷积网络结构进行分析。最后的结论是把3D拆分成时间上的1D和空间上的2D能得到更好的效果。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115102912016.png"
alt="image-20221115102912016" />
<figcaption aria-hidden="true">image-20221115102912016</figcaption>
</figure>
<p>作者尝试了以下五种 2D/3D 卷积结构，来处理视频数据：</p>
<ol type="a">
<li>R2D：纯 2D 卷积，对每一帧单独的提取图像特征；</li>
<li>MCx：先 3D 卷积处理视频输入，再用计算开销更小的 2D
卷积继续提取特征；</li>
<li>rMCx：先 2D 卷积处理各帧图像输入，再用 3D
卷积对时空特征进行建模；</li>
<li>R3D：纯 3D 卷积，与 C3D、I3D 类似，这里的 R3D 也是 CVPR-2018
的一篇工作，是将 2D 的 ResNet 转换到 3D；</li>
<li>R(2+1)D：将 3D 拆分为空间维度的 2D 卷积和时间维度的 1D 卷积</li>
</ol>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115103836603.png"
alt="image-20221115103836603" />
<figcaption aria-hidden="true">image-20221115103836603</figcaption>
</figure>
<p>从参数量的角度上看，R2D明显参数量少，但效果不行，而3D和2D混合的方法都能达到50左右，最后论文提出的R（2+1）D在参数量与R3D一样的情况下，依然效果好了很多。</p>
<figure>
<img src="F:\Master\研究生课程\模式识别\image-20221115104452852.png"
alt="image-20221115104452852" />
<figcaption aria-hidden="true">image-20221115104452852</figcaption>
</figure>
<p>就是将一个<span class="math inline">\(d t\times d\times d\)</span> 的
3D 卷积，拆分为一个 2D 的空间维度上的 $ 1dd$ 的卷积，和一个 1D
的时间维度上的 <span class="math inline">\(t\times 1\times 1\)</span>
的卷积。中间进行了一次投射 <span
class="math inline">\(M_i\)</span>,可以保持与纯 3D
卷积的参数量相当，偏于公平的对比。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115110443121.png"
alt="image-20221115110443121" />
<figcaption aria-hidden="true">image-20221115110443121</figcaption>
</figure>
<p>上图展示了训练和测试过程的错误，把R3D和R（2+1）D做了对比，
结果展示无论是18层的浅层网络还是34层的深层网络，R（2+1)D的训练和测试误差都要比R3D低
。</p>
<p>R(2+1)D 取得最优效果，作者给出了两点解释：</p>
<ol type="1">
<li>将 3D 网络拆分为 R(2+1)D
，网络经过了更多的非线性激活函数，整体模型的非线性增强，从而表达能力更强；</li>
<li>R(2+1)D 网络将一整个 3D
网络拆分开，整个模型更容易训练。如上图，明显看到 R(2+1)D
网络收敛得更快更好。</li>
</ol>
<h2 id="slowfast">SlowFast</h2>
<p>ICCV-2019</p>
<p>这篇论文是把精度和效率结合的比较好的，研究动机来源于人眼中分别处理静态图像的场景信息和动态图像的运动信息的两种不同细胞，于是借鉴双流网络的结构，提出有
Slow Pathway 和 Fast Pathway
两支网络分别处理静态信息和动态信息的SlowFast网络。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115141919372.png"
alt="image-20221115141919372" />
<figcaption aria-hidden="true">image-20221115141919372</figcaption>
</figure>
<blockquote>
<p>蓝色部分是 Slow
Pathway，它用来处理静态的场景信息，它的特点是输入小，模型大。假设我们有一共
64 帧视频，这里按照每隔 16 帧取一帧，可以得到 4
帧作为输入（见图），但是它的模型每一层的通道数是比较大的（见表），从而这一分支中模型整体参数量比较大，这也对应着人眼中处理静态场景信息的
p 细胞占比较大。</p>
<p>绿色部分是 Fast
Pathway，它用来处理动态的运动信息，它的特点是输入大，模型小。同样 64
帧的视频，每隔 4 帧取一帧，共有 16
帧输入（见图），但是它每层的通道数比较小（见表），整体参数量较小，对应着
m 细胞占比较小。</p>
</blockquote>
<p>最后这个两分支的网络，在这两分支之间还使用了late
connection的方式结合起来，所以也就是双流之间的信息是可交互的，从而能够更好地学习到这个时空特征，最后达到一个速度和精度的结合。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115142838755.png"
alt="image-20221115142838755" />
<figcaption aria-hidden="true">image-20221115142838755</figcaption>
</figure>
<p>从网络看，快慢分支都是I3D，除了通道数不同，都是残差网络。从output
size看，在时序部分都不做下采样，始终保持输入的时间长度。</p>
<h1 id="video-transformer">Video transformer</h1>
<h2 id="timesformer">Timesformer</h2>
<p>ICML-2021</p>
<p>这篇论文探索了如何把图像的transformer迁移到视频。</p>
<figure>
<img
src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115145706200.png"
alt="image-20221115145706200" />
<figcaption aria-hidden="true">image-20221115145706200</figcaption>
</figure>
<figure>
<img
src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115150223140.png"
alt="image-20221115150223140" />
<figcaption aria-hidden="true">image-20221115150223140</figcaption>
</figure>
<p>具体来说，作者探索了如上五种结构。</p>
<p>Space Attention：即只在当前帧的空间维度计算自注意力，相当于只用 ViT
处理当前帧；(S) Joint Space-Time
Attention：在时间所有帧和空间所有像素计算自注意力，无疑一旦帧数或者分辨率高了，显存是不够的；(ST)
Divided Space-Time
Attention：现在时间维度计算自注意力，再在空间维度计算自注意力，类似
R(2+1)D 的设计；(T+S) Sparse Local Global
Attention：时间上在所有帧计算自注意力，但是在每一帧中只计算一个局部小窗口的元素，然后再扩大窗口范围再计算自注意力，而非直接计算全部像素的自注意力，类似
Swin Transformer；(L+G) Axial
Attention：分别在时间、宽度、长度三个维度上计算自注意力(T+W+H)</p>
<figure>
<img
src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151025142.png"
alt="image-20221115151025142" />
<figcaption aria-hidden="true">image-20221115151025142</figcaption>
</figure>
<p>作者在这五种结构上做了消融实验，结果表明JST和DST的效果比其他三种要好，但JST的显存占用过大。</p>
<figure>
<img
src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151432639.png"
alt="image-20221115151432639" />
<figcaption aria-hidden="true">image-20221115151432639</figcaption>
</figure>
<h1 id="总结-2">总结</h1>
<figure>
<img
src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151938926.png"
alt="image-20221115151938926" />
<figcaption aria-hidden="true">image-20221115151938926</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">视频行为识别博客笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-09-05 15:17:56" itemprop="dateCreated datePublished" datetime="2022-09-05T15:17:56+08:00">2022-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-13 12:26:02" itemprop="dateModified" datetime="2023-03-13T12:26:02+08:00">2023-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>博客原文地址：<a
target="_blank" rel="noopener" href="https://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review#sec-2">Deep
Learning for Videos: A 2018 Guide to Action Recognition
(qure.ai)</a></p>
<h1 id="动作识别以及难点">动作识别以及难点</h1>
<p>1.巨量的计算资源</p>
<p>一个简单的用于分类101类的卷积二维网络只有5M个参数，而同样的结构膨胀到3D结构时，会产生33M个参数。</p>
<p>2.需考虑上下时刻场景</p>
<p>动作识别需包含跨帧获取时空信息</p>
<p>3.设计分类网络结构</p>
<p>需设计能够捕获时空信息的架构</p>
<p>4.没有标准的<strong>benchmark</strong></p>
<h1 id="方法概述">方法概述</h1>
<h2 id="传统cv方法">传统CV方法</h2>
<p>基本可汇总为以下三步：</p>
<p>1.针对视频明显特征区域做提取，提取为密集向量或稀疏的兴趣点集合。（这一步之前一直是人工提取，后来提出的iDT算法改善了该流程）</p>
<p>2.提取的特征转化为固定尺寸的向量，来描述该视频的内容。这一步最流行的做法是Bag
of visual words</p>
<p>3.定义分类器，根据提取的视频特征向量，选定分类器，做分类训练和预测</p>
<h2 id="深度学习方法">深度学习方法</h2>
<p>3D卷积于2013年被用于动作识别，且无需其他的输入作为帮助。而2014年有两篇突破性的研究论文被发表。</p>
<h3 id="单流网络">单流网络</h3>
<p>https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf</p>
<p>这篇论文使用2D预训练卷积以探索多种方法来融合连续帧的时间信息</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/Karpathy_fusion.jpg"
alt="Karpathy_fusion" />
<figcaption aria-hidden="true">Karpathy_fusion</figcaption>
</figure>
<p>如上图,视频的所有连续帧都作为不同设置的输入。 <em>Single frame</em>
使用单一的网络架构在最后阶段融合所有帧的信息;<em>Late
fusion</em>融合使用两个共享参数的网络，间隔15帧，并在最后结合预测。<em>Early
fusion</em>在第一层通过卷积将超过10帧进行融合。 <em>Slow
fusion</em>涉及多个阶段的融合，兼顾早期和晚期融合之间的平衡。</p>
<p>不过与基于人工标定特征的方法相比,效果不好,作者推断有以下问题:一是学习到的时空特征没有捕捉到运动特征;二是由于数据集的多样性较低，学习如此详细的特征非常困难</p>
<h3 id="双流网络">双流网络</h3>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/2stream_high.png"
alt="2 stream architecture" />
<figcaption aria-hidden="true">2 stream architecture</figcaption>
</figure>
<p>https://arxiv.org/pdf/1406.2199.pdf</p>
<p>这篇论文在上文单流网络的基础上,设计一个获取动作特征的光流模型。这样就形成了双流模型，一个负责获取空间信息，一个负责获取时间信息。</p>
<p>尽管该方法取得了不错的效果，但还是有以下几个缺点： 一
视频的预测还是依据从视频中抽取的部分样本。对于长视频来说，在特征学习中还是会损失时序信息。
二
在训练时，从视频中抽取片段样本时由于是均匀抽取，这样会有错误标签的现象（即指定动作并不存在该样本片段中）。
三 在光流使用前，需要对视频预先做光流的抽取操作。</p>
<h1 id="论文总结">论文总结</h1>
<p>以下论文在某种程度上是两篇论文(单流和两流)的演变。而围绕这些论文反复出现的方法可以总结如下，所有的论文都是基于这些基本观点的即兴创作。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/recurrent_theme_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="lrcnlong-term-recurrent-convolutional-network">LRCN（Long-term
Recurrent Convolutional Network）</h2>
<p>https://arxiv.org/abs/1411.4389</p>
<h3 id="关键贡献">关键贡献</h3>
<ol type="1">
<li>基于之前的工作使用RNN来代替基于流的设计</li>
<li>用于视频表示的编码器架构的扩展</li>
<li>动作识别的端到端可训练架构</li>
</ol>
<h3 id="解释">解释</h3>
<p>在这之前有利用CNN对视频片段做特征提取，然后再用LSTM对时序的特征做最终分类，但效果不好。而LRCN是在卷积块(编码器)之后使用LSTM块(解码器)即端到端训练，同时也使用RGB和光流都作为输入，并将预测结果加权相加得到好的效果。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220905164221964.png"
alt="image-20220905164221964" />
<figcaption aria-hidden="true">image-20220905164221964</figcaption>
</figure>
<h3 id="缺陷">缺陷</h3>
<ol type="1">
<li>将视频分为片段后，会导致某些片段没有标签对应的动作，从而干扰模型的效果</li>
<li>无法捕捉长期的时间信息</li>
<li>使用光流作为特征意味着需要分别计算流特征</li>
</ol>
<p>基于上述缺陷，新的工作通过使用更低分辨率的视频和更长的视频片段（60帧）以实现更好的性能。</p>
<h2 id="c3d">C3D</h2>
<p>https://arxiv.org/pdf/1412.0767</p>
<h3 id="关键贡献-1">关键贡献</h3>
<ol type="1">
<li>利用三维卷积网络做特征提取器</li>
<li>广泛搜索最佳3D卷积内核和架构</li>
<li>使用反卷积层解释模型决策</li>
</ol>
<h3 id="解释-1">解释</h3>
<figure>
<img src="https://blog.qure.ai/assets/images//actionrec/c3d_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>另外,作者使用了反卷积层来解释这样的设计，他们的发现是，在最初的几帧中，网络关注的是空间外观，并在随后的几帧中跟踪运动。</p>
<figure>
<img src="https://blog.qure.ai/assets/images//actionrec/trial.gif"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h3 id="缺陷-1">缺陷</h3>
<ol type="1">
<li>长时间模型的建模依旧是个未解决的问题</li>
<li>过于庞大的网络在训练上计算过慢</li>
</ol>
<h3 id="note">Note：</h3>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/fstcn_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>FSTCN，分解3D卷积网络，主要思路是将三维卷积分解为空间二维卷积，然后是时间一维卷积。将一维卷积放在二维卷积层之后，实现二维时域和信道维的卷积。</p>
<h2 id="三维卷积注意力机制">三维卷积+注意力机制</h2>
<p>https://arxiv.org/abs/1502.08029</p>
<h3 id="关键贡献-2">关键贡献</h3>
<ol type="1">
<li>新颖的3D CNN-RNN编码器-解码器结构，可以捕捉局部时空信息</li>
<li>使用注意力机制来获取全局上下文</li>
</ol>
<h3 id="解释-2">解释</h3>
<p>虽然这项工作与动作识别没有直接的关系，但在视频表征方面是具有里程碑意义的工作。本文采用三维CNN
+ LSTM作为视频描述任务的基础架构。在基础上，作者使用一个预先训练的3D
CNN来提高效果。</p>
<h3 id="算法">算法</h3>
<p>其设置与LRCN中描述的编码器-解码器架构几乎相同，但有两个不同之处：</p>
<ol type="1">
<li>并非单纯的使用了3D卷积做LSTM的特征向量输入。对于每一帧，先通过3D卷积获取feature
maps，再通过2D卷积对其帧集获取feature maps集合。2D和3D
CNN使用的是预先训练的，而不是像LRCN这样的端到端训练。</li>
<li>并非平均所有帧的时间向量。加权平均值的方法用于结合时间特征。在每个时间步上，根据LSTM输出来确定注意力权重。</li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/Larochelle_paper_high.png"
alt="Attention Mechanism" />
<figcaption aria-hidden="true">Attention Mechanism</figcaption>
</figure>
<h2 id="twostreamfusion">TwoStreamFusion</h2>
<p>https://arxiv.org/abs/1604.06573</p>
<h3 id="关键贡献-3">关键贡献</h3>
<ol type="1">
<li>通过更好的远距离损失的远距离时间建模</li>
<li>新颖的多层次融合架构</li>
</ol>
<h3 id="解释-3">解释</h3>
<p>在这个工作中，作者使用了基本的双流架构，并采用了两种新颖的方法，在提高性能的同时并不会带来任何参数的显著增加。</p>
<ol type="1">
<li>空间流和时间流的融合：以洗头和刷牙为例，空间网络可以捕捉视频中的空间相关性（判断头发还是牙齿），时间网络则可以捕捉到视频中每个空间位置的周期性运动。因此，将人脸特定区域的空间特征映射到相应区域的时间特征映射是非常重要的。为了达到同样的效果，网络需要在较早的水平上进行融合，使相同像素位置的响应处于对应状态，而不是在最后进行融合。</li>
<li>跨时间框架组合时间净输出，以便对长期依赖也进行建模。</li>
</ol>
<h3 id="算法-1">算法</h3>
<p>与双流架构基本一样，除了</p>
<ol type="1">
<li><p>如下图所示，Conv_5层的输出均通过卷积层+池化层的方式融合，而此算法则在最后一层加了另一种融合，最后融合输出作为spatiotemporal
loss。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/fusion_strategies_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure></li>
<li><p>时间融合采用跨时间叠加的时间网络输出，采用conv+pooling融合的方法计算时间损失</p></li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/2streamfusion.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="tsn">TSN</h2>
<p>https://arxiv.org/abs/1608.00859</p>
<h3 id="关键贡献-4">关键贡献</h3>
<ol type="1">
<li>针对长期时间建模的有效解决方案</li>
<li>确定BN、dropout和预训练是一种有效的尝试</li>
</ol>
<h3 id="解释-4">解释</h3>
<p>与基本的双流架构有两个主要的不同：</p>
<ol type="1">
<li>他们建议在整个视频中稀疏地采样片段，以更好地建模长期时间信号，而不是在整个视频中随机采样。</li>
<li>为了最终的预测，作者在视频层面探索了多种策略。最好的策略是
<ol type="1">
<li>通过对片段平均，分别组合数十个时间和空间流(以及其他流，如果涉及其他输入模式)</li>
<li>对所有类融合最终的空间和时间得分使用加权平均和应用softmax。</li>
</ol></li>
</ol>
<p>该工作的另一个重要部分是解决过拟合问题(由于数据集规模较小)，并演示使用现在流行的技术，如批处理规范化、Dropout和预训练来应对。作者还评估了两种新的光流输入模式，即弯曲光流和RGB差。</p>
<h3 id="算法-2">算法</h3>
<p>在训练和预测过程中，将一段视频分成K段，每段时长相等。然后，从K个片段中随机抽取片段。其余的步骤仍然类似于上面提到的双流架构的更改。</p>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/tsn_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="actionvlad">ActionVLAD</h2>
<p>https://arxiv.org/pdf/1704.02895.pdf</p>
<h3 id="关键贡献-5">关键贡献</h3>
<ol type="1">
<li>可学习的视频级聚合功能</li>
<li>具有视频级聚合特征的端到端可训练模型，以捕获长期依赖</li>
</ol>
<h3 id="解释-5">解释</h3>
<p>作者最显著的贡献是使用了可学习的特性聚合(VLAD)，而不是使用maxpool或avgpool的普通聚合。聚合技术类似于视觉词汇。有多个基于锚点(比如<span
class="math inline">\(c_1\)</span>, <span
class="math inline">\(c_k\)</span>)的学习词汇，代表k个典型的动作(或子动作)相关的时空特征。两个流结构中的每个流的输出都按照k空间的动作词特征进行编码——每个特征都是输出与任何给定的空间或时间位置对应的锚点的差值。</p>
<p>（没看懂）</p>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/actionvlad.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>平均或最大池只作为一个描述符来表示整个点的分布，这对于表示由多个子动作组成的整个视频来说是次优的。相比之下，提出的视频聚合通过将描述符空间分割为<strong>k个单元</strong>并在每个单元内池化来表示具有多个子动作的描述符的整个分布。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/pooling_difference_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="hiddentwostream">HiddenTwoStream</h2>
<p>https://arxiv.org/abs/1704.00389</p>
<h3 id="关键贡献-6">关键贡献</h3>
<p>提出一种新颖的使用单独的网络实时生成光流输入的架构</p>
<h3 id="解释-6">解释</h3>
<p>光流在双流架构的使用使得必须预先计算每个采样帧的光流，从而对存储和速度产生不利影响。这篇论文提出了一种使用无监督架构来生成光流的方法。</p>
<p>光流法可以被当成是一种图像重建问题。给定一对相邻的帧<span
class="math inline">\(l_1、l_2\)</span>作为输入，CNN生成一个流场<span
class="math inline">\(V\)</span>，然后利用预测的流场<span
class="math inline">\(V\)</span>和<span
class="math inline">\(l_2\)</span>，利用反翘曲将<span
class="math inline">\(l_1\)</span>重构为<span
class="math inline">\(l_1^{&#39;}\)</span>，使<span
class="math inline">\(l_1\)</span>与重构的差值最小。</p>
<h3 id="算法-3">算法</h3>
<p>作者探索了多种策略和架构，在不太影响精度的前提下，以最大的帧数和最小的参数产生光流。最后的体系结构与前面提到的双流体系结构相同</p>
<ol type="1">
<li>时间流现在有堆叠在一般时间流架构的顶部的光流生成网络(MotionNet)。时间流的输入现在是后续帧而不是预处理的光流。</li>
<li>对于MotionNet的无监督训练，还有额外的多层次损失</li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/hidden2stream_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="i3d">I3D</h2>
<p>https://arxiv.org/abs/1705.07750</p>
<h3 id="关键贡献-7">关键贡献</h3>
<ol type="1">
<li>利用预训练将基于3D的模型结合到两个流架构中</li>
<li>Kinetics数据集用于未来的基准测试和改进的行动数据集的多样性</li>
</ol>
<h3 id="解释-7">解释</h3>
<p>作者不是使用单一的3D网络，而是在双流架构中为两个流使用两个不同的3D网络。此外，为了利用预训练的2D模型，作者在第三维中重复了2D预训练的权重。现在的空间流输入由时间维度上叠加的帧组成，而不是像基本的两种流结构那样由单个帧组成。</p>
<h2 id="t3d">T3D</h2>
<p>https://arxiv.org/abs/1711.08200</p>
<h3 id="关键贡献-8">关键贡献</h3>
<ol type="1">
<li>跨可变深度组合时间信息的架构</li>
<li>新颖的训练架构和技术，以监督2D预训练的网络转移到3D网络</li>
</ol>
<h3 id="解释-8">解释</h3>
<p>作者扩展了在I3D上完成的工作，但建议使用基于单一流3D
DenseNet的架构，在密集块之后叠加多深度时间池化层(时间过渡层)，以捕获不同的时间深度。多深度池化是通过池化不同时间大小的核来实现的。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/ttl_layer_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>除上述内容外，作者还设计了一种新的技术，以在预先培训的2D Conv
Nets和T3D中监督转移学习。2D预三角网和T3D都是从视频中呈现的框架和剪辑，其中剪辑和视频可能来自同一视频。该体系结构是基于相同的三角形来预测0/1的，并且预测的误差通过T3D
NET进行了反向传播，以便有效地传输知识。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/transfer_learning_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
