\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}


% Include other packages here, before hyperref.
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}






% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{7321} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\newcommand{\czq}{\textcolor{blue}}

\begin{document}

%%%%%%%%% TITLE
\title{Hypergraph Transformer for Skeleton-based Action Recognition}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }


\author{Yuxuan Zhou\textsuperscript{$1$},
Zhi-Qi Cheng\textsuperscript{$2$},
Chao Li\textsuperscript{$3$},
Yanwen Fang\textsuperscript{$4$},
Yifeng Geng\textsuperscript{$3$},
Xuansong Xie\textsuperscript{$3$},
Margret Keuper\textsuperscript{$5$}\\
\textsuperscript{1}{University of Mannheim}
\textsuperscript{2}{Carnegie Mellon University}\\
\textsuperscript{3}{Alibaba Group}
\textsuperscript{4}{The University of Hong Kong}
\textsuperscript{5}{University of Siegen}\\
% \textsuperscript{5}{Max Planck Institute for Informatics, Saarland Informatics Campus}\\
{\tt \small zhouyuxuanyx@gmail.com, zhiqic@cs.cmu.edu,
lllcho.lc@alibaba-inc.com, u3545683@connect.hku.hk}\\
{\tt \small cangyu.gyf@alibaba-inc.com, xingtong.xxs@taobao.com,
keuper@mpi-inf.mpg.de
}
}


\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
Skeleton-based action recognition aims to recognize human actions given human joint coordinates with skeletal interconnections.
By defining a graph with joints as vertices and their natural connections as edges, previous works successfully adopted Graph Convolutional networks (GCNs) to model joint co-occurrences and achieved superior performance. More recently, a limitation of GCNs is identified, i.e., the topology is fixed after training. To relax such a restriction, Self-Attention (SA) mechanism has been adopted to make the topology of GCNs adaptive to the input, resulting in the state-of-the-art hybrid models. Concurrently, attempts with plain Transformers have also been made, but they still lag behind state-of-the-art GCN-based methods due to the lack of structural prior. Unlike hybrid models, we propose a more elegant solution to incorporate the bone connectivity into Transformer via a graph distance embedding. Our embedding retains the information of skeletal structure during training, whereas GCNs merely use it for initialization. 
% an important underlying issue of Graph Models w.r.t. the characteristics of skeleton data, i.e., intrinsic high-order relations are ignored. 
More importantly, we reveal an underlying issue of graph models in general, i.e., pairwise aggregation essentially ignores the high-order kinematic dependencies between body joints. 
% In fact, certain re-occurring groups of body joints are often involved in the kinematics of specific actions (e.g., subconscious hand movement for keeping balance while walking), which is a higher-order structure.  
% Vanilla attention is incapable of describing such underlying relations that are persistent and beyond pair-wise.
To fill this gap, we propose a new self-attention (SA) mechanism on hypergraph, termed Hypergraph Self-Attention (HyperSA), to incorporate intrinsic higher-order relations into the model. 
% exploit these unique aspects of skeleton data to close the performance gap between Transformers and GCNs.
We name the resulting model \textbf{Hyperformer}, and it beats state-of-the-art graph models w.r.t.~accuracy and efficiency on NTU RGB+D, NTU RGB+D 120, and Northwestern-UCLA datasets. Our code is available at \href{https://github.com/ZhouYuxuanYX/Hypergraph-Transformer-for-Skeleton-based-Action-Recognition}{Hyperformer Github Repository}.
% On the larger NTU RGB+D 120 dataset, the significant performance gains brought by our Hyperformer further validate the effectiveness of our approach for modeling high-order relations of joints. 
\end{abstract}

%%%%%%%%% BODY TEXT


\begin{figure}[]
  \centering
 %  \vfill
      \centering
      \includegraphics[width=0.48\textwidth]{hypergraph_tease.pdf}
     \caption{Illustration of our HyperSA using a frame from the action class ``Clapping Hands".
     HyperSA accommodates the additional high-order relations besides the skeletal interconnections.}
     \label{fig:tease}
\end{figure}



\section{Introduction}
\label{sec:intro}
%Human action recognition finds its application in a wide range of areas, such as video retrieval, surveillance, and human-machine interaction. 
%In recent years, skeleton-based human action recognition approaches have attracted increasing attention due to their computational efficiency and robustness to environmental variations in backgrounds or camera viewpoints.
%The body keypoints can also be easily acquired by sensors such as Kinect \cite{zhang2012microsoft} or reliable pose estimation algorithms \cite{cao2017realtime}. 
Skeleton-based human action recognition has attracted increasing attention due to its computational efficiency and robustness to environmental variations and camera viewpoints.
One of the key advantages of skeleton-based action recognition is that body keypoints can be easily acquired using sensors such as Kinect \cite{zhang2012microsoft}  or reliable pose estimation algorithms \cite{cao2017realtime}. 
This offers a more reliable alternative to traditional RGB or depth-based methods, making it a promising solution for various real-world applications.

% too much about GCN, should be modified to fit our Theme
Graph Convolution Networks (GCNs) have been widely used for modeling off-grid data.
To our knowledge, Yan et al.~\cite{yan2018spatial} were the first to treat joints and their natural connections as nodes and edges of a graph, and employ a GCN \cite{kipf2016semi} on such a predefined graph to learn joint interactions.
Since then, GCNs have become the de facto standard of choice for skeleton-based action recognition.
To further capture the interactions between physically unconnected joints, state-of-the-art GCNs \cite{chi2022infogcn, chen2021channel, ye2020dynamic, shi2019two, song2021constructing, song2020stronger} adopt a learnable topology which only uses the physical connections for initialization.
Even so, they still need to rely on attention mechanisms to relax the restriction of the fixed topology, which is the key to the improved performances, as shown in their ablation studies.

% More recently, the application of Transformer models \cite{vaswani2017attention} in this area has been studied \cite{plizzari2021spatial, shi2020decoupled}, motivated by the great success of Language \cite{devlin2018bert, liu2019roberta} and Vision Transformers \cite{dosovitskiy2021an, touvron2021training}. Actually, the application of Transformer models to skeleton data is more straightforward than their application to images. Transformers model body joint co-occurrences via the Multi-Head Self-Attention (MHSA) mechanism and are thus not restricted to any fixed topology. Nevertheless, the performance of Transformers is still inferior to state-of-the-art GCNs in skeleton-based action recognition. 

%A natural question that arises is whether a purely attention-based Transformer is a better candidate for this task. Yet, the performance of current such models is far from being satisfying \cite{plizzari2021spatial, shi2020decoupled}. We found this is due to the fact that the formulation of the vanilla Transformer ignores the special characteristics of skeleton data. 

Given these facts, it is natural to question whether a purely attention-based Transformer model would be a better candidate for skeleton-based action recognition. However, current research \cite{plizzari2021spatial, shi2020decoupled} has shown that the performance of such models is far from satisfactory. This can be attributed to the fact that the formulation of the vanilla Transformer ignores the unique characteristics of skeleton data, i.e., 
% homogeneous graph, relational bias
% permutation invariance, k-hop embedding, positional informaiton is important
% \begin{itemize}
% 对应这个问题的改动很小，在方法部分作为补充提出就行，不是最重要的创新点！！！！！
% \item \textbf{Heterogeneity}: Transformer assumes the input tokens to be homogeneous, whereas human body joints are inherently heterogeneous, e.g., each physical joint plays a unique role and thus has different relations to others.
% Such inherent relations persist in different
% actions regardless of the input joint coordinates or action class. 
% \item Bone Connectivity: Transformer relies on permutation invariant attention operation which ignores the positional information.
%The vanilla attention operation assumes permutation invariance and thus destroys the positional information. For alleviating this issue, absolute positional embeddings \cite{vaswani2017attention, dosovitskiy2021an, touvron2021training} are widely adopted. Yet they are incapable of representing the sophisticated bone connectivity between human body joints.
%In contrast to absolute positional embeddings, relative positional embeddings prove to be superior for Transformers on various tasks on language \cite{shaw2018self, dai2019transformer, he2020deberta}, vision \cite{liu2021swin, zhou2022sp, wu2021rethinking, zhou2022sp} and graph \cite{ying2021transformers} data, because they retain more structural information than the former. 
the permutation invariant attention operation is agnostic to the bone connectivity between human body joints. To address this issue, absolute positional embeddings have been used~\cite{vaswani2017attention, dosovitskiy2021an, touvron2021training} , but they still lack the necessary structural information. In contrast, relative positional embeddings have been shown to be more effective for Transformers in various tasks, involving language~\cite{shaw2018self, dai2019transformer, he2020deberta}, vision \cite{liu2021swin, zhou2022sp, wu2021rethinking, zhou2022sp}, and graph data \cite{ying2021transformers}. To incorporate the information of bone connectivity of human body as well, we  also introduce a powerful relative positional embedding based on graph distance, inspired by Spatial Encoding in \cite{ying2021transformers}. Our embedding retains the information of skeletal structure during the entire training process, whereas GCNs merely use it for initialization. 

More importantly, we reveal an underlying issue of graph models for this task in general.
For human actions, each type of body joint has its own unique physical functionality. As a result, certain re-occurring groups of body joints are often involved in specific actions, such as the subconscious hand movement for maintaining balance. 
% It could potentially improve the performance of Transformer-based models by allowing them to recognize body joints that often cooperate in different actions. 
Vanilla attention is not capable of capturing these underlying relationships that are independent from joint coordinates and go beyond pair-wise interactions. 
To compensate for this, we employ the concept of hypergraph \cite{zhou2006learning, feng2019hypergraph, bai2021hypergraph} to accommodate the higher-order relations of body joints. With the hypergraph representation, we propose a novel variant of Self-Attention (SA) called \textbf{Hypergraph Self-Attention (HyperSA)}, which considers both pair-wise and high-order relations. As shown in \cref{fig:tease}, given a partition of the human body joints into different groups, a representation of each group is derived based on its assigned joints. The group representation is then linearly transformed and multiplied with joint queries, allowing joint-to-group interactions in addition to the vanilla joint-to-joint SA. 
Though HyperSA works well with empirical partitions, we additionally propose an approach to search the optimal partition strategy automatically, further improving its performance.


% Inspired by Spatial Encoding in \cite{ying2021transformers}, we introduce a powerful K-Hop Relative Positional Embedding (K-Hop RPE) based on the distance of the shortest path (SPD) between joint pairs.

%Moreover, each type of body joint has its unique physical functionality for human action. Consequently, certain re-occurring groups of body joints are often involved in specific actions, such as the subconscious hand movement for keeping balance. Vanilla attention is incapable of describing such underlying relations that are independent from joint coordinates and beyond pair-wise. 

% This is special for skeleton data, because image patches or word tokens which don't have 
% This implies that there exist inherent high-order relations which are persistent and beyond pair-wise.
% Taking such high-order relations into account is proved to be beneficial for modeling generic non-structured data \cite{}.
% \end{itemize}

% For Transformer models, we analyze that SA and MLP layers are essentially responsible for modeling inter- and intra-token relations, respectively. Unlike image patches or word tokens which are high dimensional or abstract, joint tokens merely represent three-dimensional input coordinates with limited information. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key for successful action recognition. We thus suggest to remove MLP layers for computation and memory reduction and show in \cref{sec:ablation} that MLP layers are indeed neglectable.

% In contrast to absolute positional embedding, relative positional embedding proves to be superior for Transformer on various tasks on language \cite{shaw2018self, dai2019transformer, he2020deberta}, vision \cite{liu2021swin, zhou2022sp, wu2021rethinking} and graph \cite{ying2021transformers} data, because it retains more structural information than the former.
At the same time, Transformers spend a large portion of their capacity on intra-token modeling in their feed-forward layer. While this is important for complex tokens such as image patches or word embeddings, we analyze that such an expensive step is not necessary for joint coordinates which are merely three-dimensional. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key to successful action recognition. We thus suggest removing MLP layers for computation and memory reduction, and show in \cref{sec:design} that MLP layers are indeed negligible.
This leads to a light-weighted Transformer which is comparable to GCNs in model size and computation cost. 



% To incorporate the information of bone connectivity of human body as well, we  also introduce a powerful k-Hop Relative Positional Embedding (k-Hop RPE) based on the distance of the shortest path (SPD), inspired by Spatial Encoding in \cite{ying2021transformers}. 

% Altogether, we construct a model named Hypergraph Transformer (Hyperformer), with our HyperSA for modeling joint co-occurrences and an efficient temporal convolution module for temporal modeling. Hyperformer is the first Transformer model that achieves state-of-the-art performance on skeleton-based action recognition benchmarks including NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA datasets.
% Our study not only breaks the limitations of established GCN frameworks in this field, but also sheds light on how to design Transformers to better exploit task-specific knowledge. We believe the latter is the key for Transformers to become a universal model regardless of the form of data. 

% In this work, we made the following contributions:
Our main contributions can be summarized as follows:
\begin{itemize}
    \item We propose to incorporate the structural information of human skeleton into Transformer via a relative positional embedding based on graph distance, leveraging the gap between Transformer and state-of-the-art hybrid models. 
    
    \item We devise a novel variant of Self-Attention (SA) called \textbf{Hypergraph Self-Attention (HyperSA)}, based on the hypergraph representation. 
    To our best knowledge, our work is the first to apply the hypergraph representation for skeleton-based action recognition, which considers the pair-wise as well as high-order joint relations, going beyond current state-of-the-art methods. 
    
    \item We construct a light-weighted Transformer based on our proposed relative positional embedding and HyperSA. It beats state-of-the-art graph models on skeleton-based action recognition benchmarks w.r.t.~both efficiency and accuracy.
\end{itemize}

% creatively apply the concept of hypergraphs \cite{zhou2006learning, feng2019hypergraph, bai2021hypergraph} to represent the underlying higher-order relations of body joints. With the hyergraph representation, we propose a novel variant of Self-Attention (SA) called \textbf{Hypergraph Self-Attention (HyperSA)}, which accommodates both pair-wise and high-order relations. As shown in \cref{fig:model}, given a partitioning of the human body joints
% into groups, a representation of each group is derived based on its assigned joints. The group representation is then linearly transformed and multiplied with joint queries, allowing joint-to-group interactions 
% in addition to the vanilla joint-to-joint SA. Although HyperSA works with empirical partitions, we propose an approach to search the optimal partition strategy automatically.
% To incorporate the information of bone connectivity of human body as well, we  also introduce a powerful k-Hop Relative Positional Embedding (k-Hop RPE) based on the distance of the shortest path (SPD), inspired by Spatial Encoding in \cite{ying2021transformers}. 

% Altogether, we construct a model named Hypergraph Transformer (Hyperformer), with our HyperSA for modeling joint co-occurrences and an efficient temporal convolution module for temporal modeling. Hyperformer is the first Transformer model that achieves state-of-the-art performance on skeleton-based action recognition benchmarks including NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA datasets.
% Our study not only breaks the limitations of established GCN frameworks in this field, but also sheds light on how to design Transformers to better exploit task-specific knowledge. We believe the latter is the key for Transformers to become a universal model regardless of the form of data. 

%%%%%%%%%%%

\section{Related work}
In this section, we highlight the most related work to ours regarding skeleton representation as well as the spectacle of application.

\subsection{Representation of skeleton data}
\noindent \textbf{Graph Representation} Graph is the most prevalent choice for representing non-euclidean data and human skeleton can be naturally represented as graph. 
Comparing to other graph models \cite{xu2018how, gilmer2017neural, velivckovic2017graph}, the Graph Convolutional Network (GCN) proposed by Kipf \etal~\cite{kipf2016semi} is widely adopted for action recognition due to its simplicity and thus higher resistance to overfitting.
% \noindent \textbf{Graph Attention}
% \subsubsection{Graph Convolution} 
% To apply convolution on graphs, there is an additional challenge of how to handle different numbers of neighboring nodes. Graph convolution generalizes the convolution operation on image data to non-Euclidean data. 
% Comparing to many advanced GCN models such as GIN \cite{xu2018how} and MPNN \cite{gilmer2017neural},
% the GCN proposed by Kipf et al. \cite{kipf2016semi} is widely adopted for action recognition due to its simplicity and thus higher resistance to overfitting. 
% In contrast to convolution, 
% attention mechanism is naturally feasible for learning on graph. 
% Previous work \cite {GAT, Graph Transformer Networks} merely adapted attention mechanisms to GNN architecture. 
Recently, Graphformer \cite{ying2021transformers} achieves state-of-the-art performance on generic graph data. It reveals the necessity of effectively encoding
the structural information into Transformers. Their finding conforms to our analysis w.r.t.~the aspect of topology, and we analogously propose a powerful k-Hop RPE for our Hyperformer.   

\noindent \textbf{Hypergraph Representation} In real-world scenarios,
relationships could go beyond pairwise associations. Hypergraph further considers higher-order correlations among data. Although hypergraphs can be modeled as
a graph approximately via techniques such as clique expansion\cite{zhou2006learning}, such approximations fail to capture higher-order relationships in the data and result
% this line causes 6 critical errors
in unreliable performance~\cite{chien2019hs, li2017inhomogeneous}.
This motivates
the study of learning on hypergraphs \cite{hein2013total, zhang2017re, feng2019hypergraph, yadati2019hypergcn}. Attention-based hypergraph models have also been proposed for multi-modal learning \cite{kim2020hypergraph} and inductive text classification \cite{ding2020more}. Our proposed HyperSA is the first hypergraph attention which is specially designed for skeleton-based action recognition.

\subsection{Skeleton-based action recognition}
In early years, RNNs \cite{du2015hierarchical, song2017end,zhang2017view}
have been a popular choice to tackle the problem of skeleton-based human action recognition.
The application of CNNs for this task 
\cite{ke2017a, liu2017enhanced}
is also well studied.  
Nevertheless, the spatial interactions between joints are ignored in the above methods, 
and GCNs have become a more common choice in this field, by successfully modelling the spatial configurations
as graphs. 

\textbf{GCN-based approaches}  Yan \etal~\cite{yan2018spatial} first introduced GCN \cite{kipf2016semi} to model the joint correlations and demonstrated its effectiveness for action recognition. However, 
the limitation of assuming a fixed topology according to the natural connections is identified later, and most follow-up works adopt a learnable topology for action recognition.
Many among them \cite{cheng2020decoupling, shi2019two, chen2021channel, ye2020dynamic} also employ attention or similar mechanisms to produce a data-dependent component of the  
topology (analogous to Graph Attention Networks \cite{velivckovic2017graph}), boosting GCN's performance further.

\textbf{Transformer-based approaches}
Attempts to tackle this problem with Transformers have been made recently.
They mainly focus on handling the challenge brought by the extra temporal dimension. 
\cite{plizzari2021spatial} propose a two-stream model consisting of spatial and temporal Self-Attention for modeling intra- and inter-frame correlations, respectively. Instead, \cite{shi2020decoupled} employ a Transformer which models the spatial and temporal dimension in an alternate fashion. 
Nevertheless, none of them achieved comparable results to state-of-the-art GCN-based approaches. Since they simply follow the design of vanilla Transformers, the special characteristics of skeleton data are ignored.

\section{Preliminaries}
In this section, we recap the definition of Self-Attention and hypergraphs.

\subsection{Self-Attention}
% describe the functionality of attention: assigning weight to elements

% 这一段讲得太多了，精简一下文字
% 讲得太复杂了
% 强调它的pairwise
% 把公式1和公式2全去掉用口述就行，直接用公式3,
% mutli-head self-attention 不用写公式4了一句话带过

% 画个对比图！！！！
% 需要给个例子，那些是一组

%  Transformers \cite{vaswani2017attention} employ 4 projection matrices for calculating the \textit{Key}, \textit{Query} and \textit{Value} and the final output: $W^Q \in \mathbb{R}^{d/h \times d/h}$, $W^K \in \mathbb{R}^{d/h \times d/h}$, $W^V \in \mathbb{R}^{d/h \times d/h}$, $W^O \in \mathbb{R}^{d \times d}$.
Given an input sequence in the form of $X=(\vec{x}_1,...,\vec{x}_n)$, each token $\vec{x}_i$ is first projected into \textit{Key} $\vec{k}_i$ , \textit{Query} $\vec{q}_i$  and \textit{Value} $\vec{v}_i$  triplets.
% {\rm exp}
Then the so-called attention score $A_{ij}$ between two tokens is obtained by applying a softmax function to the dot product of $\vec{q}_i$ and $\vec{k}_j$ \cite{vaswani2017attention}:
% {\rm exp}
\begin{equation}\label{eq:1} 
 A_{ij} = \vec{q}_i \cdot \vec{k}_j^\top,
\end{equation}
the final output at each position is computed as the weighted sum of all Values:
\begin{equation}
\label{eq:2}
\vec{y}_i = \sum_{j=1}^n A_{ij}\vec{v}_j
.
\end{equation}
An extension called Multi-Head Self-Attention (MHSA) is often adopted by Transformers in practice. It
divides the channel dimension into subgroups and apply Self-Attention to each subgroup in parallel to learn different kinds of inter-dependencies. For simplicity, we omit the notation of MHSA in this paper.
% To obtain the final output, a linear transformation is applied to the concatenated outputs from each head:
% \begin{equation}
% \label{eq:4}
%     \vec{y_i} = [\vec{y_i}^{head \: 1}, \vec{y_i}^{head \: 2}, ..., \vec{y_i}^{head \: h}]W^O.
% \end{equation}
% graph CONVOLUTION 别讲了
% related work多讲讲和子任务相关的，少讲点通用的理论
\subsection{Hypergraph representation}

% 
Unlike standard graph edges, a hyperedge in a hypergraph connects
two \emph{or more} vertices. An unweighted hypergraph is defined as $\mathcal{H} =
(\mathcal{V}, \mathcal{E})$, which consists of a vertex set $\mathcal{V}$ and a hyperedge set $\mathcal{E}$. The hypergraph $\mathcal{H}$ can be denoted by a $\vert \mathcal{V} \vert \times \vert \mathcal{E} \vert $ incidence matrix $H$, with entries defined as follows:
\begin{equation}
\label{eq:3}
h_{v, e}=\left\{
	\begin{aligned}
	1, \quad if \quad v \in e\\
	0, \quad if \quad v \notin e\\
	\end{aligned}\right
	.
\end{equation}
%
The degree of a node $ v \in \mathcal{V}$ is defined as $ d(v) =
\sum_{e \in \mathcal{E}} h_{v, e}$, and the degree of a hyperedge $e \in \mathcal{E}$ is defined
as $d(e) = \sum_{v \in \mathcal{V}} h_{v, e}$. The degree matrices $D_e$ and $D_v$ are constructed by setting all the edge degrees and all the vertex degrees as their diagonal entries,
respectively.

In this work, we consider the special case of $d(v)=1$ for all vertices, i.e.~, body joints are divided into $\vert \mathcal{E} \vert$ disjoint subsets, which is efficient in practice. Notably, the incidence matrix $H$ is equivalent to a partition matrix in this case.
Each row is a one hot vector denoting the group to which each joint belongs.


%画一个人体关键点的hypergraph实际例子，让人看得清楚！！！
% hypergraph部分要跟实际的例子结合


\begin{figure*}[ht]
  \centering

      \includegraphics[width=0.9\textwidth]{Hyperformer.pdf}
     \caption{Model architecture overview and illustration of our proposed HyperSA layer.}
     \label{fig:model}
\end{figure*}


\section{Method}
As analyzed in \cref{sec:intro}, multiple specific joints often move cooperatively in an action, \ie~there are inherent higher-order relations beyond the pair-wise relation. Therefore, we propose to introduce the prior information of the intrinsic hyper connections into vanilla Self-Attention. Specifically, a novel \textbf{Hypergraph Self-Attention (HyperSA)} layer is introduced, which makes Transformers aware of extra higher-order relations shared by a subset of joints connected to each hyperedge. 

\subsection{Deriving the hyperedge feature}
%  我这个其实是和sa是并列的关系，而不是真正改了那个attention，所以我把它作为Transformer结构上的改变

Given an incidence matrix $H$, we propose an effective approach to obtain the feature representation for each subset of joints connected to a hyperedge. 
Let $C$ denote the number of feature dimensions, individual joint features $X \in \mathbb{R}^{\vert \mathcal{V} \vert \times C}$ are first aggregated into subset representations $E \in \mathbb{R}^{ \vert \mathcal{E} \vert \times C}$ by the following rule:
\begin{equation}
\label{eq:4}
     E = D_e^{-1}H^\top XW_{e},
\end{equation}
% 解释一下HX是求和的含义，
% D_e-1 代表求平均
where:
\begin{itemize}
    \item The product of incidence matrix $H$ and input $X$ essentially sums up the belonging joint features of each subset. 
    \item The inverse degree matrix of hyperedges are multiplied for the purpose of normalization. 
    \item The projection matrix $W_{e} \in \mathbb{R}^{C\times C}$ further transforms the features of each hyperedge to obtain their final representations.
\end{itemize}
 % 3.2 和4.1 直接合起来写！！！
 
% 公式7没有信息量不要了，一句话表示
% Then we linearly transforms the features of each hyperedge to obtain their final representation via a shared projection matrix $W_{e} \in \mathbb{R}^{C\times C}$. 
% \begin{equation}
%     E = EW_{e},
% \end{equation}

% Finally, to update the features of each input joint, we propose to select their corresponding group relation representations and add the selected representations 
Then we construct an augmented hyperedge representation $E_{aug} \in \mathbb{R}^{ \vert \mathcal{V} \vert \times C}$ by assigning hyperedge representations to the position of each associated joint:
% \begin{equation}
%      E_{aug} = H^TE
% \end{equation}

% In summary, we compute the augmented hyperedge features as follows:
\begin{equation}
\label{eq:5}
    E_{aug} = HD_e^{-1}H^\top XW_{e}.
\end{equation}


% By stacking our proposed HORA, the modified SA in \cref{eq:5} and Temporal Convolution (TCN) layers one after another as shown in \cref{fig:arch}, we construct a Transformer model tailored for skeleton data, called High-Order-Relation-Aware (HORA) Transformer. By default, both the vanilla and our HORA Transformer model the spatial configuration of each frame individually. Thus 
% our work is orthogonal to approaches which study the incorporation of spatial nd temporal modelling for the application of Transformers on this task \cite{plizzari2021spatial, shi2020decoupled}. 
% 这个放到最后，先把结构讲完再讲training！！！
% 直接讲自己怎么学的
% empirical直接用来对比不用介绍！！！！！
% \subsection{Partition strategy}
% In this section, we present two ways to obtain a reasonable incidence matrix for our task, including adopting the empirical partition and learning the partition. 

% \subsubsection{Empirical partition}
% % the importance of symmetry
% Empirically, the human skeleton could be divided into a number of body parts. Several fine-grained partitions have been studied in previous work, and we adopt a common 6-way partition based on \cite{song2020stronger, wang2021iip}: head, trunk, upper left limb, upper right limb, lower left limb, and lower right limb. We also experiment with a 2-way partition which simply divides the body into upper and lower part. We experimentally show that our Hyperformer with both empirical partitions yield excellent performance. This confirms our assumption on the significance of high-order group inherent relation in human actions. However, finding an optimal empirical partition strategy is laborious and the optimal partition strategy is restricted to a certain skeleton with a fixed number of recorded joints. In this work, we also provide an approach to automate the search process for an effective partition strategy. 
% % The proposed method is then no longer restricted to human skeleton, but can also be easily adapted to related data such as animal skeleton.  no related work found for animal action recognition, no support for this claim
\subsection{Encoding human skeleton structure}
Human body joints are naturally connected with bones and form, together with the latter, a bio-mechanical model. In such a mechanical system, the movement of each joint in an action is strongly influenced by their connectivities. Therefore, it is beneficial to take the structural information of human skeleton into account.

Analogous to the established Relative Positional Embedding (RPE) for image \cite{wu2021rethinking} and language \cite{he2020deberta, shaw2018self} Transformer, we propose a powerful k-Hop Relative Positional Embedding $R_{ij} \in \mathbb{R}^C$, which is indexed from a learnable parameter table by the Shortest Path Distance (SPD) between the $i^{th}$ and $j^{th}$ joints. 
% discuss after the definition
In comparison to the learnable scalar spatial encoding in \cite{ying2021transformers}, it has larger capacity and interacts with the query additionally.



\subsection{Hypergraph Self-Attention}
With the obtained hyperedge representation and skeleton topology encoding, we now define our Hypergraph Self-Attention as follows:
\begin{align}
\label{eq:6}
\begin{split}
     A_{ij} = & \underbrace{\vec{q}_i \cdot \vec{k}_j^\top}_{\text{(a)}} + \underbrace{\vec{q}_i \cdot E_{aug, j}^\top}_{\text{(b)}} \\
     & + \underbrace{\vec{q}_i \cdot R_{\phi(i, j)}^\top}_{(c)} + \underbrace{\vec{u} \cdot E_{aug, j}^\top}_{(d)},
    \end{split}
\end{align}
where $\vec{u} \in \mathbb{R}^{C} $ is a learnable static key regardless of the query position. 
\begin{itemize}
    \item Term (a) alone is the vanilla SA, which represents joint-to-joint attention.
    \item Term (b) computes the joint-to-hyperedge attention between the $i^{th}$ query and the corresponding hyperedge of the $j^{th}$ key.
    \item Term (c) is the term for injecting the structural information of human skeleton with k-Hop Relative Positional Embedding. 
    \item Term (d) is intended for calculating the attentive bias of different hyperedges independent of the query position. It assigns the same amount of attention to each joint connected to a certain hyperedge.
\end{itemize}

Note that terms (a) and (b) can be combined by distributive law and require merely an extra step of matrix addition. Moreover, term (d) has $O(\vert \mathcal{V} \vert C^2)$ complexity and thus requires negligible computation in comparison to term~(a).

\noindent \textbf{Relational Bias}
Transformers assume the input tokens to be homogeneous, whereas human body joints are inherently heterogeneous, \eg each physical joint plays a unique role and thus has different relations to others.
% Such inherent relations persist in different
% actions regardless of the input joint coordinates or action class. 
Taking the heterogeneity of the skeleton data into account, we propose to represent the inherent relation of each joint pair as a scalar trainable parameter $B_{ij}$, called Relational Bias (RB).  It is added to the attention scores before aggregating the global information:
\begin{equation}
\label{eq:7}
\vec{y}_i = \sum_{j=1}^n (A_{ij} + B_{ij}) \vec{v}_j
,
\end{equation}

\subsection{Partition strategy}
\label{sec:learn}

\begin{figure}[]
  \centering
 %  \vfill
  \begin{subfigure}[t]{0.18\textwidth}
      \centering
      \includegraphics[width=0.75\textwidth]{6way.png}
      \caption{Empirical partition a.}
      \label{fig: 2}
  \end{subfigure}
      % \hfill
    \hspace{1.5cm}
  \begin{subfigure}[t]{0.18\textwidth}
      \centering
      \includegraphics[width=0.75\textwidth]{2way.png}
      \caption{Empirical partition b.}
      \label{fig:3}
  \end{subfigure}
  \hspace{0.6cm}
        % \hfill
  \begin{subfigure}[t]{0.18\textwidth}
      \centering
  \includegraphics[width=0.75\textwidth]{learned1.png}
      \caption{Learned partition a.}
      \label{fig:4}
  \end{subfigure}
        \hspace{1.5cm}
  \begin{subfigure}[t]{0.18\textwidth}
      \centering
  \includegraphics[width=0.75\textwidth]{learned2.png}
      \caption{Learned  partition b.}
      \label{fig:leg}
  \end{subfigure}
     \caption{Visualization of the empirical and learned partitions. Different node colors stand for different subgroups for each partition strategy.}
     \label{fig:2}
\end{figure}

Empirically, human skeletons could be divided into a number of body parts, which have been well studied in previous work~\cite{thakkar2018part, huang2020part, song2020stronger}.
% and we adopt a common 6-way partition based on \cite{song2020stronger, wang2021iip}: head, trunk, upper left limb, upper right limb, lower left limb, and lower right limb. We also experiment with a 2-way partition which simply divides the body into upper and lower part.
We experimentally show that our Hyperformer with empirical partitions yields excellent performance. 
% This confirms our assumption on the significance of high-order group inherent relation in human actions. 
However, finding an optimal empirical partition strategy is laborious and the optimal partition strategy is restricted to a certain skeleton with a fixed number of recorded joints. In this work, we also provide an approach to automate the search process for an effective partition strategy.


To make the partition matrix learnable, we parameterize and relax the binary partition matrix to its continuous version by applying a softmax along its column axis:
\begin{equation}
    \Tilde{H} = \{ \Tilde{h}_{ve} = \frac{{\rm exp} (h_{ve})} {\sum^{\vert \mathcal{E}\vert}_{e=1} {\rm exp} (h_{ve})}; i=1...\vert \mathcal{V} \vert, j=1...\vert \mathcal{E} \vert \}.
\end{equation}
The problem of finding an optimal discrete partition matrix $H$ is thus reduced to learning an optimal continuous partition matrix $\Tilde{H}$, which can be optimized jointly with Transformer parameters. 
% To facilitate the optimization, we add two additional constraints considering the characteristics of human body. 


% new experiments show that symmetry can be automatically learned (with different seeds as long as the number of epochs is large enough) and the performance is even better without smoothness regularization !!!

% \textbf{Smoothness regularization} Since physically connected joints are often involved in an action simultaneously, we propose a smoothness loss as follows, which encourages neighboring joints of human skeleton to be in the same relational group:
% \begin{equation}
%     L_{\rm smooth} = \sum^M_i \sum^N_j \sum^N_{k} |\Tilde{p}_{ij} - \Tilde{p}_{ik}| \cdot A_{jk},
% \end{equation}
% where $A_{jk} \in \mathbb{R}^{N\times N}$ denotes the natural connections of body joints, which is often referred to as the so-called adjacency matrix for GCNs. Note that it is important to apply l1 loss for the purpose of smoothness regularization, because it encourages $\Tilde{p}_{ij}$ to be piece-wise constant in the neighborhood of an individual joint. 

% \textbf{Symmetry constraint} Moreover, we observe that symmetry plays an important role in human action, e.g., left and right part of body often exhibit the same movement in parallel or in an alternate manner at jumping, running, etc. Therefore, we include such a hard-coded symmetry constraint by forcing the left and right joint pairs to share the same group parameter:
% \begin{equation}
%     \Tilde{p}_{\mathrm{left}} = \Tilde{p}_{\mathrm{right}}. 
% \end{equation}



At the end of the optimization, a discrete partition matrix can be obtained by applying an argmax operation along each row of $\Tilde{H}$:
\begin{equation}
    H = argmax(\Tilde{H}).
\end{equation}
Note that a number of different proposals can be easily acquired by varying the initialization of $\Tilde{H}$. We experimentally show that all the proposals prove to be reasonable. Interestingly, all the learned proposals are symmetric as shown in \cref{fig:2}, indicating that symmetry is an important aspect of inherent joint relations. 

\begin{figure*}[ht]
  \centering
      \includegraphics[width=0.65\textwidth]{attention_visualizaiton.pdf}
     \caption{Visualization of the attention scores for the action class "Jump Up". The directed edges represent the attention weights w.r.t.~the query joint of left wrist and range from light orange to dark red with the increase of the weights. The black edges stand for the bones and the joints are assigned different colors according to their connected hyperedges as in \cref{fig:2} (c). }
     \label{fig:attention}
\end{figure*}

\subsection{Model architecture}

We first revisit the architectural design of Transformers for skeleton data.
Then we built our Hyperformer based on our analysis. HyperSA is employed for spatial modeling of each frame and a lightweight convolutional module is adopted for temporal modeling, following the design of state-of-the-art models \cite{chi2022infogcn, chen2021channel} in this field. 

% \subsection{Revisiting the role of MLP layers}
% For Transformer models, we analyze that SA and MLP layers are essentially responsible for modeling inter- and intra-token relations, respectively. Unlike image patches or word tokens which are high dimensional or abstract, joint tokens merely represent three-dimensional input coordinates with limited information. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key for successful action recognition. We thus suggest to remove MLP layers for computation and memory reduction and show in \cref{sec:ablation} that MLP layers are indeed neglectable.

\noindent \textbf{Spatial Modeling}
We apply Layer Normalization (LN) before the multi-head HyperSA and add a residual connection to the output, following the standard Transformer architecture \cite{vaswani2017attention}.
Based on our analysis in \cref{sec:intro}, we further remove the Multi-Layer-Perceptron (MLP) layers. To introduce non-linearity, a ReLU layer is added after each block of spatial and temporal modeling modules instead.

% move to experiments section
% For Transformers, SA and MLP layers are essentially responsible for modeling inter- and intra-token relations, respectively. Unlike image patches or word tokens which are high dimensional or
% abstract, joint tokens merely represent three-dimensional input coordinates with limited information.
% This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key for successful action recognition. We experimentally demonstrate that MLP layers are neglectable for this task in \cref{sec: ffn}.  

\noindent \textbf{Temporal Modeling}
To model the temporal correlation of
the human pose, we adopt the Multi-Scale Temporal Convolution (MS-TC) module \cite{chen2021channel, liu2020disentangling, chi2022infogcn} for our final model. This module contains three convolution branches with a 1 × 1 convolution to reduce channel dimension, followed by different combinations of kernel sizes
and dilations. The outputs of convolution branches are
concatenated.



Hyperformer is constructed by stacking HyperSA and Temporal Convolution layers alternately as follows:
\begin{align}
    & z^{(l)} = \text{HyperSA}(LN(z^{(l-1)})) + z^{(l-1)} \\
    & z^{(l)} = \text{TemporalConv}(LN(z^{(l)})) + z^{(l-1)} \\
    & z^{(l)} = \text{ReLU}(z^{(l)})
\end{align}

% analyze why remove mlp etc.

% Plan:

% 1. hyper self-attention
%     1) group representation
%         empirical
%         learned
%     2) hyper self-attention

% 2. k-hop relative embedding

% 3. relaitonal bias


% 2. put the removing mlp, group conv/proj into Section Experiments/Preliminary analysis of Transformer

\section{Experiments}
In this section, we first compare our Hyperformer to  state-of-the-art approaches on skeleton-based human action recognition benchmarks and show the superior performance of our model. 
Then we conduct an ablation study for a deeper understanding of our proposed HyperSA. Finally, we evaluate our approach qualitatively by visualizing each component of HyperSA.

\subsection{Experimental settings}
\subsubsection{Datasets}
\label{data}
We evaluate our proposed Hyperformer on three commonly adopted public datasets NTU-RGB+D \cite{shahroudy2016ntu}, NTU-RGB+D120 \cite{liu2019ntu} and Northwestern-UCLA \cite{wang2014cross} which are briefly introduced in the following. 

% 尽量再重写！！！！！顺序什么再写！！！！！

\noindent
\textbf{NTU RGB+D} 
\cite{shahroudy2016ntu} is a widely used public dataset for skeleton-based human action recognition. It contains
56880 skeletal action sequences. 
% These action sequences were performed by 40 volunteers and 
% split into a total number of 60 categories. 
Each action sequence is conducted by one or two subjects and captured by three Microsoft Kinect-V2 depth sensors set at the same height but from different horizontal viewpoints simultaneously. 
% Each skeleton includes 3D coordinates for 25 human body joints.
There are two benchmarks for evaluation including Cross-Subject (X-Sub)
and Cross-View (X-View) settings. For the X-Sub, the training and test sets come from two disjoint sets of 20 subjects each. 
% They contain 40320 and 16560 samples respectively.
For X-View, the training set contains 37920 samples captured by the camera views 2 and 3, and the test set includes 18960 sequences captured by camera view 1. 
% Note that there are 302 wrong samples that should be ignored.

\noindent
\textbf{NTU RGB+D 120} \cite{liu2019ntu} is an extension of NTURGB+D dataset with 57367 additional skeleton sequences over 60 additional action classes.
% , consisting of a total number of 113945 samples over 
% 120 action classes which are performed by 106 volunteers.
It is currently the largest available dataset with 3D joint annotations for human action recognition and contains 32 setups, each of which represents a different location and background. Two benchmark evaluations were suggested by the author including Cross-Subject (X-Sub) and Cross-Setup (X-Setup) settings. 

% For Cross-subject (X-sub), the dataset is divided into two subsets according to the subjects' identities. Half of the 106 subjects are used for training and the other for testing. For Cross-setup (X-setup), samples with even setup IDs are used as training data, and samples with odd setup IDs are used for testing. 532 bad samples should be ignored
% for this dataset.

\noindent
\textbf{Northwestern-UCLA} \cite{wang2014cross} dataset is
recorded by three Kinect sensors from different viewpoints. It includes 1494 video sequences of 10 action categories. 
% Each action is performed by 10 different actors.

% The author proposed to construct training set with samples from the first two cameras, and testing set
% with samples from the third camera.

\subsubsection{Implementation details}
\label{implement}
All experiments are conducted with the PyTorch \cite{paszke2019pytorch} deep learning library.
We train the model for a total number of 140 epochs with standard cross-entropy loss.
The learning rate is initialized to 0.025 and reduced at 110 and 120 epochs by 0.1, basically following the strategy in \cite{chi2022infogcn}.
For NTU RGB+D and NTU RGB+D 120, the batch size is set to 64, each sample is resized to 64 frames, and we adopt the code of \cite{zhang2020semantics} for data pre-processing.
For Northwestern-UCLA, we use a batch size of 16, and
follow the data pre-processing in \cite{cheng2020skeleton,chen2021channel}.
Our code is based on the official implementations of \cite{touvron2021training}, \cite{chen2021channel} and \cite{zhang2020semantics}. We employ a model with a total number of 10 layers and 216 hidden channel dimensions for all the experiments.


\begin{table*}[ht]
  \centering
    \caption{Action classiﬁcation performance on the NTU RGB+D and NTU RGB+D 120 dataset. Following the common setup, we report results using 4 modalities for a fair comparison. InfoGCN \cite{chi2022infogcn} reported results using extra MMD losses and we mark them with *.}
    \resizebox{0.86\textwidth}{!}{
  \begin{tabular}{@{}l|lccccc@{}}
    \toprule
   \multirow{2}*{Type} & \multirow{2}*{Methods} & \multirow{2}*{Parameters(M)} &\multicolumn{2}{c}{NTU RGB+D 60} &  \multicolumn{2}{c}{NTU RGB+D 120} \\
         &   &  & X-Sub(\%) & X-View(\%)    &  X-Sub(\%)  & X-Set(\%)   \\
    \midrule
    % \multirow{3}*{RNN} & HBRNN \cite{du2015hierarchical} & 59.1 & 64.0 \\
    %   & P-LSTM \cite{shahroudy2016ntu} & 62.9 & 70.3 \\
        %   &    Ensemble TS-LSTM \cite{lee2017ensemble} & 74.6 & 81.3 \\
     \multirow{2}*{RNN} & VA-LSTM \cite{zhang2017view} & - & 79.4 & 87.6 & - & - \\
    &  AGC-LSTM \cite{si2019attention} & 22.9M & \underline{89.2} &  \underline{95.0} & - & - \\ 
    \hline
    %  \multirow{3}*{CNN} & Res-TCN \cite{kim2017interpretable} & 74.3 & 83.1 \\
     % \multirow{3}*{CNN} & HCN \cite{li2018co} & - & 86.5 & 91.1 & - & - \\
       \multirow{2}*{CNN} & VA-CNN (aug.) \cite{zhang2019view} & 24.09M & 88.7 & 94.3 & - & - \\
      & Ta-CNN+ \cite{xu2021topology} & 1.06M & \underline{90.7} & \underline{95.1} & \underline{85.7} & \underline{87.3} \\
      \hline
    \multirow{4}*{GCN}   
    
      & Shift-GCN \cite{cheng2020skeleton} & 2.8M & 90.7 & 96.5 & 85.9 & 87.6\\
      & DC-GCN+ADG \cite{cheng2020decoupling} & 4.9M & 90.8 & 96.6 & 86.5 & 88.1 \\ 
       & MS-G3D \cite{liu2020disentangling}  & 2.8M & 91.5 & 96.2  & 86.9 & 88.4  \\
      
       & MST-GCN  \cite{chen2021multi} & 12.0M& \underline{91.5} & \underline{96.6} & \underline{87.5} & \underline{88.8}\\
    
       \hline
        \multirow{4}*{\shortstack{Hybrid Model \\ (
        GCN + Att})} 
         % & SGN \cite{zhang2020semantics} & 0.7M & 89.0 & 94.5 & 79.2 & 81.5 \\
       % & 2s-AGCN \cite{shi2019two}  & 6.9M & 88.5 & 95.1 & 82.9 & 84.9  \\
    
       % & PA-ResGCN-B19 \cite{song2020stronger}  &3.6M & 90.9  & 96.0 & 87.3  & 88.3  \\
       & Dynamic GCN \cite{ye2020dynamic}  & 14.4M &  91.5 & 96.0 & 87.3 & 88.6 \\
       & EfficientGCN-B4 \cite{song2021constructing} & 2.0M & 91.7 & 95.7 & 88.3 & 89.1  \\
      % & CTR-GCN \cite{chen2021channel} & 1.5M &  92.4 & 96.8 & 88.9 & 90.6 \\
            & InfoGCN (Joint Only) \cite{chi2022infogcn} & 1.6M & 89.8* & 95.2* & 85.1*(84.3) & 86.3* \\
       & InfoGCN \cite{chi2022infogcn} & 1.6M & \underline{92.7*} & \underline{\textbf{96.9*}} &\underline{89.4*}(89.1) & \underline{90.7*} \\
   \hline
   \multirow{4}*{Transformer} 
    & ST-TR \cite{plizzari2021spatial} & 12.1M & 89.9 & 96.1    & 82.7 & 84.7 \\
   & DSTA \cite{shi2020decoupled} & 4.1M & 91.5 & 96.4 & 86.6 & 89.0\\
%   & IIP-Transformer \cite{wang2021iip} & 92.3 & 96.4 \\
%   &  Hyperformer(Joint) & 2.6M &   &  &  &  \\
% \hline
& Hyperformer (Joint Only) & 2.6M 
& 90.7 & 95.1 & 86.6 & 88.0
\\
% by extending the training epoch from 120 to 140, the motion modality has been improved, but the joint modality is unchanged, 
% this saves the effort for ablation

     &   \textbf{Hyperformer} & 2.6M & \underline{\textbf{92.9}}  & \underline{96.5} & \underline{\textbf{89.9}} & 
     % cset increased by 0.1, csub unchanged for extending 10 epochs training
     \underline{\textbf{91.3}} \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:ntu}
\end{table*}

\begin{table}[h]
  \centering
    \caption{Action classiﬁcation performance on the Northwestern-UCLA dataset.}
    \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{@{}l|lcc@{}}
    \toprule
       \multirow{1}*{Type} & \multirow{1}*{Methods} 
          &Acc (\%)  \\
    \midrule
   \multirow{2}*{RNN} &
        TS-LSTM \cite{lee2017ensemble} & 89.2 \\
       &      2s-AGC-LSTM \cite{si2019attention} & \underline{93.3} \\
        \hline 
                  \multirow{2}*{CNN} 
        & VA-CNN (aug.) \cite{zhang2019view} & 90.7 \\
         & Ta-CNN \cite{xu2021topology} & \underline{96.1} \\
         \hline
          \multirow{3}*{GCN} &
      4s-shift-GCN \cite{cheng2020skeleton} & 94.6 \\
      & DC-GCN+ADG \cite{cheng2020decoupling} & 95.3 \\ 
       % & CTR-GCN \cite{chen2021channel} & 96.5 \\
        & InfoGCN \cite{chi2022infogcn} (*with MMD losses)
        & \underline{96.6*} \\
        \hline
           \multirow{1}*{Transformer} & \textbf{Hyperformer} & 
    
           % already used 150 epochs for ucla
           % can only try to change the decay steps 
              % Done. No improvement, 96.557, same after rounding!

            % try the last time with halved learning rate,
            % just found infogcn/ctrgcn adopt halved lr for uclar, might help\
            % waiting for results
            % halved lr becomes worse

            % moving decay epochs from 120&130 backward to 110&120 helps a lot! so decaying at 110&120 is the best strategy for all the datasets now

           % average score of 3 times run
           \underline{\textbf{96.7}} \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:ucla}
\end{table}
\subsection{Comparison with state-of-the-art approaches}
Following most recent state-of-the-art approaches \cite{cheng2020skeleton, ye2020dynamic, chen2021channel, chen2021multi}, we adopt a multi-stream fusion strategy, \ie,
there are 4 streams which take different modalities including \textbf{joint}, \textbf{bone}, \textbf{joint motion} and \textbf{bone motion} as input respectively. Joint modality refers to the original skeleton coordinates; bone modality represents the differential of spatial coordinates; joint motion and bone motion modalities use the differential on
temporal dimension of joint and bone modalities, respectively. The softmax scores
of 4 streams are added to obtain the fused score. 

% Note that DSTA \cite{shi2020decoupled} also fuses 4 streams but the modalities are different from the above mentioned common modalities. Moreover, we only list results of InfoGCN \cite{chi2022infogcn} using 4 instead of 6 modalities for a fair comparison.

The comparison on the three datasets is shown in \cref{tab:ntu} and \cref{tab:ucla}, respectively. As shown in \cref{tab:ntu}, our model reaches state-of-the-art results on all benchmarks, except the Cross-View Setup of NTU RGB+D dataset. Our Hyperformer performs slightly worse than InfoGCN \cite{chi2022infogcn}. Nevertheless, the latter relies on two additional MMD loss terms and a number of associated hyperparameters, including loss coefficients, noise ratio and z prior gain. NTU RGB+D 120 is a more challenging dataset, on which the results of previous Transformer-based approaches are not satisfying. On the contrary, our model achieves the best results among all model categories. This conforms to the findings that attention mechanism benefits more from a large amount of data than Convolution \cite{dosovitskiy2021an, touvron2021training}. The Northwestern-UCLA dataset is particularly challenging since it contains much fewer training samples, making it even harder for Transformer-based models to compete. With the prior knowledge of bone connectivity and underlying high-order relations of joints, our Hyperformer still yields state-of-the-art results in such a low-data regime.









\subsection{Ablation study}
In this section, we revisit the role of MLP layers for this task and compare Hyperformer with the vanilla Transformer to show the effectiveness of our proposed model. We also analyze the contribution of each component of our HyperSA. In addition,
we compare our learned partition strategy with empirical ones to show its effectiveness.
For the ablation study, all experiments
are conducted on the
% to save time, do ablation on the smaller ntu60!!!!
X-sub benchmark of NTU RGB+D using the joint modality as input only.


% \subsubsection{The role of MLP layers}
% \begin{table}[h]
%  \caption{Ablation study on MLP layers.}
% %   \caption{Effect of FFN layers.}
%   \centering
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     Model (MS-TC included) & \multicolumn{1}{c}{MLP} & Parameters & Acc(\%) \\
%     \midrule
%     % Baseline &  - &    84.8 \\
%     % \hline
%     \multirow{2}*{Transformer}  & \checkmark &   &  \\
%       & -  &  &  \\
%       \hline
%     % &   & upper body, lower body  & 85.5 \\
%       \multirow{2}*{Hyperformer} & \checkmark &  &  \\
%         & - &  &  \\
%         %  &  &  2 & 85.5 \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:ffn}
% \end{table}
% For Transformer models, SA and FFN layers are essentially responsible for modeling inter- and intra-token relations, respectively. Unlike image patches or word tokens which are high dimensional or abstract, joint tokens merely represent three-dimensional input coordinates with limited information. This implies that the modeling of inter-token relations, or the so-called joint co-occurrences, is the key for successful action recognition. We thus suggest to remove MLP layers for computation and memory reduction and show in \cref{tab:ffn} that MLP layers are indeed neglectable. 


% 标题换一下，更详细准确一些，放到最前面
\subsubsection{The design of Hyperformer}
\label{sec:design}
Before replacing standard SA layers with our HyperSA, we removed the MLP layers based on the results in \cref{tab:construct}. As can be seen, our HyperSA layers contribute most to the final performance, with an significant improvement of $2.8\%$ absolute accuracy. The MS-TC module further improves over vanilla TC by $0.4\%$.
% When we increase model width from the total number of 384 to 576 channels, we observe a further improvement. The resulting model is still much smaller than the vanilla baseline in \cref{tab:ffn}.  
In \cref{tab:construct}, it can be seen that our Hyperformer achieves significantly higher accuracy than the vanilla baseline with fewer parameters thanks to the listed design choices. We provide more detailed results in \cref{tab:more}, validating the effectiveness of each individual HyperSA components. 


% no need to mention group conv in tcn, which is trival and just implementation details

% it may be better to directly say we replace FFN with our proposed HORA layer

% \begin{table}[h]
%   \caption{Constructing a baseline Transformer.}
%   \centering
%   \begin{tabular}{@{}llccc@{}}
%     \toprule
%     Model & \multicolumn{1}{c}{Parameters}  & Num. of Layers & Num. of Channels & Num. of Heads & Acc(\%) \\
%     \midrule
%     \multirow{3}*{Transformer} 
%       & vanilla   & & \\
%           &  remove FFN & & \\
%              &  remove heads aggregation & & \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:constraint}
% \end{table}
% % group conv
% % remove mlp


\begin{table}[h]
\caption{Constructing Hyperformer from the vanilla baseline. Note that the adopted MS-TC module for our final model has fewer parameters than vanilla Temporal Convolution (TC) due to the dimension reduction via 1x1 convolutions, see \cref{fig:model}}
  \centering
  \resizebox{0.45\textwidth}{!}{
  \begin{tabular}{@{}lccc@{}}
    \toprule
   Model  & \multicolumn{1}{c}{Parameters} & FLOPs  & Acc(\%) \\
    \midrule
       \multirow{1}*{SA + MLP + TC} & 7.2M & 25.6G &  88.3 \\
        SA + TC & 3.6M & 14.1G & 87.5 \\
            %  4.1M &  \\
            %   \hline
        \multirow{1}*{HyperSA + TC}   & 4.1M &  16.7G & 90.3 \\
          \multirow{1}*{HyperSA + MS-TC}   & 2.6M & 14.8G & 90.7 \\
            %  && &   & 6.6M & 85.7 \\
            %  &  &             \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:construct}
\end{table}



\begin{table}[h]
\caption{The effectiveness of the HyperSA components.}
  \centering
  \resizebox{0.4\textwidth}{!}{
  \begin{tabular}{@{}lccc@{}}
    \toprule
   Model  &  Acc(\%) \\
    \midrule
        SA + TC  & 87.5 \\
            %  4.1M &  \\
            %   \hline
            % \midrule
        \multirow{1}*{SA + TC + Joint-to-hyperedge attn}     & 89.6 \\
         \multirow{1}*{SA + TC + K-Hop RPE}   &    89.5 \\
           \multirow{1}*{SA + TC + Hyperedge Attentive Bias}    &  89.6  \\
               \multirow{1}*{Full HyperSA + TC}   &   90.3 \\
            %  && &   & 6.6M & 85.7 \\
            %  &  &             \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:more}
\end{table}



% \begin{table}[h]
%   \caption{Effect of HORA layer with different partition strategies.}
%   \centering
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     Model & \multicolumn{1}{c}{Partition Strategy} & Acc(\%) \\
%     \midrule
%     % Baseline &  - &    84.8 \\
%     % \hline
%     \multirow{3}*{HORA \；Transformer}  & Uni-group & 85.5 \\
%       & \multirow{1}*{Body Part}  & 85.4 \\
%     % &   & upper body, lower body  & 85.5 \\
%       & \multirow{1}*{Learned}  & 85.7 \\
%         %  &  &  2 & 85.5 \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:partition}
% \end{table}


% \begin{table}[h]
%   \caption{Effect of constraints for learning the partition matrix.}
%   \centering
%   \begin{tabular}{@{}llccc@{}}
%     \toprule
%     Model & \multicolumn{1}{c}{Partition Strategy}  & Number of Subsets & Acc(\%) \\
%     \midrule
%     \multirow{3}*{HORA Transformer} 
%       & Learned   & & \\
%           &  + smoothness regularization & & \\
%              &  + symmetry & & \\
%     \bottomrule
%   \end{tabular}
%   \label{tab:constraint}
% \end{table}


\subsubsection{Effect of different partition strategies}
\begin{table}[h]
  \centering
  \caption{The effect of different partition strategies}
  \resizebox{0.3\textwidth}{!}{
  \begin{tabular}{@{}lcc@{}}
    \toprule
    Partition Strategy & Acc(\%) \\
    \midrule
    % Baseline &  - &    84.8 \\
    % \hline
    %  Uni-group & 85.5 \\
        \multirow{1}*{Body Parts}  & 90.5  \\
    Upper and Lower Body  &  90.4 \\
        \multirow{1}*{Learned a}  & 90.7 \\
        % it also reaches 90.7 after 150 epochs
          \multirow{1}*{Learned b}  & 90.7 \\
        %  &  &  2 & 85.5 \\
    \bottomrule
  \end{tabular}
  }
  \label{tab:partition}
    \end{table}
In \cref{tab:partition}, we compared empirical partitions according to body parts (see \cref{fig:2}), and our learned partitions. Note that we obtain different partition proposals when the model is trained with different seeds. All proposals prove to be effective.
Overall, Hyperformer delivers stable performance and achieves the best result with a learned partition using the approach described in \cref{sec:learn}.

\subsection{Qualitative results}
In order to showcase the effectiveness of our approach, we visualize the attention scores of our HyperSA as well as the four decomposed terms in \cref{eq:6} at the first layer in \cref{fig:attention}. More specifically, we draw the attention scores  w.r.t.~the query joint of left wrist for two single frames respectively. The directed edges represent the attention weights and range from light orange to dark red with the increase of attention score. The black edges stand for the bones and the joints are assigned different colors according to their connected hyperedges.  

At Frame $t$, the person stands straight and starts to swing the hands, preparing to jump up. 
The joint-to-joint attention is distracted by a large number of joints, whereas the joint-to-group attention concentrates on the upper body. As the sum of the four terms, the final attention reasonably focuses on the hands and neck.

At Frame $t+2$, the person bends the knees and leans the upper body forward to squeeze the leg muscles. Although the joint-to-joint attention is successfully attached to the feet and waist, the knees and heels are less valued. This incomplete and unstable attention is unavoidable due to the pairwise mechanism. However, our proposed joint-to-group attention solves this issue by exploiting the underlying group relation.  

\section{Discussion}
\noindent\textbf{Broader Impact. }
Skeleton-based action recognition is widely studied due to its computation efficiency and robustness against the change of lighting conditions, compared to video-based action recognition. Moreover, skeleton-data better preserves the privacy since the identities of human subjects are removed. %, such that skeleton-based action recognition can be applied even in privacy sensitive scenarios. %and has therefore a special advantage regarding privacy protection.

\noindent\textbf{Limitations.}
\label{limit}
Our paper focuses on making Transformers fit for skeleton-based action recognition and specially considers the intrinsic nature of skeleton data. Therefore, our proposed Hyperformer may not be advantageous for other tasks, but the idea of taking inherent relations into account suggests the potential to better utilize Transformers. 
\section{Conclusion}

In this work, we successfully incorporate the information of skeletal structure into Transformer by proposing a relative positional embedding based on graph distance. This is a more elegant solution than previous hybrid models. Moreover, we identified a limitation of graph models for the task of skeleton-based action recognition, i.e., high-order joint relations are ignored.
Therefore, we propose a novel HyperSA layer to make Transformer models aware of these inherent relations. 
% To avoid laborious manual search of the appropriate group relations, we also introduce an approach to let our model automatically learn them. 
The resulting model called Hyperformer establishes the state-of-the-art performance.

\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
