---
title: >-
  《Temporal Query Networks for Fine-grained Video Understanding》阅读笔记
date: 2023-02-19
tags: [动作识别,论文笔记,Fine-Grained Action Recognition]
categories: [视频理解]
---



CVPR 2021

# 摘要

本文的目标是对未裁剪视频中的动作进行细粒度分类，其中动作可以在时间上扩展，也可以只跨越视频的几帧。将其转换为查询-响应机制，其中每个查询处理特定的问题，并拥有自己的响应标签集。

贡献：

1. 提出了一个新的模型—时态查询网络（TQN)—它支持查询-响应功能，以及对细粒度操作的结构理解
2. 提出了一种新的方法-随机特征库更新-在不同长度的视频上训练网络，并使用响应细粒度查询所需的密集采样
3. 将TQN与其他体系结构和文本监督方法进行比较，分析其优缺点
4. 在FineGym和Diving48基准上广泛评估细粒度动作分类的方法并仅使用RGB特征超越最先进的方法

# Temporal Query Networks



![Temporal Query Network](https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230218195704209.png)

时间查询网络(TQN)在未修剪的视频中快速识别发生的区分性事件(只跨越几帧)，并且可以在只有弱监督的情况下进行训练，即没有事件的时间位置或持续时间信息。它通过学习一组置换不变的查询向量来实现这一点，这些查询向量对应于关于事件及其属性的预定义查询，使用Transformer[56]解码器层将其转换为响应向量，并从3D卷积网络主干中提取视觉特征。图2给出了模型的概述。视觉骨干和TQN解码器描述如下。

## Query–Attributes 查询属性

查询集query set：
$$
\mathcal{Q}=\left\{q_{i}\right\}_{i=1}^{K}
$$
其中每一个查询$q_i$都有一个相关的属性集：
$$
\mathcal{A}_{i}=\left\{a_{1}^{i}, a_{2}^{i}, \ldots, a_{n_{i}-1}^{i}, \varnothing\right\}
$$
由$q_i$的响应的可接受值$a_j^i$组成

例如，在跳水视频中，查询可以是圈数，属性集为可能的计数{0.5,1.0,2.5};或者在体操中，查询可以是项目类型，属性集为{vault, floor-exercise, balanced beam}

## 视觉 backbone

给定一个未修剪的视频，使用3D ConvNet提取8帧连续不重叠剪辑的第一个视觉特征:
$$
\boldsymbol{\Phi}=\left(\Phi_{1}, \Phi_{2}, \ldots, \Phi_{t}\right)
$$
其中$t$是剪辑片段的总数，$\Phi_{i} \in \mathbb{R}^{d}$是d维剪辑级视觉特征。

在整个视频中密集地提取特征有两个原因：

1. 避免了引起时间混叠，也避免了丢失快速事件(只跨越几帧)
2. 从完整视频中选择片段进行分类是次优的，因为这些事件的位置是未知的

## TQN decoder

给定剪辑级特征和标签查询，TQN解码器为每个查询输出一个响应。具体而言，对于每一个标签查询$q_i$，学习一个矢量$\mathbf{q}_{i} \in \mathbb{R}^{d_{q}}$，通过对视觉特征的关注产生一个响应矢量$\mathbf{r}_{i} \in \mathbb{R}^{d_{q}}$。然后将每个响应矢量$\mathbf{r}_{i}$独立线性分类到相应的属性集$\mathcal{A}_{i}$中。

## 训练

通过反向传播将来自视觉编码器和TQN解码器的模型参数与属性分类器$\Psi_{i}$进行端到端的联合训练。

这个训练的loss是一个单个分类器损失的多任务组合，是属性集$\mathcal{A}_{i}$上对数$\Psi_{i} \cdot \mathbf{r}_{i}^{(M)}$上的Softmax交叉熵损失$\mathcal{L}_{C E}$
$$
\mathcal{L}_{\text {total }}=\sum_{i=1}^{K} \mathcal{L}_{C E}^{(i)}\left(a^{i}, \Psi_{i} \cdot \mathbf{r}_{i}^{(M)}\right)
$$
其中$a_i$是标签查询$q_i$的groud-truth属性。

 本质上，TQN解码器学习建立查询向量和相关视觉特征之间的时间对应关系以生成响应。由于查询向量本身是学习的，它们被优化为“专家”，可以在未修剪的时间特征流中定位相应的事件。

## Discussion: TQN and DETR

DETR[4]是最近提出的一种基于Transformer的目标检测模型，同样采用非自回归并行解码一次性输出目标检测。然而，有三个关键的区别：

1. DETR对象查询都是等价的-因为它们的输出都指定了相同的“标签空间”(对象类和它们的RoI)，本质上查询是学习位置编码。相比之下，TQN查询具有不同的语义，具有对应事件类型和属性的语义;它们的输出响应向量每个指定一组不同的属性，属性的数量依赖于查询。
2. 由于TQN响应与这些查询绑定，它们可以在直接监督属性标签的情况下进行训练，从而避免了DETR中使用的预测和真实值之间的训练时间 Hungarian Matching[33]。
3. TQN没有时间上的定位监督，而DETR训练有提供(空间)位置。因此，尽管TQN的任务是(隐式)检测事件，但它是在更弱的监督下完成的。

# 随机更新特征库 Stochastically Updated Feature Bank

对整个未修剪的视频输入帧进行密集的时间采样是检测时间位置未知的快速判别事件的关键。但是问题在于GPU显存的限制，无法在每次训练迭代中转发密集采样的帧。作者使用特征内存库来克服这些限制。

记忆库缓存clip级别的3D卷积网络视觉特征。对于给定的视频，clip特征$\boldsymbol{\Phi}=\left(\Phi_{1}, \Phi_{2}, \ldots, \Phi_{t}\right)$可以被相互独立提取。缓存库是由预训练的3D卷积网络中提取的所有训练视频的片段特征初始化的。然后在每次训练迭代中，固定数量的$n_{online}$个随机采样clip通过视觉编码器计算得到，而剩下的$(r-n_{online})$个clip特征则从缓存库中获得。然后将两组视觉特征组合并输入TQN解码器进行最终预测和反向传播以更新模型参数。最后，将在线计算得到的记忆库中的clip特征替换为在线特征。在推理过程中，所有的特征都是在没有缓存库的情况下在线计算的。

![image-20230219134722818](https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230219134722818.png)

# 将类别分解为属性查询

在本节中，作者演示了通常与细粒度视频识别数据集相关的预定义的N个类别$\mathcal{C}=\left\{c_{1}, c_{2}, \ldots, c_{N}\right\}$集合如何被分解为属性查询。在这些数据集中，类别在微妙的细节上有所不同，例如特定类型、持续时间或特定事件序列的数量。这些事件可能是快速发生(持续时间短)，但时间位置和持续时间未知。

![image-20230219150729217](https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230219150729217.png)