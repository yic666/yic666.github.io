<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这篇博文参考李沐读论文系列中视频理解的相关内容 # 双流网络 前言 视频本身是一个很好的数据来源，比2D的单个图像包含更多信息，比如物体移动的信息，以及长期的时序信息，包括音频信息。  image-20220913100303892  这篇论文是第一篇利用深度学习进行视频理解效果与之前的手工特征方法效果相当的方法。 双流网络顾名思义就是使用了两个卷积神经网络，如下图所示：  im">
<meta property="og:type" content="article">
<meta property="og:title" content="视频理解相关论文">
<meta property="og:url" content="http://example.com/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:description" content="这篇博文参考李沐读论文系列中视频理解的相关内容 # 双流网络 前言 视频本身是一个很好的数据来源，比2D的单个图像包含更多信息，比如物体移动的信息，以及长期的时序信息，包括音频信息。  image-20220913100303892  这篇论文是第一篇利用深度学习进行视频理解效果与之前的手工特征方法效果相当的方法。 双流网络顾名思义就是使用了两个卷积神经网络，如下图所示：  im">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913100303892.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913101016813.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913104904429.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913110726700.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913140943884.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913151055074.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917200215637.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917201204252.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918150440864.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918151315556.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918165944620.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918201122804.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918210327820.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919193757565.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195350007.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195832134.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115102912016.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115103836603.png">
<meta property="og:image" content="f:/Master/研究生课程/模式识别/image-20221115104452852.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115110443121.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115141919372.png">
<meta property="og:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115142838755.png">
<meta property="og:image" content="c:/Users/16353/AppData/Roaming/Typora/typora-user-images/image-20221115145706200.png">
<meta property="og:image" content="c:/Users/16353/AppData/Roaming/Typora/typora-user-images/image-20221115150223140.png">
<meta property="og:image" content="c:/Users/16353/AppData/Roaming/Typora/typora-user-images/image-20221115151025142.png">
<meta property="og:image" content="c:/Users/16353/AppData/Roaming/Typora/typora-user-images/image-20221115151432639.png">
<meta property="og:image" content="c:/Users/16353/AppData/Roaming/Typora/typora-user-images/image-20221115151938926.png">
<meta property="article:published_time" content="2022-09-12T14:20:46.000Z">
<meta property="article:modified_time" content="2023-03-13T04:26:18.096Z">
<meta property="article:author" content="Yic-gdut">
<meta property="article:tag" content="动作识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913100303892.png">


<link rel="canonical" href="http://example.com/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/","path":"2022/09/12/视频理解相关论文/","title":"视频理解相关论文"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>视频理解相关论文 | Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yic</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E5%92%8C%E5%BC%95%E8%A8%80"><span class="nav-number">2.</span> <span class="nav-text">摘要和引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">双流网络结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4%E6%B5%81"><span class="nav-number">3.1.</span> <span class="nav-text">空间流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E9%97%B4%E6%B5%81"><span class="nav-number">3.2.</span> <span class="nav-text">时间流</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%8F%A0%E5%8A%A0%E5%85%89%E6%B5%81%E5%9B%BE"><span class="nav-number">3.3.</span> <span class="nav-text">如何叠加光流图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="nav-number">3.4.</span> <span class="nav-text">实现细节</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA%E5%92%8C%E6%80%BB%E7%BB%93"><span class="nav-number">4.</span> <span class="nav-text">结论和总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#i3d"><span class="nav-number"></span> <span class="nav-text">I3D</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">1.</span> <span class="nav-text">相关工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82-1"><span class="nav-number">2.</span> <span class="nav-text">实现细节</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95inflated"><span class="nav-number">2.1.</span> <span class="nav-text">如何Inflated</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#bootstrapping"><span class="nav-number">2.2.</span> <span class="nav-text">Bootstrapping</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#deep-video"><span class="nav-number"></span> <span class="nav-text">Deep Video</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">1.</span> <span class="nav-text">方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">2.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#beyond-short-snippets"><span class="nav-number">3.</span> <span class="nav-text">Beyond Short Snippets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#convolutional-two-stream-network-fusion-for-video-action-recognition"><span class="nav-number">4.</span> <span class="nav-text">Convolutional Two-Stream Network Fusion for Video Action Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#spatial-fusion"><span class="nav-number">4.1.</span> <span class="nav-text">Spatial Fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E5%93%AA%E5%90%88%E5%B9%B6"><span class="nav-number">4.2.</span> <span class="nav-text">在哪合并</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-fusion"><span class="nav-number">4.3.</span> <span class="nav-text">Temporal fusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tsn"><span class="nav-number">5.</span> <span class="nav-text">TSN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-segment"><span class="nav-number">5.1.</span> <span class="nav-text">Temporal Segment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A5%BD%E7%94%A8%E7%9A%84%E6%8A%80%E5%B7%A7"><span class="nav-number">5.2.</span> <span class="nav-text">好用的技巧</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#d%E7%BD%91%E7%BB%9C%E7%B3%BB%E5%88%97"><span class="nav-number"></span> <span class="nav-text">3D网络系列</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#c3d"><span class="nav-number">1.</span> <span class="nav-text">C3D</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#non-local"><span class="nav-number">2.</span> <span class="nav-text">Non-local</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#r21d"><span class="nav-number">3.</span> <span class="nav-text">R(2+1)D</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#slowfast"><span class="nav-number">4.</span> <span class="nav-text">SlowFast</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#video-transformer"><span class="nav-number"></span> <span class="nav-text">Video transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#timesformer"><span class="nav-number">1.</span> <span class="nav-text">Timesformer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number"></span> <span class="nav-text">总结</span></a></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="视频理解相关论文 | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          视频理解相关论文
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-09-12 22:20:46" itemprop="dateCreated datePublished" datetime="2022-09-12T22:20:46+08:00">2022-09-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-13 12:26:18" itemprop="dateModified" datetime="2023-03-13T12:26:18+08:00">2023-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>这篇博文参考李沐读论文系列中视频理解的相关内容 # 双流网络</p>
<h2 id="前言">前言</h2>
<p>视频本身是一个很好的数据来源，比2D的单个图像包含更多信息，比如物体移动的信息，以及长期的时序信息，包括音频信息。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913100303892.png" alt="image-20220913100303892" /><figcaption aria-hidden="true">image-20220913100303892</figcaption>
</figure>
<p>这篇论文是第一篇利用深度学习进行视频理解效果与之前的手工特征方法效果相当的方法。</p>
<p>双流网络顾名思义就是使用了两个卷积神经网络，如下图所示：</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913101016813.png" alt="image-20220913101016813" /><figcaption aria-hidden="true">image-20220913101016813</figcaption>
</figure>
<p>视频相当于连续的多张图片，因此早期的工作将视频的关键帧抽取出来，一个个输入卷积网络，最后以Early Fasion或者Late Fasion的方式合并结果，达到一种时空学习的效果，但效果不如手工设置特征。因此，双流网络的作者认为卷积网络适合学习局部特征，而不适合学习物体的移动信息（Motion-information)，于是作者使用了光流法来获取物体的移动信息，以学习光流和动作分类的映射。作者把双流网络分为空间流神经网络和时间流神经网络，空间流的输入为单帧图像，而时间流的输入为一系列光流图片。</p>
<h2 id="摘要和引言">摘要和引言</h2>
<p>在这篇论文中，作者研究了如何使用深度卷积网络实现视频的动作识别。主要问题的难点在于如何同时学习两种信息：一种是从静止的的图像上学到信息，比如物体的形状大小颜色，以及整体的场景信息；另一种是物体之间的移动信息，或者说是视频中的时序信息。这篇论文主要有三个贡献：双流网络、能取得很好效果的在少量数据上训练的光流网络、muliti-task learning。</p>
<p>视频天生就能提供一种数据增强，因为在视频中，同一个物体会有多种形变以及遮挡和光照的改变，是一种非常自然的数据增强。</p>
<h2 id="双流网络结构">双流网络结构</h2>
<p>空间流学习空间特征，时间流学习运动特征，最后使用late fusion合并结果。可以通过简单的加权平均，也可以训练一个多分类的线性SVM。</p>
<h3 id="空间流">空间流</h3>
<p>空间流神经网络的输入就是一帧帧的图像，其实就是个图像分类的任务。静止的信息本身就是个有用的信息，因为很多动作与识别出的物体是紧密相关的。因此单独空间流网络已经能达到很好的效果了，而且还能使用Imagenet去预训练。</p>
<h3 id="时间流">时间流</h3>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913104904429.png" alt="image-20220913104904429" /><figcaption aria-hidden="true">image-20220913104904429</figcaption>
</figure>
<p>（a）和（b）为视频中的前后两帧，从这两帧可以得到如（c）的光流，在数学上光流实际上就是一个在平面上的向量，因此得到分为（d）水平方向和（e）垂直方向两个方向的可视化结果。从维度上看，假如输入为<span class="math inline">\(w*h\)</span>的RGB图像也就是输入维度是<span class="math inline">\(w*h*3\)</span>，而得到的光流图维度则为<span class="math inline">\(w*h*2\)</span>，2代表两个方向。</p>
<h3 id="如何叠加光流图">如何叠加光流图</h3>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913110726700.png" alt="image-20220913110726700" /><figcaption aria-hidden="true">image-20220913110726700</figcaption>
</figure>
<p>第一种方式就是简单直接地将得到的光流图堆叠在一起，每次叠加的都是同样位置的光流特征，这种方式不需要更多的预处理，但没有充分利用光流的信息。第二种则是根据光流的轨迹，在轨迹上进行数值的叠加。另外，作者也使用了双向光流的方法。</p>
<h3 id="实现细节">实现细节</h3>
<p>事实上，两个流网络都可以看作是Alexnet的变体都是五层卷积加两层全连接，训练部分都是标准的操作。</p>
<ol type="1">
<li><p>测试部分：</p>
<p>无论视频多长，都是等间距抽取25帧。对每一帧都做ten crop，即这一帧及其翻转都取四个边角以及中间部分，也就是会有10个crop，即最后会有250帧。由于视频的测试方式多种多样，因此不同的测试方式的比较不公平。</p></li>
<li><p>光流处理：</p>
<p>使用在GPU上运行的算法去获得光流（0.06s)，耗时大。而光流的另一个问题是这种密集光流所占的空间大，而这篇论文把光流图resize，并以jpeg的形式存储，使得从1.5TB降到27GB。但即使如此，光流法所占的时间和空间成本都是相当高的。</p></li>
</ol>
<h2 id="结论和总结">结论和总结</h2>
<p>这篇论文提出了一个具有竞争力的深度视频分类模型，包含了基于卷积神经网络的独立空间和时间识别流， 是视频理解的开山之作。</p>
<p>这篇论文给予了一种使用先验知识作为网络输入的思路来解决单神经网络不能解决的问题，提供了一种多流网络的思路，另外双流网络可以当作是一种多模态学习的先例。</p>
<h1 id="i3d">I3D</h1>
<p>从论文标题可以看出这篇论文的两个重要贡献： I3D模型（Inflated 3D ConvNet）、Kinetics数据集。I3D直译为扩散的3D卷积神经网络，扩散的意思就是把原有的2D模型“扩散”到3D。</p>
<h2 id="相关工作">相关工作</h2>
<p>在2D的图像分类领域已经有了主导的神经网络结构，如VGG和Resnet，而视频领域至今依旧没有主导的架构。当时主要有以下几种选择，要么是纯2D网络，后面再接上比如LSTM这样的操作，从而对时间进行建模，要么是加上光流使得网络具备对运动信息的建模能力，要么则是直接使用3D网络来学习Special Temporal 的时空特征 。</p>
<p>而这篇论文提出的I3D（Two-Stream Inflated 3D ConvNets），有两个关键词：双流和inflated。由于3D网络的参数量过于巨大，而且也没有合适足够的数据去训练，因此网络不能太深（C3D）。作者在使用了其提出来的inflated操作直接就能使用例如Inception、VGG、Resnet这样的网络，而且能使用这些网络预训练的参数作为初始化。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913140943884.png" alt="image-20220913140943884" /><figcaption aria-hidden="true">image-20220913140943884</figcaption>
</figure>
<p>图a的方法就是卷积神经网络跟一个LSTM，把这个视频问题当作是一个图像分类问题，一张一张图片输入到卷积神经网络去做特征提取，接着把这些特征输入到LSTM网络。</p>
<p>图b的方法就是非常暴力的3D卷积网络，直接将视频段当成一个Valume，输入到网络之中。这样的网络能进行时空学习，而且卷积核是三维的，因此参数量变得很大。</p>
<p>图c的方法就是上文介绍过的双流网络。</p>
<p>图d的方法是将b和c的方法结合在一起，与双流网络一样，分为空间流和时间流，而不同于双流网络使用的Late Fasion去融合结果，而是使用Early Fasion，在没出结果之前，就把两个特征先融合在一起，然后使用一个3D卷积神经网络去处理得到分类结果。</p>
<p>以上四种方法作者都在其提出的数据集上进行了测试，然后提出双流I3D网络，即图e。</p>
<h2 id="实现细节-1">实现细节</h2>
<h3 id="如何inflated">如何Inflated</h3>
<p>简单来说，就是直接暴力地把把一个2D网络转换为3D网络。就是简单的把2D的kernel变成3D的kernel，2D的pooling层变成3D的pooling层。这样就可以不用自己设置网络，直接使用之前的2D网络。比如最新的Timesformer也是从Vit从2D inflated 到3D</p>
<h3 id="bootstrapping">Bootstrapping</h3>
<p>Bootstrapping字面意思就是引导，就是如何从以及训练好的2D模型出发去初始化一个3D模型，然后继续训练以得到更好的效果。如何用 2D CNN 的预训练权重来初始化一个 3D CNN。用同样一张图片，反复复制粘贴，变成一个boring的视频，然后只要保证这个无聊的视频输入到3D CNN的输出和图片输入到2D CNN的输出一致，就能保证初始化是正确的。具体的做法就是把所有的2D filters在时间维度上复制了n次， 得到一个n帧的视频，为了保持输出的一致，除了 2D 参数复制n次之外，还需将参数除以n。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220913151055074.png" alt="image-20220913151055074" /><figcaption aria-hidden="true">image-20220913151055074</figcaption>
</figure>
<h2 id="总结">总结</h2>
<p>这篇论文通过一系列的实验证明了在大规模数据集作预训练后，再做迁移学习能得到很好的效果。另外，在网络结构的设计上，作者也认为本文并没有进行全面的探索，像 action tubes、attention mechanism 这些结构都没有进行尝试。这些都是未来研究中不错的跟进方向。</p>
<p>这篇论文从两个方面上解决了训练的问题，一方面是没有数据也能使用Inflated的操作转换得到3D网络，并且能使用预训练的2D网络的参数去初始化3D网络，如P3D、R(2+1)D以及TimesFormer等；而另一方面，如果从头设计一个3D网络，这篇论文提供了一个足够大的数据集k 400，而且不需要依赖于ImageNet预训练的参数，如slowfast、X3D、mvit等工作都是从头开始训练的。自此，视频理解领域的后续研究工作就可以从更多角度展开了。</p>
<h1 id="deep-video">Deep Video</h1>
<p>CVPR 2014</p>
<h2 id="方法">方法</h2>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917200215637.png" alt="image-20220917200215637" /><figcaption aria-hidden="true">image-20220917200215637</figcaption>
</figure>
<ol type="1">
<li>Single Frame 其实就是一个图片分类的任务，单帧输入到卷积神经网络，不包含时间信息和视频信息</li>
<li>Late Fusion 和 Early Fusion 上文介绍过</li>
<li>Slow Fusion 在网络的过程中，对特征进行合并</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220917201204252.png" alt="image-20220917201204252" /><figcaption aria-hidden="true">image-20220917201204252</figcaption>
</figure>
<p>由于效果不好，作者尝试了图像领域的多分辨率卷积，将输入分成两部分（原图和正中间的抠图）。这种网络可以理解成一种双流网络，但双流的权值是共享的；也可以理解说是一种早期的注意力机制，这样的操作使得网络更关注图片中心的区域。</p>
<h2 id="总结-1">总结</h2>
<p>本文的主要贡献并不是在于任务性能的提升（实际也并没有提升），而是在于进行了深度 CNN 网络在视频理解领域的初步探索。在本文之后，大量的使用深度神经网络处理视频数据的工作涌现出来，视频理解领域正式进入深度学习时代。</p>
<p># 双流网络系列</p>
<p>## 原始双流网络改进方向</p>
<ol type="1">
<li><p>Late fusion 可否更换为Early Fusion，如果能先做空间和时间流的特征交互，应该效果会变好</p></li>
<li><p>原始双流网络使用的是一种基于Alexnet的变体，采用VGG、Inception Net、ResNet等更好的backbone是否会得到更好的效果，如何在小数据集上去训练大模型，如何控制好过拟合问题？</p></li>
<li><p>原始双流网络是直接把图片通过卷积神经网络的特征直接用做分类，而众所周知，RNN或LSTM可以很好的处理时序数据，或许可以使用LSTM处理卷积神经网络抽取的特征，得到更强的特征。</p></li>
<li><p>如何做长时间的视频理解？原始双流网络处理的视频都是非常短的，远小于一般一个动作的时间。</p></li>
</ol>
<h2 id="beyond-short-snippets">Beyond Short Snippets</h2>
<p>CVPR 2015</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918150440864.png" alt="image-20220918150440864" /><figcaption aria-hidden="true">image-20220918150440864</figcaption>
</figure>
<p>这篇论文的题目中的Short Snippets指的就是2-3s的视频段，基本方法如上图，如果按照双流网络的思想，原始的输入只有几帧，处理的视频是很短的，为了处理长的视频，得找到合适的方法对抽取的特征做pooling，比如简单的Max pooling或Average pooling，这篇文章对这些pooling做了详细的研究，类似Deep video做了late pooling等，最后的结果conv pooling效果最好，也尝试了使用LSTM做特征融合，而最后的结果提升有限。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918151315556.png" alt="image-20220918151315556" /><figcaption aria-hidden="true">image-20220918151315556</figcaption>
</figure>
<p>上图c代表卷积神经网络对每个视频帧抽取的特征，且这些卷积神经网络权值共享，接着使用五层LSTM去处理抽取的视频特征，最后使用softmax做分类。</p>
<p>正常来说LSTM在处理时序信息应该是会得到更好的效果的，但在处理短视频时，视频在时间上整个画面几乎没有变化，比较连续，也就是说输入到LSTM的时序特征没有变化，LSTM也就学不到什么东西。</p>
<h2 id="convolutional-two-stream-network-fusion-for-video-action-recognition">Convolutional Two-Stream Network Fusion for Video Action Recognition</h2>
<p>CVPR 2016</p>
<p>这篇论文详细的讲如何做双流网络的合并，如何在时间流和空间流之中做Early Fusion。作者从三个角度展开了研究：一是如何进行空间维度的特征融合（Spatial Fusion），二是如何进行时间维度的特征融合（Temporal Fusion)，三是应该在哪一层进行特征融合。</p>
<h3 id="spatial-fusion">Spatial Fusion</h3>
<p>如何保证时间流和空间流的特征图在同样的位置产生的通道response可以联系起来，在特征图层面做合并，作者做了以下尝试：</p>
<ol type="1">
<li><p>Max Fusion：对于a和b两个不同的特征图，只取最大值作为合并后的特征图</p></li>
<li><p>Concatenation fusion：将两个分支中对应位置的值拼接起来</p></li>
<li><p>Conv fusion：将两个分支的值送入到一个卷积，得到结果</p></li>
<li><p>Bilinear fusion：在每个位置上计算两特征的矩阵外积在哪合并</p></li>
</ol>
<h3 id="在哪合并">在哪合并</h3>
<p>作者针对在哪一层合并，做了大量的消融实验，得到了两种效果比较好的方式，如下图。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918165944620.png" alt="image-20220918165944620" /><figcaption aria-hidden="true">image-20220918165944620</figcaption>
</figure>
<p>第一种是空间流和时间流分别做，然后到conv4之后，进行一次fusion，在模型中间已经二合一，成为一流的网络。而第二种则是单独做到conv5以后，把空间流和时间流的特征做一次合并，得到一个spatial temporal 的特征，然后继续保持空间流的完整性，在最后做一次合并。</p>
<h3 id="temporal-fusion">Temporal fusion</h3>
<p>当有很多视频帧，每一帧都有抽取了特征，如何在时间轴上把这些特征合并起来，得到最后的特征。作者尝试了两种方式：3D Pooling和3D Conv + 3D Pooling</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918201122804.png" alt="image-20220918201122804" /><figcaption aria-hidden="true">image-20220918201122804</figcaption>
</figure>
<p>上图是作者提出的整体框架，蓝色是空间流，绿色是时间流，同样先分别用这两个网络去抽取RGB和光流图像的特征，抽取好特征之后就使用上文所说的Fusion方法，使用3D Conv + 3D Pooling的方法聚合特征，通过一个FC层，最后计算Spatiotemporal的损失函数。由于在视频处理中，时序信息很重要，因此作者单独把时间流拿出来再做一个3D pooling，最后做一个针对时间上的损失函数。 也就是最后的框架包含了两个分支，最后同样使用late fusion的方式得到分类结果。</p>
<h2 id="tsn">TSN</h2>
<p>eccv 2016</p>
<p>这篇论文提出一种简单的方式处理比较长的视频，并且确定了很多好用的技巧。</p>
<h3 id="temporal-segment">Temporal Segment</h3>
<p>这篇论文处理长视频的思路非常简单，就是把视频分成多段。具体来说，假如把视频分为三段，每一段随机抽取一帧，并以这一帧作为起点去选几帧计算光流图像，输入双流网络，每一段都如此处理，但权值共享。把每一个段出来的logits做一个consensus，也就是融合（乘法，加法，max，average或者lstm）每个段出来的分类结果，最后把时间流和空间流的结果做late fusion。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220918210327820.png" alt="image-20220918210327820" /><figcaption aria-hidden="true">image-20220918210327820</figcaption>
</figure>
<p>Temporal Segment 的思想不仅可以用在这种短视频这种裁剪好的video clip上，也可以运用在没裁剪过的长视频，就算分的段落的含义不一样，只要在consensus上不使用average的方式，比如使用lstm去模拟时间的走势。而且这种思想还可以用在弱监督和无监督的工作上。</p>
<h3 id="好用的技巧">好用的技巧</h3>
<ol type="1">
<li>Cross Modality Pre-training：</li>
</ol>
<p>Modality指的就是图像和光流。作者提出了也使用Imagenet预训练参数来对光流分支初始化，而问题就是RGB图片的通道数和光流的通道数不对应，无法直接参数初始化。而本文认为，直接把三通道的权重取平均，直接复制20份就行了。而实验结果证明了这样做的可行性。</p>
<ol start="2" type="1">
<li><p>Regularization Techniques：</p>
<p>BN是一种广泛运用的正则化方法，但由于视频数据集太小会导致过拟合。本文提出了partial BN，只打开第一层的BN来适应新的输入，后面的全部冻住不动以避免过拟合。</p></li>
<li><p>Data Augmentation</p>
<p>为了防止过拟合，数据增强是一个必不可少的工具。作者提出了两个东西：corner croppong和scale-jittering。前者强制对边角处的图像进行裁剪；后者是对图片随机裁剪，得到不同长宽比的图片。</p></li>
</ol>
<h1 id="d网络系列">3D网络系列</h1>
<p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/126238074">深度学习时代的视频理解综述_Adenialzz的博客</a></p>
<h2 id="c3d">C3D</h2>
<p>ICCV-2015</p>
<p>本文是早期探索 3D CNN 在视频理解领域的工作之一。做法非常简单，就是简单的搭了一个深度的 3D 卷积神经网络。与再之前的工作的差异在于使用了大型的数据集（Sports 1M），并使用了更深的网络结构。</p>
<p>文章给出的模型结构图非常简单，就是几层卷积和池化的堆叠，只是其中卷积核是 3D 的： <span class="math inline">\(3\times 3\times3\)</span> 。值得一提的是，作者最终的做法是将 fc6 得到的 4096 维的特征直接用 SVM 来做分类，又快效果又好。本文被称为 C3D 也是指的这个 fc6 得到的 4096 维的特征，被称为 C3D 特征。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919193757565.png" alt="image-20220919193757565" /><figcaption aria-hidden="true">image-20220919193757565</figcaption>
</figure>
<p>之前提到，C3D 的性能在同期工作中并不突出，那为什么这篇工作有如此大的影响力呢？实际上，这篇工作的主要贡献有两点。一是实现深度 3D CNN 模型，并在大型数据集上进行训练，证明了在视频领域，3D CNN 比 2D CNN 再融合（DeepVideo）的效果要更好。二是作者指明并提供了抽特征（而非微调）的研究方式。在当时的年代，想要用大型的深度 3D CNN 模型跑大型的数据集不是每一个实验室都能做到的，作者提供了一个接口，上传视频，返回 4096 维的 C3D 特征。这就方便后续的研究者们根据大型 3D CNN 提取的特征，再进行下游任务的研究。这无疑极大地推动了当时视频领域的研究进程。因此，除了方法上有令人眼前一亮的新意之外，如果能够提供一些资源（如大数据集或大预训练模型），能够推动整个领域的研究进程，也是值得被铭记的工作。</p>
<h2 id="non-local">Non-local</h2>
<p>CVPR-2017</p>
<p>之前在双流网络系列，我们介绍过使用 LSTM 来建模时序信息。在 2017 年，NLP 领域发生了一件大事，Transformer 横空出世。我们知道，Transformer 的核心就是 self-attention 自注意力模块，它能够对长距离的序列信息进行建模。在之后，基于 Transformer 的模型几乎横扫了 NLP 领域和近两年的 CV 领域。这篇 2018 年的工作，就是将自注意力机制的 Non-local 算子，引入到了 CNN 模型中。并且为了适配视频任务，本文的 Non-local 算子是时空（spacetime）维度的。除了视频任务之外，本文也测试了 Non-local 算子在检测/分割等其他视觉任务上的性能。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195350007.png" alt="image-20220919195350007" /><figcaption aria-hidden="true">image-20220919195350007</figcaption>
</figure>
<p>上图就是用于时空的non-local算子，这实际上就是注意力机制，下面三个类似QKV，QK相乘得到注意力矩阵，再于V相乘，并有个残差操作。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220919195832134.png" alt="image-20220919195832134" /><figcaption aria-hidden="true">image-20220919195832134</figcaption>
</figure>
<p>消融实验体现了作者的设计思路。第一步是注意力怎么算，第二步是non-local用在哪一层，第三步是加多少个non-local，第四个就是证明spacetime上的自注意力机制的重要性。表格g探索了 non-local 在长视频（128帧）上的性能，可以看到提升也是很显著的。</p>
<h2 id="r21d">R(2+1)D</h2>
<p>CVPR 2018</p>
<p>这篇文章对时空的卷积做了详尽的实验，对各种2D+3D的卷积网络结构进行分析。最后的结论是把3D拆分成时间上的1D和空间上的2D能得到更好的效果。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115102912016.png" alt="image-20221115102912016" /><figcaption aria-hidden="true">image-20221115102912016</figcaption>
</figure>
<p>作者尝试了以下五种 2D/3D 卷积结构，来处理视频数据：</p>
<ol type="a">
<li>R2D：纯 2D 卷积，对每一帧单独的提取图像特征；</li>
<li>MCx：先 3D 卷积处理视频输入，再用计算开销更小的 2D 卷积继续提取特征；</li>
<li>rMCx：先 2D 卷积处理各帧图像输入，再用 3D 卷积对时空特征进行建模；</li>
<li>R3D：纯 3D 卷积，与 C3D、I3D 类似，这里的 R3D 也是 CVPR-2018 的一篇工作，是将 2D 的 ResNet 转换到 3D；</li>
<li>R(2+1)D：将 3D 拆分为空间维度的 2D 卷积和时间维度的 1D 卷积</li>
</ol>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115103836603.png" alt="image-20221115103836603" /><figcaption aria-hidden="true">image-20221115103836603</figcaption>
</figure>
<p>从参数量的角度上看，R2D明显参数量少，但效果不行，而3D和2D混合的方法都能达到50左右，最后论文提出的R（2+1）D在参数量与R3D一样的情况下，依然效果好了很多。</p>
<figure>
<img src="F:\Master\研究生课程\模式识别\image-20221115104452852.png" alt="image-20221115104452852" /><figcaption aria-hidden="true">image-20221115104452852</figcaption>
</figure>
<p>就是将一个<span class="math inline">\(d t\times d\times d\)</span> 的 3D 卷积，拆分为一个 2D 的空间维度上的 $ 1dd$ 的卷积，和一个 1D 的时间维度上的 <span class="math inline">\(t\times 1\times 1\)</span> 的卷积。中间进行了一次投射 <span class="math inline">\(M_i\)</span>,可以保持与纯 3D 卷积的参数量相当，偏于公平的对比。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115110443121.png" alt="image-20221115110443121" /><figcaption aria-hidden="true">image-20221115110443121</figcaption>
</figure>
<p>上图展示了训练和测试过程的错误，把R3D和R（2+1）D做了对比， 结果展示无论是18层的浅层网络还是34层的深层网络，R（2+1)D的训练和测试误差都要比R3D低 。</p>
<p>R(2+1)D 取得最优效果，作者给出了两点解释：</p>
<ol type="1">
<li>将 3D 网络拆分为 R(2+1)D ，网络经过了更多的非线性激活函数，整体模型的非线性增强，从而表达能力更强；</li>
<li>R(2+1)D 网络将一整个 3D 网络拆分开，整个模型更容易训练。如上图，明显看到 R(2+1)D 网络收敛得更快更好。</li>
</ol>
<h2 id="slowfast">SlowFast</h2>
<p>ICCV-2019</p>
<p>这篇论文是把精度和效率结合的比较好的，研究动机来源于人眼中分别处理静态图像的场景信息和动态图像的运动信息的两种不同细胞，于是借鉴双流网络的结构，提出有 Slow Pathway 和 Fast Pathway 两支网络分别处理静态信息和动态信息的SlowFast网络。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115141919372.png" alt="image-20221115141919372" /><figcaption aria-hidden="true">image-20221115141919372</figcaption>
</figure>
<blockquote>
<p>蓝色部分是 Slow Pathway，它用来处理静态的场景信息，它的特点是输入小，模型大。假设我们有一共 64 帧视频，这里按照每隔 16 帧取一帧，可以得到 4 帧作为输入（见图），但是它的模型每一层的通道数是比较大的（见表），从而这一分支中模型整体参数量比较大，这也对应着人眼中处理静态场景信息的 p 细胞占比较大。</p>
<p>绿色部分是 Fast Pathway，它用来处理动态的运动信息，它的特点是输入大，模型小。同样 64 帧的视频，每隔 4 帧取一帧，共有 16 帧输入（见图），但是它每层的通道数比较小（见表），整体参数量较小，对应着 m 细胞占比较小。</p>
</blockquote>
<p>最后这个两分支的网络，在这两分支之间还使用了late connection的方式结合起来，所以也就是双流之间的信息是可交互的，从而能够更好地学习到这个时空特征，最后达到一个速度和精度的结合。</p>
<figure>
<img src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20221115142838755.png" alt="image-20221115142838755" /><figcaption aria-hidden="true">image-20221115142838755</figcaption>
</figure>
<p>从网络看，快慢分支都是I3D，除了通道数不同，都是残差网络。从output size看，在时序部分都不做下采样，始终保持输入的时间长度。</p>
<h1 id="video-transformer">Video transformer</h1>
<h2 id="timesformer">Timesformer</h2>
<p>ICML-2021</p>
<p>这篇论文探索了如何把图像的transformer迁移到视频。</p>
<figure>
<img src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115145706200.png" alt="image-20221115145706200" /><figcaption aria-hidden="true">image-20221115145706200</figcaption>
</figure>
<figure>
<img src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115150223140.png" alt="image-20221115150223140" /><figcaption aria-hidden="true">image-20221115150223140</figcaption>
</figure>
<p>具体来说，作者探索了如上五种结构。</p>
<p>Space Attention：即只在当前帧的空间维度计算自注意力，相当于只用 ViT 处理当前帧；(S) Joint Space-Time Attention：在时间所有帧和空间所有像素计算自注意力，无疑一旦帧数或者分辨率高了，显存是不够的；(ST) Divided Space-Time Attention：现在时间维度计算自注意力，再在空间维度计算自注意力，类似 R(2+1)D 的设计；(T+S) Sparse Local Global Attention：时间上在所有帧计算自注意力，但是在每一帧中只计算一个局部小窗口的元素，然后再扩大窗口范围再计算自注意力，而非直接计算全部像素的自注意力，类似 Swin Transformer；(L+G) Axial Attention：分别在时间、宽度、长度三个维度上计算自注意力(T+W+H)</p>
<figure>
<img src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151025142.png" alt="image-20221115151025142" /><figcaption aria-hidden="true">image-20221115151025142</figcaption>
</figure>
<p>作者在这五种结构上做了消融实验，结果表明JST和DST的效果比其他三种要好，但JST的显存占用过大。</p>
<figure>
<img src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151432639.png" alt="image-20221115151432639" /><figcaption aria-hidden="true">image-20221115151432639</figcaption>
</figure>
<h1 id="总结-2">总结</h1>
<figure>
<img src="C:\Users\16353\AppData\Roaming\Typora\typora-user-images\image-20221115151938926.png" alt="image-20221115151938926" /><figcaption aria-hidden="true">image-20221115151938926</figcaption>
</figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" rel="tag"># 动作识别</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/" rel="prev" title="视频行为识别博客笔记">
                  <i class="fa fa-chevron-left"></i> 视频行为识别博客笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/22/PoseConv3d%E7%AC%94%E8%AE%B0/" rel="next" title="PoseConv3d笔记">
                  PoseConv3d笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
