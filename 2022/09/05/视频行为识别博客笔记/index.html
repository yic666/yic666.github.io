<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="博客原文地址：Deep Learning for Videos: A 2018 Guide to Action Recognition (qure.ai) 动作识别以及难点 1.巨量的计算资源 一个简单的用于分类101类的卷积二维网络只有5M个参数，而同样的结构膨胀到3D结构时，会产生33M个参数。 2.需考虑上下时刻场景 动作识别需包含跨帧获取时空信息 3.设计分类网络结构 需">
<meta property="og:type" content="article">
<meta property="og:title" content="视频行为识别博客笔记">
<meta property="og:url" content="http://example.com/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:description" content="博客原文地址：Deep Learning for Videos: A 2018 Guide to Action Recognition (qure.ai) 动作识别以及难点 1.巨量的计算资源 一个简单的用于分类101类的卷积二维网络只有5M个参数，而同样的结构膨胀到3D结构时，会产生33M个参数。 2.需考虑上下时刻场景 动作识别需包含跨帧获取时空信息 3.设计分类网络结构 需">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://blog.qure.ai/assets/images/actionrec/2stream_high.png">
<meta property="og:image" content="https://blog.qure.ai/assets/images//actionrec/c3d_high.png">
<meta property="og:image" content="https://blog.qure.ai/assets/images//actionrec/trial.gif">
<meta property="og:image" content="https://blog.qure.ai/assets/images/actionrec/fstcn_high.png">
<meta property="og:image" content="https://blog.qure.ai/assets/images/actionrec/tsn_high.png">
<meta property="og:image" content="https://blog.qure.ai/assets/images/actionrec/actionvlad.png">
<meta property="article:published_time" content="2022-09-05T07:17:56.000Z">
<meta property="article:modified_time" content="2023-01-10T13:18:58.174Z">
<meta property="article:author" content="Yic-gdut">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="动作识别">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.qure.ai/assets/images/actionrec/2stream_high.png">


<link rel="canonical" href="http://example.com/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/","path":"2022/09/05/视频行为识别博客笔记/","title":"视频行为识别博客笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>视频行为识别博客笔记 | Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yic</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB%E4%BB%A5%E5%8F%8A%E9%9A%BE%E7%82%B9"><span class="nav-number">1.</span> <span class="nav-text">动作识别以及难点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E6%A6%82%E8%BF%B0"><span class="nav-number">2.</span> <span class="nav-text">方法概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9Fcv%E6%96%B9%E6%B3%95"><span class="nav-number">2.1.</span> <span class="nav-text">传统CV方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">2.2.</span> <span class="nav-text">深度学习方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E6%B5%81%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.1.</span> <span class="nav-text">单流网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8C%E6%B5%81%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.2.</span> <span class="nav-text">双流网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93"><span class="nav-number">3.</span> <span class="nav-text">论文总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#lrcnlong-term-recurrent-convolutional-network"><span class="nav-number">3.1.</span> <span class="nav-text">LRCN（Long-term
Recurrent Convolutional Network）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE"><span class="nav-number">3.1.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A"><span class="nav-number">3.1.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E9%99%B7"><span class="nav-number">3.1.3.</span> <span class="nav-text">缺陷</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#c3d"><span class="nav-number">3.2.</span> <span class="nav-text">C3D</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-1"><span class="nav-number">3.2.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-1"><span class="nav-number">3.2.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%BA%E9%99%B7-1"><span class="nav-number">3.2.3.</span> <span class="nav-text">缺陷</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#note"><span class="nav-number">3.2.4.</span> <span class="nav-text">Note：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E7%BB%B4%E5%8D%B7%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">3.3.</span> <span class="nav-text">三维卷积+注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-2"><span class="nav-number">3.3.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-2"><span class="nav-number">3.3.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-number">3.3.3.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#twostreamfusion"><span class="nav-number">3.4.</span> <span class="nav-text">TwoStreamFusion</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-3"><span class="nav-number">3.4.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-3"><span class="nav-number">3.4.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95-1"><span class="nav-number">3.4.3.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tsn"><span class="nav-number">3.5.</span> <span class="nav-text">TSN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-4"><span class="nav-number">3.5.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-4"><span class="nav-number">3.5.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95-2"><span class="nav-number">3.5.3.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#actionvlad"><span class="nav-number">3.6.</span> <span class="nav-text">ActionVLAD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-5"><span class="nav-number">3.6.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-5"><span class="nav-number">3.6.2.</span> <span class="nav-text">解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hiddentwostream"><span class="nav-number">3.7.</span> <span class="nav-text">HiddenTwoStream</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-6"><span class="nav-number">3.7.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-6"><span class="nav-number">3.7.2.</span> <span class="nav-text">解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95-3"><span class="nav-number">3.7.3.</span> <span class="nav-text">算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#i3d"><span class="nav-number">3.8.</span> <span class="nav-text">I3D</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-7"><span class="nav-number">3.8.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-7"><span class="nav-number">3.8.2.</span> <span class="nav-text">解释</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#t3d"><span class="nav-number">3.9.</span> <span class="nav-text">T3D</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E8%B4%A1%E7%8C%AE-8"><span class="nav-number">3.9.1.</span> <span class="nav-text">关键贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A7%A3%E9%87%8A-8"><span class="nav-number">3.9.2.</span> <span class="nav-text">解释</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/09/05/%E8%A7%86%E9%A2%91%E8%A1%8C%E4%B8%BA%E8%AF%86%E5%88%AB%E5%8D%9A%E5%AE%A2%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="视频行为识别博客笔记 | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          视频行为识别博客笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-09-05 15:17:56" itemprop="dateCreated datePublished" datetime="2022-09-05T15:17:56+08:00">2022-09-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-01-10 21:18:58" itemprop="dateModified" datetime="2023-01-10T21:18:58+08:00">2023-01-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>博客原文地址：<a
target="_blank" rel="noopener" href="https://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review#sec-2">Deep
Learning for Videos: A 2018 Guide to Action Recognition
(qure.ai)</a></p>
<h1 id="动作识别以及难点">动作识别以及难点</h1>
<p>1.巨量的计算资源</p>
<p>一个简单的用于分类101类的卷积二维网络只有5M个参数，而同样的结构膨胀到3D结构时，会产生33M个参数。</p>
<p>2.需考虑上下时刻场景</p>
<p>动作识别需包含跨帧获取时空信息</p>
<p>3.设计分类网络结构</p>
<p>需设计能够捕获时空信息的架构</p>
<p>4.没有标准的<strong>benchmark</strong></p>
<h1 id="方法概述">方法概述</h1>
<h2 id="传统cv方法">传统CV方法</h2>
<p>基本可汇总为以下三步：</p>
<p>1.针对视频明显特征区域做提取，提取为密集向量或稀疏的兴趣点集合。（这一步之前一直是人工提取，后来提出的iDT算法改善了该流程）</p>
<p>2.提取的特征转化为固定尺寸的向量，来描述该视频的内容。这一步最流行的做法是Bag
of visual words</p>
<p>3.定义分类器，根据提取的视频特征向量，选定分类器，做分类训练和预测</p>
<h2 id="深度学习方法">深度学习方法</h2>
<p>3D卷积于2013年被用于动作识别，且无需其他的输入作为帮助。而2014年有两篇突破性的研究论文被发表。</p>
<h3 id="单流网络">单流网络</h3>
<p>https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf</p>
<p>这篇论文使用2D预训练卷积以探索多种方法来融合连续帧的时间信息</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/Karpathy_fusion.jpg"
alt="Karpathy_fusion" />
<figcaption aria-hidden="true">Karpathy_fusion</figcaption>
</figure>
<p>如上图,视频的所有连续帧都作为不同设置的输入。 <em>Single frame</em>
使用单一的网络架构在最后阶段融合所有帧的信息;<em>Late
fusion</em>融合使用两个共享参数的网络，间隔15帧，并在最后结合预测。<em>Early
fusion</em>在第一层通过卷积将超过10帧进行融合。 <em>Slow
fusion</em>涉及多个阶段的融合，兼顾早期和晚期融合之间的平衡。</p>
<p>不过与基于人工标定特征的方法相比,效果不好,作者推断有以下问题:一是学习到的时空特征没有捕捉到运动特征;二是由于数据集的多样性较低，学习如此详细的特征非常困难</p>
<h3 id="双流网络">双流网络</h3>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/2stream_high.png"
alt="2 stream architecture" />
<figcaption aria-hidden="true">2 stream architecture</figcaption>
</figure>
<p>https://arxiv.org/pdf/1406.2199.pdf</p>
<p>这篇论文在上文单流网络的基础上,设计一个获取动作特征的光流模型。这样就形成了双流模型，一个负责获取空间信息，一个负责获取时间信息。</p>
<p>尽管该方法取得了不错的效果，但还是有以下几个缺点： 一
视频的预测还是依据从视频中抽取的部分样本。对于长视频来说，在特征学习中还是会损失时序信息。
二
在训练时，从视频中抽取片段样本时由于是均匀抽取，这样会有错误标签的现象（即指定动作并不存在该样本片段中）。
三 在光流使用前，需要对视频预先做光流的抽取操作。</p>
<h1 id="论文总结">论文总结</h1>
<p>以下论文在某种程度上是两篇论文(单流和两流)的演变。而围绕这些论文反复出现的方法可以总结如下，所有的论文都是基于这些基本观点的即兴创作。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/recurrent_theme_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="lrcnlong-term-recurrent-convolutional-network">LRCN（Long-term
Recurrent Convolutional Network）</h2>
<p>https://arxiv.org/abs/1411.4389</p>
<h3 id="关键贡献">关键贡献</h3>
<ol type="1">
<li>基于之前的工作使用RNN来代替基于流的设计</li>
<li>用于视频表示的编码器架构的扩展</li>
<li>动作识别的端到端可训练架构</li>
</ol>
<h3 id="解释">解释</h3>
<p>在这之前有利用CNN对视频片段做特征提取，然后再用LSTM对时序的特征做最终分类，但效果不好。而LRCN是在卷积块(编码器)之后使用LSTM块(解码器)即端到端训练，同时也使用RGB和光流都作为输入，并将预测结果加权相加得到好的效果。</p>
<figure>
<img
src="https://raw.githubusercontent.com/yic666/Blogimg/master/image-20220905164221964.png"
alt="image-20220905164221964" />
<figcaption aria-hidden="true">image-20220905164221964</figcaption>
</figure>
<h3 id="缺陷">缺陷</h3>
<ol type="1">
<li>将视频分为片段后，会导致某些片段没有标签对应的动作，从而干扰模型的效果</li>
<li>无法捕捉长期的时间信息</li>
<li>使用光流作为特征意味着需要分别计算流特征</li>
</ol>
<p>基于上述缺陷，新的工作通过使用更低分辨率的视频和更长的视频片段（60帧）以实现更好的性能。</p>
<h2 id="c3d">C3D</h2>
<p>https://arxiv.org/pdf/1412.0767</p>
<h3 id="关键贡献-1">关键贡献</h3>
<ol type="1">
<li>利用三维卷积网络做特征提取器</li>
<li>广泛搜索最佳3D卷积内核和架构</li>
<li>使用反卷积层解释模型决策</li>
</ol>
<h3 id="解释-1">解释</h3>
<figure>
<img src="https://blog.qure.ai/assets/images//actionrec/c3d_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>另外,作者使用了反卷积层来解释这样的设计，他们的发现是，在最初的几帧中，网络关注的是空间外观，并在随后的几帧中跟踪运动。</p>
<figure>
<img src="https://blog.qure.ai/assets/images//actionrec/trial.gif"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h3 id="缺陷-1">缺陷</h3>
<ol type="1">
<li>长时间模型的建模依旧是个未解决的问题</li>
<li>过于庞大的网络在训练上计算过慢</li>
</ol>
<h3 id="note">Note：</h3>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/fstcn_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>FSTCN，分解3D卷积网络，主要思路是将三维卷积分解为空间二维卷积，然后是时间一维卷积。将一维卷积放在二维卷积层之后，实现二维时域和信道维的卷积。</p>
<h2 id="三维卷积注意力机制">三维卷积+注意力机制</h2>
<p>https://arxiv.org/abs/1502.08029</p>
<h3 id="关键贡献-2">关键贡献</h3>
<ol type="1">
<li>新颖的3D CNN-RNN编码器-解码器结构，可以捕捉局部时空信息</li>
<li>使用注意力机制来获取全局上下文</li>
</ol>
<h3 id="解释-2">解释</h3>
<p>虽然这项工作与动作识别没有直接的关系，但在视频表征方面是具有里程碑意义的工作。本文采用三维CNN
+ LSTM作为视频描述任务的基础架构。在基础上，作者使用一个预先训练的3D
CNN来提高效果。</p>
<h3 id="算法">算法</h3>
<p>其设置与LRCN中描述的编码器-解码器架构几乎相同，但有两个不同之处：</p>
<ol type="1">
<li>并非单纯的使用了3D卷积做LSTM的特征向量输入。对于每一帧，先通过3D卷积获取feature
maps，再通过2D卷积对其帧集获取feature maps集合。2D和3D
CNN使用的是预先训练的，而不是像LRCN这样的端到端训练。</li>
<li>并非平均所有帧的时间向量。加权平均值的方法用于结合时间特征。在每个时间步上，根据LSTM输出来确定注意力权重。</li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/Larochelle_paper_high.png"
alt="Attention Mechanism" />
<figcaption aria-hidden="true">Attention Mechanism</figcaption>
</figure>
<h2 id="twostreamfusion">TwoStreamFusion</h2>
<p>https://arxiv.org/abs/1604.06573</p>
<h3 id="关键贡献-3">关键贡献</h3>
<ol type="1">
<li>通过更好的远距离损失的远距离时间建模</li>
<li>新颖的多层次融合架构</li>
</ol>
<h3 id="解释-3">解释</h3>
<p>在这个工作中，作者使用了基本的双流架构，并采用了两种新颖的方法，在提高性能的同时并不会带来任何参数的显著增加。</p>
<ol type="1">
<li>空间流和时间流的融合：以洗头和刷牙为例，空间网络可以捕捉视频中的空间相关性（判断头发还是牙齿），时间网络则可以捕捉到视频中每个空间位置的周期性运动。因此，将人脸特定区域的空间特征映射到相应区域的时间特征映射是非常重要的。为了达到同样的效果，网络需要在较早的水平上进行融合，使相同像素位置的响应处于对应状态，而不是在最后进行融合。</li>
<li>跨时间框架组合时间净输出，以便对长期依赖也进行建模。</li>
</ol>
<h3 id="算法-1">算法</h3>
<p>与双流架构基本一样，除了</p>
<ol type="1">
<li><p>如下图所示，Conv_5层的输出均通过卷积层+池化层的方式融合，而此算法则在最后一层加了另一种融合，最后融合输出作为spatiotemporal
loss。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/fusion_strategies_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure></li>
<li><p>时间融合采用跨时间叠加的时间网络输出，采用conv+pooling融合的方法计算时间损失</p></li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/2streamfusion.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="tsn">TSN</h2>
<p>https://arxiv.org/abs/1608.00859</p>
<h3 id="关键贡献-4">关键贡献</h3>
<ol type="1">
<li>针对长期时间建模的有效解决方案</li>
<li>确定BN、dropout和预训练是一种有效的尝试</li>
</ol>
<h3 id="解释-4">解释</h3>
<p>与基本的双流架构有两个主要的不同：</p>
<ol type="1">
<li>他们建议在整个视频中稀疏地采样片段，以更好地建模长期时间信号，而不是在整个视频中随机采样。</li>
<li>为了最终的预测，作者在视频层面探索了多种策略。最好的策略是
<ol type="1">
<li>通过对片段平均，分别组合数十个时间和空间流(以及其他流，如果涉及其他输入模式)</li>
<li>对所有类融合最终的空间和时间得分使用加权平均和应用softmax。</li>
</ol></li>
</ol>
<p>该工作的另一个重要部分是解决过拟合问题(由于数据集规模较小)，并演示使用现在流行的技术，如批处理规范化、Dropout和预训练来应对。作者还评估了两种新的光流输入模式，即弯曲光流和RGB差。</p>
<h3 id="算法-2">算法</h3>
<p>在训练和预测过程中，将一段视频分成K段，每段时长相等。然后，从K个片段中随机抽取片段。其余的步骤仍然类似于上面提到的双流架构的更改。</p>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/tsn_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="actionvlad">ActionVLAD</h2>
<p>https://arxiv.org/pdf/1704.02895.pdf</p>
<h3 id="关键贡献-5">关键贡献</h3>
<ol type="1">
<li>可学习的视频级聚合功能</li>
<li>具有视频级聚合特征的端到端可训练模型，以捕获长期依赖</li>
</ol>
<h3 id="解释-5">解释</h3>
<p>作者最显著的贡献是使用了可学习的特性聚合(VLAD)，而不是使用maxpool或avgpool的普通聚合。聚合技术类似于视觉词汇。有多个基于锚点(比如<span
class="math inline">\(c_1\)</span>, <span
class="math inline">\(c_k\)</span>)的学习词汇，代表k个典型的动作(或子动作)相关的时空特征。两个流结构中的每个流的输出都按照k空间的动作词特征进行编码——每个特征都是输出与任何给定的空间或时间位置对应的锚点的差值。</p>
<p>（没看懂）</p>
<figure>
<img src="https://blog.qure.ai/assets/images/actionrec/actionvlad.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>平均或最大池只作为一个描述符来表示整个点的分布，这对于表示由多个子动作组成的整个视频来说是次优的。相比之下，提出的视频聚合通过将描述符空间分割为<strong>k个单元</strong>并在每个单元内池化来表示具有多个子动作的描述符的整个分布。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/pooling_difference_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="hiddentwostream">HiddenTwoStream</h2>
<p>https://arxiv.org/abs/1704.00389</p>
<h3 id="关键贡献-6">关键贡献</h3>
<p>提出一种新颖的使用单独的网络实时生成光流输入的架构</p>
<h3 id="解释-6">解释</h3>
<p>光流在双流架构的使用使得必须预先计算每个采样帧的光流，从而对存储和速度产生不利影响。这篇论文提出了一种使用无监督架构来生成光流的方法。</p>
<p>光流法可以被当成是一种图像重建问题。给定一对相邻的帧<span
class="math inline">\(l_1、l_2\)</span>作为输入，CNN生成一个流场<span
class="math inline">\(V\)</span>，然后利用预测的流场<span
class="math inline">\(V\)</span>和<span
class="math inline">\(l_2\)</span>，利用反翘曲将<span
class="math inline">\(l_1\)</span>重构为<span
class="math inline">\(l_1^{&#39;}\)</span>，使<span
class="math inline">\(l_1\)</span>与重构的差值最小。</p>
<h3 id="算法-3">算法</h3>
<p>作者探索了多种策略和架构，在不太影响精度的前提下，以最大的帧数和最小的参数产生光流。最后的体系结构与前面提到的双流体系结构相同</p>
<ol type="1">
<li>时间流现在有堆叠在一般时间流架构的顶部的光流生成网络(MotionNet)。时间流的输入现在是后续帧而不是预处理的光流。</li>
<li>对于MotionNet的无监督训练，还有额外的多层次损失</li>
</ol>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/hidden2stream_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<h2 id="i3d">I3D</h2>
<p>https://arxiv.org/abs/1705.07750</p>
<h3 id="关键贡献-7">关键贡献</h3>
<ol type="1">
<li>利用预训练将基于3D的模型结合到两个流架构中</li>
<li>Kinetics数据集用于未来的基准测试和改进的行动数据集的多样性</li>
</ol>
<h3 id="解释-7">解释</h3>
<p>作者不是使用单一的3D网络，而是在双流架构中为两个流使用两个不同的3D网络。此外，为了利用预训练的2D模型，作者在第三维中重复了2D预训练的权重。现在的空间流输入由时间维度上叠加的帧组成，而不是像基本的两种流结构那样由单个帧组成。</p>
<h2 id="t3d">T3D</h2>
<p>https://arxiv.org/abs/1711.08200</p>
<h3 id="关键贡献-8">关键贡献</h3>
<ol type="1">
<li>跨可变深度组合时间信息的架构</li>
<li>新颖的训练架构和技术，以监督2D预训练的网络转移到3D网络</li>
</ol>
<h3 id="解释-8">解释</h3>
<p>作者扩展了在I3D上完成的工作，但建议使用基于单一流3D
DenseNet的架构，在密集块之后叠加多深度时间池化层(时间过渡层)，以捕获不同的时间深度。多深度池化是通过池化不同时间大小的核来实现的。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/ttl_layer_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>
<p>除上述内容外，作者还设计了一种新的技术，以在预先培训的2D Conv
Nets和T3D中监督转移学习。2D预三角网和T3D都是从视频中呈现的框架和剪辑，其中剪辑和视频可能来自同一视频。该体系结构是基于相同的三角形来预测0/1的，并且预测的误差通过T3D
NET进行了反向传播，以便有效地传输知识。</p>
<figure>
<img
src="https://blog.qure.ai/assets/images/actionrec/transfer_learning_high.png"
alt="SegNet Architecture" />
<figcaption aria-hidden="true">SegNet Architecture</figcaption>
</figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
              <a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" rel="tag"># 动作识别</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/03/26/Numpy/" rel="prev" title="NumPy">
                  <i class="fa fa-chevron-left"></i> NumPy
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/12/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/" rel="next" title="视频理解相关论文">
                  视频理解相关论文 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
