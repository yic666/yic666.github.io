<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Prompting Visual-Language Models for Efficient Video Understanding ECCV2022 paper code 基于图像的视觉语言模型">
<meta property="og:type" content="article">
<meta property="og:title" content="Efficient Video Understanding">
<meta property="og:url" content="http://example.com/2023/04/01/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Efficient-Video-Understanding/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:description" content="Prompting Visual-Language Models for Efficient Video Understanding ECCV2022 paper code 基于图像的视觉语言模型">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-04-01T06:25:58.000Z">
<meta property="article:modified_time" content="2023-11-26T13:02:22.554Z">
<meta property="article:author" content="Yic-gdut">
<meta property="article:tag" content="CLIP">
<meta property="article:tag" content="Efficient">
<meta property="article:tag" content="Prompt">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/04/01/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Efficient-Video-Understanding/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/04/01/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Efficient-Video-Understanding/","path":"2023/04/01/视频动作识别/Efficient-Video-Understanding/","title":"Efficient Video Understanding"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Efficient Video Understanding | Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yic</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#prompting-visual-language-models-for-efficient-video-understanding"><span class="nav-number">1.</span> <span class="nav-text">Prompting
Visual-Language Models for Efficient Video Understanding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%83%8F%E7%9A%84%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.</span> <span class="nav-text">基于图像的视觉语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pre-training"><span class="nav-number">1.1.1.</span> <span class="nav-text">Pre-training</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inference"><span class="nav-number">1.1.2.</span> <span class="nav-text">Inference</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#discussion"><span class="nav-number">1.1.3.</span> <span class="nav-text">Discussion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3prompting-clip"><span class="nav-number">1.2.</span> <span class="nav-text">视频理解Prompting CLIP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%97%AE%E9%A2%98%E8%AE%BE%E6%83%B3"><span class="nav-number">1.2.1.</span> <span class="nav-text">问题设想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%8F%90%E7%A4%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%87%AA%E9%80%82%E5%BA%94"><span class="nav-number">1.2.2.</span> <span class="nav-text">基于提示学习的模型自适应</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E5%BA%8F%E5%BB%BA%E6%A8%A1"><span class="nav-number">1.2.3.</span> <span class="nav-text">时序建模</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83loss"><span class="nav-number">1.2.4.</span> <span class="nav-text">训练loss</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dual-path-adaptation-from-image-to-video-transformers"><span class="nav-number">2.</span> <span class="nav-text">Dual-path
Adaptation from Image to Video Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#baseline"><span class="nav-number">2.1.</span> <span class="nav-text">Baseline</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spatial-adaptation"><span class="nav-number">2.2.</span> <span class="nav-text">Spatial adaptation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-adaptation"><span class="nav-number">2.3.</span> <span class="nav-text">Temporal adaptation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#aim-adapting-image-models-for-efficient-video-action-recognition"><span class="nav-number">3.</span> <span class="nav-text">AIM:
Adapting Image Models for Efficient Video Action Recognition</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A9%BA%E9%97%B4-adaptation"><span class="nav-number">3.1.</span> <span class="nav-text">空间 Adaptation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%97%B6%E9%97%B4-adaptation"><span class="nav-number">3.2.</span> <span class="nav-text">时间 Adaptation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%81%94%E5%90%88-adaptation"><span class="nav-number">3.3.</span> <span class="nav-text">联合 Adaptation</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/01/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Efficient-Video-Understanding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Efficient Video Understanding | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Efficient Video Understanding
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-01 14:25:58" itemprop="dateCreated datePublished" datetime="2023-04-01T14:25:58+08:00">2023-04-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-11-26 21:02:22" itemprop="dateModified" datetime="2023-11-26T21:02:22+08:00">2023-11-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" itemprop="url" rel="index"><span itemprop="name">视频动作识别</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1
id="prompting-visual-language-models-for-efficient-video-understanding">Prompting
Visual-Language Models for Efficient Video Understanding</h1>
<p>ECCV2022</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.04478">paper</a> <a
target="_blank" rel="noopener" href="https://github.com/ju-chen/Efficient-Prompt">code</a></p>
<h2 id="基于图像的视觉语言模型">基于图像的视觉语言模型</h2>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230406151516.png" /></p>
<span id="more"></span>
<h3 id="pre-training">Pre-training</h3>
<p>给定一个batch的N对(图像，文本)，使用两个单独的编码器计算图像和文本的feature
embeddings，并计算所有N个可能的(图像，文本)对之间的密集余弦相似度矩阵。训练目标是联合优化图像和文本编码器，通过最大化N对正确的(图像，文本)关联之间的相似性，同时通过密集矩阵上的对称交叉熵最小化N
×(N−1)错误对的相似性，即噪声对比学习。这两个编码器都包含一个tokeniser，用于将图像补丁或语言单词转换为向量，也就是会有visual
tokens和textual tokens。</p>
<h3 id="inference">Inference</h3>
<p>训练完成之后，I-VL模型便可以部署在开放词汇表上的图像分类任务，通过使用从文本编码器<span
class="math inline">\(\Phi_{\text{text}}\)</span>生成相应的视觉分类器完成分类任务。举例来说，如果为了分辨一张图片是猫还是狗，那分类器（<span
class="math inline">\(c_{cat}\)</span>和<span
class="math inline">\(c_{dog}\)</span>）可以生成为: <span
class="math display">\[
\begin{aligned}c_{\mathrm{cat}}&amp;=\Phi_{\mathrm{text}}(\mathrm{TOKENISER}(\text{“this
is a photo of}[\underline{cat}]\text{&#39;&#39;}))\\
c_{\mathrm{dog}}&amp;=\Phi_{\operatorname{text}}(\mathrm{TOKENISER}(\text{“this
is a photo of}[\underline{dog}]\text{&#39;&#39;}))\end{aligned}
\]</span> 而“this is a photo of
[·]”是手工制作的提示模板，可以被有效地用于图像分类。</p>
<h3 id="discussion">Discussion</h3>
<blockquote>
<p>尽管在零样本图像分类方面取得了巨大的成功，I-VL模型也显示出对手工制作的提示模板很敏感，显然对其对新的下游任务的有效适应造成了限制，在这些任务中，专家知识可能很难浓缩或不可用。因此，考虑将这种提示设计过程自动化，探索有效的方法，以使预训练的基于图像的视觉-语言模型适应新的下游任务，而训练数量最少。</p>
</blockquote>
<h2 id="视频理解prompting-clip">视频理解Prompting CLIP</h2>
<p>作者认为I-VL模型上的prompt
learning会在视频领域大放异彩主要有两个原因：（实际上感觉和用图像理解的模型去预训练视频理解模型类似）
1.
视频任务需要大量的资源。具体来说，视频-文本对更难收集且训练计算成本更高，因此通过训练大规模的基于图像的视觉文本模型（I-VL），并且prompt进行高效的视频理解更好（类似于图像迁移到视频？）。
2.
视频由帧序列组成，在强大的基于图像的模型上建立时间依赖性是一个自然且经济的选择。</p>
<h3 id="问题设想">问题设想</h3>
<p>数据集表示为<span class="math inline">\(\mathcal{D} = \{
\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{val}}
\}\)</span>，e.g.<span class="math inline">\(\mathcal{D}_{\text{train}}
= \{(\mathcal{V}_1, y_1), \dots, (\mathcal{V}_n, y_n)
\}\)</span>，其中视频为<span class="math inline">\(\mathcal{V}_i \in
\mathbb{R}^{T \times H \times W \times 3}\)</span>，而标签<span
class="math inline">\(y_i\)</span>根据下游任务的不同而有不同的情况：识别任务为<span
class="math inline">\(c_{train}\)</span>中的动作标签、定位任务为密集的动作类别标签的T时间戳、检索任务为细粒度文本描述。</p>
<p>在closed-set的情况下，训练和验证的动作类别是相同的；零样本情况下，训练和验证的动作类别是不相关的。</p>
<h3 id="基于提示学习的模型自适应">基于提示学习的模型自适应</h3>
<p>主要目的是引导预训练只需最少的训练即可执行各种视频任务。具体而言，通过将连续随机向量序列(“提示向量”)与文本标记预先/追加，实现高效的模型适应。在训练时，CLIP的图像和文本编码器都保持冻结，梯度将通过文本编码器，只更新提示向量。最终，这些可学习的向量将构建文本编码器可以理解的“虚拟”提示模板，并生成所需的分类器或查询嵌入，详细内容如下</p>
<ol type="1">
<li>动作识别：为了生成动作分类器，我们通过将标记化的类别名称输入预训练的文本编码器<span
class="math inline">\(\mathrm{\Phi}_{\text{text}}\)</span>来构造“虚拟”提示模板，如下式，其中<span
class="math inline">\(a_i \in \mathbb{R}
^{D}\)</span>表示第i个提示向量，由几个可学习参数组成，<span
class="math inline">\(D\)</span>是向量维度。提示向量<span
class="math inline">\(\{a_i\}\)</span>会与所有的动作类别共享，也就是只是对于任务是专有的。
<span class="math display">\[
\begin{align*}
&amp;c_{\text{archery}} = \mathrm{\Phi}_{\text{text}}(a_{1}, \dots,
\mathrm{TOKENISER}(\text{&#39;&#39;}\text{\underline{archery}&#39;&#39;}),
\dots, a_{k}) \\
&amp;c_{\text{bowling}} = \mathrm{\Phi}_{\text{text}}(a_{1},
\dots,  \mathrm{TOKENISER}(\text{&#39;&#39;}\text{\underline{bowling}&#39;&#39;}),
\dots, a_{k})
\end{align*}
\]</span></li>
<li>动作定位：采用two-stage范式，首先检测潜在的类别未知动作建议(详见第4.1节)，然后对这些检测到的建议执行动作分类。</li>
<li>视频文字提取：类似地将整个句子标记化，并将标记化的结果与可学习的提示向量馈送到文本编码器，以生成每个句子的查询嵌入。</li>
<li>总结：一般来说，模型适应的学习提示有以下好处:
1）所有任务都可以使用同一个共享的backbone，并且能达到有竞争力的性能；2）适应新任务只需要优化少数提示向量，便于实现few-shot问题；3）能够更好地利用丰富的训练数据，并进一步泛化到封闭集类别之外。</li>
</ol>
<h3 id="时序建模">时序建模</h3>
<p>作者通过使用一个简单而轻量级的时序建模模块来弥补图像到视频之间的差距。具体来说，通过在冻结图像编码器的帧级特征上附加一个Transformer编码器将CLIP的图像encoder升级为视频encoder：
<span class="math display">\[
\begin{align*}
v_i = \mathrm{\Phi}_{\text{video}}(\mathcal{V}_i) =
\mathrm{\Phi}_(\{
\mathrm{\Phi}_{\text{image}}(I_{i1}), \dots,
\mathrm{\Phi}_{\text{image}}(I_{iT})\})
\end{align*}
\]</span> 为了表示时间顺序，图像特征上添加了可学习的时间位置编码。<span
class="math inline">\(v_i \in \mathbb{R} ^{T \times D}\)</span>是<span
class="math inline">\(T\)</span>帧的密集特征嵌入。</p>
<h3 id="训练loss">训练loss</h3>
<p>给定一批(视频，文本)训练对，视觉流最终得到密集的帧级特征嵌入~(<span
class="math inline">\(v_i\)</span>);而对于文本流，根据考虑的下游任务，它最终会得到一组操作分类器(<span
class="math inline">\(c_i \in
\mathcal{C}_{\text{action}}\)</span>)或文本查询嵌入(<span
class="math inline">\(c_i \in
\mathcal{C}_{\text{query}}\)</span>)。对于动作识别和文本-视频检索，通过取密集特征的均值池来进一步计算视频片段级特征:
<span class="math display">\[
\begin{align}
    \overline{v}_i = \mathrm{\Phi}_{POOL}(v_i) \in \mathbb{R}^{1 \times
D}
\end{align}
\]</span>
对于动作定位，对每个检测到的动作建议中的密集特征进行平均池化，以获得提案级特征。为了简单起见，还将这个提议级别的特性表示为<span
class="math inline">\(\overline{v}_i\)</span>。
训练过程中，共同优化文本提示向量和时间Transformer，使得视频片段(提案)特征及其配对分类器或文本查询嵌入在其他特征中发出最高的相似性分数。这是通过简单的NCE损失实现的
<span class="math display">\[
\begin{align}
\mathcal{L} = &amp;- \sum_i \big( \log \frac{\exp(\overline{v}_i \cdot
c_{i} / \tau)}{\sum\limits_{j} \exp(\overline{v}_i \cdot c_j / \tau)}
\big)
\end{align}
\]</span></p>
<h1 id="dual-path-adaptation-from-image-to-video-transformers">Dual-path
Adaptation from Image to Video Transformers</h1>
<p>CVPR 2023</p>
<p>paper: https://arxiv.org/abs/2303.09857</p>
<p>code: https://github.com/park-jungin/dualpath</p>
<h2 id="baseline">Baseline</h2>
<p>Parameter-efficient transfer learning
(PETL),参数高效迁移学习，在自然语言处理(NLP)中首次被使用，用于解决全/部分微调的内存和参数效率低下的问题。主要目标是通过仅使用少量可训练参数进行微调，在下游任务上获得相当或超过的性能。</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421132548.png" /></p>
<ol type="1">
<li>Visual prompt tuning (VPT) 在Transformer块的Input Tokens前添加<span
class="math inline">\(K\)</span>个可训练的prompt
token，同时冻结预训练的参数。Input Tokens为<br />
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421133836.png" /></li>
<li>AdaptFormer学习一个可训练的bottleneck模块。这个模块会与Transformer块中的MLP层平行，即输入为中间特征<span
class="math inline">\(z\)</span>，AdaptFormer block的输出可表示为 <img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421134535.png" /></li>
<li>Pro-tuning使用连续的2D卷积层从每个Transformer块的输出中预测特定于任务的视觉提示v。每个块的输出会被reshaped为
<span class="math inline">\(P \times P \times C\)</span>以应用于2D卷积
。 <img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421140433.png" /></li>
<li>ST-adapter采用的adapter结构为在向下投影层和激活函数之间插入深度方向的3D卷积层。相较于AdaptFormer，ST-adapter会接受所有帧的token使得模型可以捕捉到视频中的时间特征。输出可表示为：
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421141151.png" /></li>
</ol>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421142701.png" /></p>
<h2 id="spatial-adaptation">Spatial adaptation</h2>
<p>即使在视频中，已经在大量的数据集上训练过的图像模型也可能对空间contexts进行编码，具有突出的空间建模能力。为了更适合视频理解，作者采用了两个平行于transformer块的MHA和MLP的adapter。平行adapter可以允许模型在保持用于识别对象的原始contexts的同时，能在视频的空间信息中学得可以用于动作识别的contexts。</p>
<p>空间token的集<span
class="math inline">\(\mathbf{X}^{\text{SP}}_t\)</span>包括可学习的位置编码<span
class="math inline">\(\mathbf{p}^{SP}\)</span>和空间class token <span
class="math inline">\(\mathbf{x}_{t}^{\mathrm{SP}}\{[\mathrm{CLS}]\}\)</span>。第<span
class="math inline">\(l\)</span>个transformer块的Spatial
adaptation可由下列公式表示：</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421150350.png" /></p>
<p>接着对来自最后一个transformer块的空间分类token进行平均以获得全局空间表示<span
class="math inline">\(y^{sp}\)</span></p>
<h2 id="temporal-adaptation">Temporal adaptation</h2>
<p>虽然Spatial
adaptation能够使得图像模型适应视频数据中的空间contexts，但图像模型仍然无法对时间信息进行建模。使视频transformer能够建模时间context的关键是能够学习到视频中跨帧的局部patch之间的关系。因此，作者提出了一种新的类网格帧集转换技术，将多个帧聚合成一个统一的类网格帧集，能模拟空间和时间建模且大大减少计算量。在每个transformer块中，分别为MHA和MLP采用了两个额外的串行adapter。</p>
<p>在视频中采样<span class="math inline">\(T\)</span>帧后，使用<span
class="math inline">\(w\)</span>和<span
class="math inline">\(h\)</span>的因子进行缩放，缩放后的帧大小为<span
class="math inline">\([W/w \times H/h \times
3]\)</span>。接着根据时间顺序堆叠<span class="math inline">\(w\times
h\)</span>个缩放帧，并重构堆叠帧，以与原始帧大小相同的网格形式构造一组帧，从而得到的类网格帧集的总数为<span
class="math inline">\(T_G = T/wh\)</span>。按同样的方式获得第<span
class="math inline">\(g\)</span>个帧集的时间token<span
class="math inline">\(\mathbf{X}_g^{\text{TP}}\)</span>，并与可学习的时间分类token<span
class="math inline">\(\mathbf{x}_{g}^{\mathrm{SP}}\{[\mathrm{CLS}]\}\)</span>相结合。与Spatial
adaptation不同，使用了固定3D位置编码<span
class="math inline">\(\mathbf{p}^{TP}\)</span>，考虑到patch的绝对时间顺序和空间位置。作者依次将适配器附加到每个变压器块中的MHA和MLP层的顶部，第<span
class="math inline">\(l\)</span>个transformer块的temporal
adaptation可由下列公式表示：</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421154936.png" /></p>
<p>接着对来自最后一个transformer块的时间分类token进行平均以获得全局空间表示<span
class="math inline">\(y^{sp}\)</span>。对于最后的预测，将全局空间和时间表示连接起来，并将它们输入两个FC层之间具有GeLU激活的分类器。</p>
<h1
id="aim-adapting-image-models-for-efficient-video-action-recognition">AIM:
Adapting Image Models for Efficient Video Action Recognition</h1>
<p>ICLR 2023</p>
<p>paper: https://arxiv.org/abs/2302.03024</p>
<p>code: https://github.com/taoyang1122/adapt-image-models</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230407140936.png" /></p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230407151431.png" /></p>
<h2 id="空间-adaptation">空间 Adaptation</h2>
<p>Adapter是在NLP上被广泛使用的一种高效微调技术。从图中（b）可以看出，Adapter是一个bottleneck架构，由两个全连接(FC)层和中间的激活层组成。为了使预训练的空间特征适应目标视频数据，空间Adaptation通过在自注意层后添加Adapter实现。在训练过程中，只更新Adapter的参数而冻结其他部分。</p>
<h2 id="时间-adaptation">时间 Adaptation</h2>
<p>作者提出了一个新的方法：复用图像模型中预先训练好的自注意层进行时间建模。
具体地说，他们将原始的自注意层表示为S-MSA用于空间建模，将复用的自注意层表示为T-MSA用于时间建模。可以在图中（c）看到，他们将T-MSA放在S-MSA前面，然后通过reshape的方法调整输入的embedding，使得同样的MSA可以用于不同的维度。与空间Adaptation一样，在MSA后加上了一个时间Adapter，但不同的是这个Adapter没有skip
connection，原因是希望将适应模型初始化为接近原始模型。</p>
<h2 id="联合-adaptation">联合 Adaptation</h2>
<p>联合Adaptation就是在最后的MLP处加上了一个与其并行的Adapter，而这个Adapter的结构与时间Adaptation的一样，最后adapted
block可被写为 <span class="math display">\[
\begin{aligned}
    z^T_{l} &amp;= z_{l-1} +
\operatorname{Adapter}(\operatorname{T-MSA}(\operatorname{LN}(z_{l-1})))\\
    z^S_{l} &amp;= z^T_{l} +
\operatorname{Adapter}(\operatorname{S-MSA}(\operatorname{LN}(z^T_{l})))\\
    z_{l} &amp;= z^S_{l} +
\operatorname{MLP}(\operatorname{LN}(z^S_{l})) + s \cdot
\operatorname{Adapter}(\operatorname{LN}(z^S_{l}))
    \end{aligned}
\]</span></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CLIP/" rel="tag"># CLIP</a>
              <a href="/tags/Efficient/" rel="tag"># Efficient</a>
              <a href="/tags/Prompt/" rel="tag"># Prompt</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/28/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Video-Swin-Transformers/" rel="prev" title="Video-Swin-Transformers">
                  <i class="fa fa-chevron-left"></i> Video-Swin-Transformers
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/06/26/%E8%A7%86%E9%A2%91%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/Video-Test-Time-Adaptation-for-Action-Recognition/" rel="next" title="Video Test-Time Adaptation for Action Recognition">
                  Video Test-Time Adaptation for Action Recognition <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
