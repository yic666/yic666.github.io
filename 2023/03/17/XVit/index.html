<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="paper: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.05968 code: https:&#x2F;&#x2F;github.com&#x2F;1adrianb&#x2F;video-transformers 摘要 本文研究的是利用Transformer进行视频识别。最近在这一领域的尝试在识别精度方面已经证明了有希望的结果，但在许多情况下，由于对时间信息的额外建模，它们也被证明会导致显著的计算开销。在这项工作中，我">
<meta property="og:type" content="article">
<meta property="og:title" content="XVit">
<meta property="og:url" content="http://example.com/2023/03/17/XVit/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:description" content="paper: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2106.05968 code: https:&#x2F;&#x2F;github.com&#x2F;1adrianb&#x2F;video-transformers 摘要 本文研究的是利用Transformer进行视频识别。最近在这一领域的尝试在识别精度方面已经证明了有希望的结果，但在许多情况下，由于对时间信息的额外建模，它们也被证明会导致显著的计算开销。在这项工作中，我">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-03-17T02:44:02.000Z">
<meta property="article:modified_time" content="2023-03-23T14:06:03.298Z">
<meta property="article:author" content="Yic-gdut">
<meta property="article:tag" content="Transformer-based">
<meta property="article:tag" content="代码">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/03/17/XVit/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/03/17/XVit/","path":"2023/03/17/XVit/","title":"XVit"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>XVit | Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yic</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.</span> <span class="nav-text">主要贡献</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#video-transformer"><span class="nav-number">3.1.</span> <span class="nav-text">Video Transformer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%91%E4%BC%BC%E5%85%A8%E6%97%B6%E7%A9%BA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">3.2.</span> <span class="nav-text">近似全时空注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#temporal-attention-aggregation"><span class="nav-number">3.2.1.</span> <span class="nav-text">Temporal Attention
aggregation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#summary-token"><span class="nav-number">3.2.2.</span> <span class="nav-text">Summary token</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">4.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93"><span class="nav-number">4.1.</span> <span class="nav-text">模型整体</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">4.2.</span> <span class="nav-text">注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#temporal-attention-aggregation-1"><span class="nav-number">4.3.</span> <span class="nav-text">Temporal Attention
aggregation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#from-chatgpt-just-for-fun"><span class="nav-number">5.</span> <span class="nav-text">From ChatGPT (Just for fun)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/17/XVit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="XVit | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          XVit
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-17 10:44:02" itemprop="dateCreated datePublished" datetime="2023-03-17T10:44:02+08:00">2023-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-23 22:06:03" itemprop="dateModified" datetime="2023-03-23T22:06:03+08:00">2023-03-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>paper: https://arxiv.org/abs/2106.05968</p>
<p>code: https://github.com/1adrianb/video-transformers</p>
<h1 id="摘要">摘要</h1>
<p>本文研究的是利用Transformer进行视频识别。最近在这一领域的尝试在识别精度方面已经证明了有希望的结果，但在许多情况下，由于对时间信息的额外建模，它们也被证明会导致显著的计算开销。在这项工作中，我们提出了一个视频Transformer模型，其复杂性与视频序列中的帧数成线性比例，因此与基于图像的Transformer模型相比没有开销。为了实现这一点，我们的模型对视频Transformer中使用的全时空注意力做了两个近似:(a)它将时间注意力限制在局部时间窗口，并利用Transformer的深度来获得视频序列的全时间覆盖。(b)它使用高效的时空混合来联合关注空间和时间位置，而不会在纯空间注意力模型的基础上产生任何额外的成本。我们还展示了如何集成2个非常轻量级的全局时间关注机制，以最小的计算成本提供额外的精度改进。我们证明了我们的模型在最流行的视频识别数据集上产生非常高的识别精度，同时比其他视频转换器模型更有效。代码将被提供。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230317104614654.png"
alt="Different approaches to space-time self-attention for video recognition." />
<figcaption aria-hidden="true">Different approaches to space-time
self-attention for video recognition.</figcaption>
</figure>
<h1 id="主要贡献">主要贡献</h1>
<ol type="1">
<li>提出一种复杂度为<span
class="math inline">\(O(TS^2)\)</span>的视频Transformer模型，与基线模型一样高效。它在效率方面（即准确度/FLOP）比最近/同时提出的工作优越很多；</li>
<li>对视频Transformer的全时空注意力进行了两次近似：
<ol type="1">
<li>将时间注意力限制在局部时间窗口内，并利用Transformer的深度获得视频序列的完整时间覆盖；</li>
<li>使用高效的时空混合，在不增加空间唯一注意力模型额外成本的情况下，共同关注空间和时间位置；</li>
</ol></li>
<li>展示了如何整合两种非常轻量级的全局时间唯一注意力机制，它们以最小的计算成本提供额外的准确性改进。</li>
</ol>
<h1 id="方法">方法</h1>
<h2 id="video-transformer">Video Transformer</h2>
<p>给定视频片段：<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{T\times H \times W \times
C }\)</span></p>
<p>根据ViT，每帧将被分割为<span class="math inline">\(K\times
K\)</span>个非重叠补丁并通过一个线性embedding层<span
class="math inline">\(\mathbf{E}\in\mathbb{R}^{3K^2 \times
d}\)</span>映射为visual
token。另外需要学习两个位置embedding加载初始化的visual token，一是<span
class="math inline">\(\mathbf{p}_{s}\in\mathbb{R}^{1 \times S\times
d}\)</span>，二是<span
class="math inline">\(\mathbf{p}_{t}\in\mathbb{R}^{T\times 1 \times
d}\)</span>。最后token序列会被L层Transformer处理。第<span
class="math inline">\(l\)</span>层的visual token被定义为： <span
class="math display">\[
\mathbf{z}^l_{s,t}\in\mathbb{R}^d, \;\;\; l=0,\dots,L-1, \;\;
s=0,\dots,S-1, \;\; t=0,\dots,T-1
\]</span> 同样，cls token<span
class="math inline">\(\mathbf{z}^l_{cls}\in\mathbb{R}^{d}\)</span>会被加在token序列。第<span
class="math inline">\(l\)</span>层变换器使用一系列多头自我注意力（MSA）、层归一化（LN）和MLP（<span
class="math inline">\(\mathbb{R}^d \rightarrow \mathbb{R}^{4d}
\rightarrow \mathbb{R}^d\)</span>）层来处理前一层的视觉标记<span
class="math inline">\(\mathbf{Z}^l\in\mathbb{R}^{(TS+1) \times
d}\)</span>，如下所示： <span class="math display">\[
\mathbf{Y}^{l}  =  \textrm{MSA}(\textrm{LN}(\mathbf{Z}^{l-1})) +
\mathbf{Z}^{l-1},\\
\mathbf{Z}^{l+1}  =  \textrm{MLP}(\textrm{LN}(\mathbf{Y}^{l})) +
\mathbf{Y}^{l}.
\]</span> 单个时空注意力的计算可被表示为 <span class="math display">\[
\mathbf{y}^{l}_{s,t} = \sum_{t&#39;=0}^{T-1} \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}, \;
\]</span> 最后，整个模型的复杂度为: <span
class="math inline">\(O(3hTSdd_h)\)</span> (<span
class="math inline">\(qkv\)</span> projections) <span
class="math inline">\(+ O(2hT^2S^2d_h)\)</span> (MSA for <span
class="math inline">\(h\)</span> attention heads) <span
class="math inline">\(+ O(TS(hd_h)d)\)</span> (multi-head projection)
<span class="math inline">\(+ O(4TSd^2)\)</span> (MLP).</p>
<h2 id="近似全时空注意力">近似全时空注意力</h2>
Baseline是一个通过在每个Transformer层应用纯空间注意力来执行对全时空注意力的简单近似的模型:
$$ <sup>{l}<em>{s,t} = </em>{s'=0}</sup>{S-1} {(^{l}<em>{s,t}
^{l}</em>{s',t})/} ^{l}_{s',t}, ;{
<span class="math display">\[\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\]</span>
<p>}</p>
<p>$$ 复杂度为<span
class="math inline">\(O(TS^2)\)</span>。在仅spatial-only注意力之后，对cls-token执行简单的时间平均
<span class="math inline">\(\mathbf{z}_{final} =
\frac{1}{T}\sum\limits_{t}
\mathbf{z}^{L-1}_{t,cls}\)</span>以获得一个特征，该特征被馈送到线性分类器。</p>
而TimeSFormer提出的factorised attention如下： $$
<span class="math display">\[\begin{split}
        \tilde{\mathbf{y}}^{l}_{s,t} = \sum_{t&#39;=0}^{T-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s,t&#39;})/\sqrt{d_h}\} \mathbf{v}^{l}_{s,t&#39;}, \\
        \mathbf{y}^{l}_{s,t} = \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{\tilde{\mathbf{q}}^{l}_{s,t} \cdot
\tilde{\mathbf{k}}^{l}_{s&#39;,t})/\sqrt{d_h}\}
\tilde{\mathbf{v}}^{l}_{s&#39;,t},
    \end{split}
    \quad
    \begin{split}
         \; \begin{Bmatrix}
          s=0,\dots,S-1\\
          t=0,\dots,T-1
        \end{Bmatrix},
    \end{split}\]</span>
<p>$$ 并且把复杂度降低到<span class="math inline">\(O(T^2S +
TS^2)\)</span>​。然而，时间注意是对固定的空间位置进行的，当有相机或物体运动以及帧间存在空间错位时，时间注意是无效的。</p>
<p><strong>模型</strong>旨在更好地近似完整的时空自注意力（SA），同时将复杂度保持在<span
class="math inline">\(O(TS^2)\)</span>，即不对spatial-only模型产生进一步的复杂性。为了达到这个目的，论文提出了<strong>第一次近似</strong>，以执行全时空注意力，但仅限于局部时间窗口<span
class="math inline">\([-t_w, t_w]\)</span>： <span
class="math display">\[
\mathbf{y}^{l}_{s,t} = \sum_{t&#39;=t-t_w}^{t+t_w} \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}= \sum_{t&#39;=t-t_w}^{t+t_w}
\mathbf{V}^{l}_{t&#39;} \mathbf{a}^l_{t&#39;},
\;\big\{\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\big\}
\]</span> 其中<span
class="math inline">\(\mathbf{V}^{l}_{t&#39;}=[\mathbf{v}^{l}_{0,t&#39;};
\mathbf{v}^{l}_{1,t&#39;}; \dots;
\mathbf{v}^{l}_{S-1,t&#39;}]\in\mathbb{R}^{d_h \times S}\)</span>，<span
class="math inline">\(\mathbf{a}^l_{t&#39;}=[a^l_{0,t&#39;},
a^l_{1,t&#39;}, \dots,
a^l_{S-1,t&#39;}]\in\mathbb{R}^{S}\)</span>是向量与相应的注意权重。对于单个
Transformer 层，<span
class="math inline">\(\mathbf{y}^{l}_{s,t}\)</span> 是局部窗口 <span
class="math inline">\([-t_w, t_w]\)</span>
中视觉标记的时空组合。因此，在 <span class="math inline">\(k\)</span> 个
Transformer 层之后，<span
class="math inline">\(\mathbf{y}^{l+k}_{s,t}\)</span> 将是局部窗口 <span
class="math inline">\([-kt_w, kt_w]\)</span>
中视觉标记的时空组合，这反过来方便地允许对整个剪辑执行时空注意力。例如，对于
<span class="math inline">\(t_w=1\)</span> 和 <span
class="math inline">\(k=4\)</span>，局部窗口变为 <span
class="math inline">\([-4,
4]\)</span>，它在典型情况下覆盖整个视频剪辑（<span
class="math inline">\(T=8\)</span>）。</p>
<p>如上的局部自注意力的复杂度为 <span
class="math inline">\(O((2t_w+1)TS^2)\)</span>。为了进一步降低这个复杂度，在第一次近似之上进行<strong>第二次近似</strong>，如下所示：在空间位置
<span class="math inline">\(s\)</span> 和 <span
class="math inline">\(s’\)</span> 之间的注意力是 <span
class="math display">\[
\sum_{t&#39;=t-t_w}^{t+t_w}  \textrm{Softmax}\{(\mathbf{q}^{l}_{s,t}
\cdot \mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}
\]</span> 即它需要计算 <span class="math inline">\(2t_w+1\)</span>
个注意力，每个时间位置在 <span class="math inline">\([-t_w,
t_w]\)</span> 上计算一个。相反，论文建议在 <span
class="math inline">\([-t_w, t_w]\)</span> 上计算一个注意力，这可以通过
<span class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 关注 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w} \triangleq
[\mathbf{k}{l}_{s’,t-t_w};\dots;\mathbf{k}{l}_{s’,t+t_w}] \in
\mathbb{R}^{(2t_w+1)d_h}\)</span> 来实现。注意，为了匹配 <span
class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 和 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
的维度，通常需要对 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
进行进一步投影到 <span
class="math inline">\(\mathbb{R}^{d_h}\)</span>，其复杂度为 <span
class="math inline">\(O((2t_w+1)d_h^2)\)</span>。为了缓解这种情况，
使用``移位技巧’‘（类似TSM），它允许在 <span
class="math inline">\(O(d_h)\)</span>
内同时执行零成本降维、时空混合和注意力（在 <span
class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 和 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
之间）。具体来说，每个 <span class="math inline">\(t’ \in [-t_w,
t_w]\)</span> 被分配 <span class="math inline">\(d_h^{t’}\)</span>
个来自 <span class="math inline">\(d_h\)</span> 的通道（即 <span
class="math inline">\(\sum_{t’} d_h^{t’} = d_h\)</span>）。设 <span
class="math inline">\(\mathbf{k}^{l}_{s’,t’}(d_h^{t’})\in
\mathbb{R}^{d_h^{t’}}\)</span> 表示索引 <span
class="math inline">\(\mathbf{k}^{l}_{s’,t’}\)</span> 中的 <span
class="math inline">\(d_h^{t’}\)</span>
个通道的运算符。然后，构造一个新的key向量： $$ ^{l}_{s',-t_w:t_w}
^{d_h}</p>
<p>$$ <img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230317163928156.png"
alt="Detailed self-attention computation graph" /></p>
上图展示了如何构造<span
class="math inline">\(\tilde{\mathbf{k}}^{l}_{s&#39;,-t_w:t_w}\)</span>，按照同样的方式可以构造一个新的value向量<span
class="math inline">\(\tilde{\mathbf{v}}^{l}_{s&#39;,-t_w:t_w}\)</span>。最后，提出的对全时空注意力的近似为:
$$ <sup>{l_s}<em>{s,t} = </em>{s'=0}</sup>{S-1} {(^{l_s}<em>{s,t}
^{l}</em>{s',-t_w:t_w})/} ^{l}_{s',-t_w:t_w}, ;{
<span class="math display">\[\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\]</span>
<p>}.</p>
<p>$$</p>
<h3 id="temporal-attention-aggregation">Temporal Attention
aggregation</h3>
<p>最后一组cls-token<span
class="math inline">\(\mathbf{z}^{L-1}_{t,cls}, 0 \leq t \leq
L-1\)</span>会被用于生成预测结果，为此，论文提出了两个方案：</p>
<ol type="1">
<li>在论文的baseline下，实验简单的时序平均<span
class="math inline">\(\mathbf{z}_{final} = \frac{1}{T}\sum_{t}
\mathbf{z}^{L-1}_{t,cls}\)</span></li>
<li>时间平均显然忽略了时序信息，因此论文提出使用轻量级的时间注意(TA)机制，该机制将参与<span
class="math inline">\(T\)</span> 个cls-token。具体来说，token <span
class="math inline">\(\mathbf{z}_{final}\)</span>
使用时序Transformer处理序列<span
class="math inline">\([\mathbf{z}^{L-1}_{0,cls}, \ldots ,
\mathbf{z}^{L-1}_{T-1,cls}]\)</span>。这类似于ViViT的(并发)工作，不同之处在于，在模型中，发现一个单一的TA层就足够了，而ViViT使用<span
class="math inline">\(L_t\)</span>层。</li>
</ol>
<h3 id="summary-token">Summary token</h3>
<p>作为TA的替代方案，论文还提出了一种简单的轻量级机制，用于在网络中间层的不同帧之间进行信息交换。给定每一帧<span
class="math inline">\(t\)</span>的token集，<span
class="math inline">\(\mathbf{Z}_{t}^{l-1}\in\mathbb{R}^{(S+1)\times
d_h}\)</span>（通过连接所有token<span
class="math inline">\(\mathbf{z}_{s,t}^{l-1},
s=0,\dots,S\)</span>来构造），计算得到<span
class="math inline">\(R\)</span>个token<span
class="math inline">\(\mathbf{Z}^{l}_{r, t} =
\phi(\mathbf{Z}_{t}^{l-1})\in\mathbb{R}^{R\times
d_h}\)</span>组成的一个新token集，这样的一个token集总结了帧信息，因此被命名为“摘要”token。然后将这些token附加到所有帧的visual
token中，以计算key和value，以便query向量处理原始key和Summary标记。论文探讨了<span
class="math inline">\(\phi(.)\)</span>执行简单空间平均的情况，即<span
class="math inline">\(\mathbf{z}^{l}_{0, t} = \frac{1}{S}\sum_{s}
\mathbf{z}^{l}_{s,t}\)</span>在每一帧的token上（对于这种情况，<span
class="math inline">\(R=1\)</span>）。请注意，对于<span
class="math inline">\(R=1\)</span>，Summary token引起的额外成本是<span
class="math inline">\(O(TS)\)</span>。</p>
<!-- # 实验

## 网络架构等细节 -->
<h1 id="代码">代码</h1>
<h2 id="模型整体">模型整体</h2>
<p>模型部分由ViT作为Base_model，在ViT的基础上主要修改了Attention部分，取其中一个<code>Transformer_block</code>打印如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(0): Block(</span><br><span class="line">  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">  (attn): Attention(</span><br><span class="line">    (qkv): Linear(in_features=768, out_features=2304, bias=True)</span><br><span class="line">    (attn_drop): Dropout(p=0.0, inplace=False)</span><br><span class="line">    (proj): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">    (proj_drop): Dropout(p=0, inplace=False)</span><br><span class="line">    (control_point): AfterReconstruction()</span><br><span class="line">    (control_point_query): TemporalShift(</span><br><span class="line">      (net): AfterReconstruction()</span><br><span class="line">    )</span><br><span class="line">    (control_point_value): TemporalShift(</span><br><span class="line">      (net): AfterReconstruction()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (drop_path): Identity()</span><br><span class="line">  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">  (mlp): Mlp(</span><br><span class="line">    (fc1): Linear(in_features=768, out_features=3072, bias=True)</span><br><span class="line">    (act): GELU(approximate=&#x27;none&#x27;)</span><br><span class="line">    (fc2): Linear(in_features=3072, out_features=768, bias=True)</span><br><span class="line">    (drop): Dropout(p=0, inplace=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>构建模型的代码主要位于<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/video_model_builder.py">video_model_builder.py</a>，采用的是在base_model的基础上做修改的方式（基础模型应该是类似于TimesFormer中的联合时空的方式），make_temporal_shift应该就是论文提到的“移位技巧”的具体代码实现。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> timm.models <span class="keyword">import</span> create_model</span><br><span class="line">self.base_model = create_model()</span><br><span class="line"><span class="keyword">if</span> self.cfg.XVIT.USE_XVIT:</span><br><span class="line">  make_temporal_shift(</span><br><span class="line">      self.base_model,</span><br><span class="line">      self.cfg.XVIT.NUM_SEGMENTS,</span><br><span class="line">      n_div=self.cfg.XVIT.SHIFT_DIV,</span><br><span class="line">      locations_list=self.cfg.XVIT.LOCATIONS_LIST,</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></p>
<h2 id="注意力">注意力</h2>
<p>而注意力部分就是这篇论文的最主要的部分，相关代码在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/transformers/transformer_block.py">transformer_block.py</a>，可以看到大多数的代码与ViT的一样，主要在<code>Attention</code>类做出了修改，对k和v做了构造，以达到对全时空注意力的近似：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.insert_control_point:</span><br><span class="line">    k = self.control_point_query(k)</span><br><span class="line">    v = self.control_point_value(v)</span><br></pre></td></tr></table></figure>
其中的<code>control_point_query</code>和<code>control_point_value</code>是一个继承<code>nn.Identity</code>类的名为<code>AfterReconstruction</code>的类，起到一个占位的作用，而在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/temporal_shift.py">temporal_shift.py</a>中的make_temporal_shift函数会用<code>TemporalShift</code>类去替代这个<code>AfterReconstruction</code>。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_temporal_shift</span>(<span class="params">net, n_segment, n_div=<span class="number">8</span>, locations_list=[]</span>):</span><br><span class="line">    n_segment_list = [n_segment] * <span class="number">20</span></span><br><span class="line">    <span class="keyword">assert</span> n_segment_list[-<span class="number">1</span>] &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(net.blocks):</span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> locations_list:</span><br><span class="line">            net.blocks[idx].attn.control_point_query = TemporalShift(</span><br><span class="line">                net.blocks[idx].attn.control_point_query,</span><br><span class="line">                n_segment=n_segment_list[counter + <span class="number">2</span>],</span><br><span class="line">                n_div=n_div,</span><br><span class="line">            )</span><br><span class="line">            net.blocks[idx].attn.control_point_value = TemporalShift(</span><br><span class="line">                net.blocks[idx].attn.control_point_value,</span><br><span class="line">                n_segment=n_segment_list[counter + <span class="number">2</span>],</span><br><span class="line">                n_div=n_div,</span><br><span class="line">            )</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TemporalShift</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, net, n_segment=<span class="number">3</span>, n_div=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TemporalShift, self).__init__()</span><br><span class="line">        self.net = net</span><br><span class="line">        self.n_segment = n_segment</span><br><span class="line">        self.fold_div = n_div</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        nt, num_heads, d, c = x.size()</span><br><span class="line">        n_batch = nt // self.n_segment</span><br><span class="line">        x = x.view(n_batch, self.n_segment, num_heads, d, c)</span><br><span class="line">        fold = c * num_heads // self.fold_div</span><br><span class="line"></span><br><span class="line">        x = (</span><br><span class="line">            x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(n_batch, self.n_segment, num_heads * c, d)</span><br><span class="line">        )</span><br><span class="line">        out = torch.zeros_like(x)</span><br><span class="line">        out[:, :-<span class="number">1</span>, :fold] = x[:, <span class="number">1</span>:, :fold]  <span class="comment"># shift left</span></span><br><span class="line">        out[:, <span class="number">1</span>:, fold : <span class="number">2</span> * fold] = x[:, :-<span class="number">1</span>, fold : <span class="number">2</span> * fold]  <span class="comment"># shift right</span></span><br><span class="line">        out[:, :, <span class="number">2</span> * fold :] = x[:, :, <span class="number">2</span> * fold :]  <span class="comment"># not shift</span></span><br><span class="line"></span><br><span class="line">        out = (</span><br><span class="line">            out.view(n_batch, self.n_segment, num_heads, c, d)</span><br><span class="line">            .permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out.view(nt, num_heads, d, c)</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>TemporalShift</code>部分代码就是TSM的代码<a
target="_blank" rel="noopener" href="https://github.com/mit-han-lab/temporal-shift-module">temporal-shift-module</a>，也就是说XViT类似于ViT以TSM的方式去获得时序信息。</p>
<h2 id="temporal-attention-aggregation-1">Temporal Attention
aggregation</h2>
<p>论文采用轻量级的时间注意(TA)机制处理最后一层注意力层所输出的cls——token，相关代码在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/head_helper.py">head_helper.py</a>，如下所示
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, position_ids</span>):</span><br><span class="line">    <span class="comment"># temporal encoder (Longformer)</span></span><br><span class="line">    B, D, E = x.shape</span><br><span class="line"></span><br><span class="line">    cls_tokens = self.cls_token.expand(</span><br><span class="line">        B, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">    )  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">    x = self.temporal_encoder(x)</span><br><span class="line">    <span class="comment"># MLP head</span></span><br><span class="line">    x = self.mlp_head(x[:, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h1 id="from-chatgpt-just-for-fun">From ChatGPT (Just for fun)</h1>
<p>Summary: The paper proposes a video recognition model using
Transformers that scales linearly with the number of frames and avoids
computational overhead by approximating space-time attention with local
temporal windows and efficient space-time mixing.</p>
<p>Core Problems and Solutions:</p>
<ul>
<li>Computational overhead induced by modeling temporal information in
video recognition with Transformers. (Solution: proposing a model that
scales linearly with the number of frames by approximating space-time
attention with local temporal windows and efficient space-time
mixing).</li>
</ul>
<p>Main Insights and Lessons Learned:</p>
<ul>
<li>The proposed video recognition model achieves state-of-the-art
results while avoiding computational overheads.</li>
<li>The use of local temporal windows and efficient space-time mixing
can significantly reduce the complexity of video recognition
models.</li>
</ul>
<p>Questions:</p>
<ol type="1">
<li>How does the proposed video recognition model compare to existing
models in terms of recognition accuracy?</li>
<li>How does the model's efficiency compare to other approaches that
avoid computational overhead in video recognition?</li>
<li>Can the proposed approach be extended to other applications that use
video data, such as action recognition or video captioning?</li>
</ol>
<p>Future Research Directions:</p>
<ol type="1">
<li>Investigating the impact of the proposed model on different video
recognition tasks and datasets.</li>
<li>Exploring the use of local temporal windows and efficient space-time
mixing in other areas of video analysis, such as action recognition or
video captioning.</li>
<li>Extending the proposed approach to incorporate other types of
attention mechanisms or to combine it with other efficient architectures
for video recognition.</li>
</ol>
<p>Relevant Documents:</p>
<ol type="1">
<li>"Attention is All You Need" by Vaswani et al.</li>
<li>"Temporal Segment Networks: Towards Good Practices for Deep Action
Recognition" by Wang et al.</li>
<li>"Deep Residual Learning for Image Recognition" by He et al.</li>
<li>"A Closer Look at Spatiotemporal Convolutions for Action
Recognition" by Tran et al.</li>
<li>"Convolutional Sequence to Sequence Learning" by Gehring et
al.✏</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Transformer-based/" rel="tag"># Transformer-based</a>
              <a href="/tags/%E4%BB%A3%E7%A0%81/" rel="tag"># 代码</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/03/13/TimesFormer%E4%B8%8EViViT/" rel="prev" title="TimeSFormer与ViViT">
                  <i class="fa fa-chevron-left"></i> TimeSFormer与ViViT
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/03/23/MViT/" rel="next" title="MViT">
                  MViT <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
