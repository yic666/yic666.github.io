<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="2022 tpami 摘要 自廉价深度传感器问世以来，RGB-D视频中的人体动作识别(HAR)得到了广泛研究。目前，单模态方法(如基于骨架和基于RGB视频)已经在越来越大的数据集上实现了实质性的改进。然而，很少研究具有模型级融合的多模态方法。本文提出一种基于模型的多模态网络(MMNet)，通过一种基于模型的方法融合骨架和RGB模态。该方法的目标是通过有效地利用不同数据模态的互补信息来提高集成识">
<meta property="og:type" content="article">
<meta property="og:title" content="《MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos》阅读笔记">
<meta property="og:url" content="http://example.com/2023/01/08/MMNet_A_Model-based_Multimodal_Network_for_Human_Action_Recognition_in_RGB-D_Videos/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:description" content="2022 tpami 摘要 自廉价深度传感器问世以来，RGB-D视频中的人体动作识别(HAR)得到了广泛研究。目前，单模态方法(如基于骨架和基于RGB视频)已经在越来越大的数据集上实现了实质性的改进。然而，很少研究具有模型级融合的多模态方法。本文提出一种基于模型的多模态网络(MMNet)，通过一种基于模型的方法融合骨架和RGB模态。该方法的目标是通过有效地利用不同数据模态的互补信息来提高集成识">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-01-08T02:00:59.000Z">
<meta property="article:modified_time" content="2023-03-13T04:28:00.038Z">
<meta property="article:author" content="Yic-gdut">
<meta property="article:tag" content="动作识别">
<meta property="article:tag" content="论文笔记">
<meta property="article:tag" content="RGB-D Action Recognition">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2023/01/08/MMNet_A_Model-based_Multimodal_Network_for_Human_Action_Recognition_in_RGB-D_Videos/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2023/01/08/MMNet_A_Model-based_Multimodal_Network_for_Human_Action_Recognition_in_RGB-D_Videos/","path":"2023/01/08/MMNet_A_Model-based_Multimodal_Network_for_Human_Action_Recognition_in_RGB-D_Videos/","title":"《MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos》阅读笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>《MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos》阅读笔记 | Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yic</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F%E8%A8%80"><span class="nav-number">2.</span> <span class="nav-text">序言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">网络架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8Ergb%E6%A8%A1%E6%80%81%E6%9E%84%E5%BB%BAst-roi"><span class="nav-number">3.1.</span> <span class="nav-text">从RGB模态构建ST-ROI</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E9%AA%A8%E6%9E%B6%E6%A8%A1%E6%80%81%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%85%B3%E8%8A%82%E6%9D%83%E9%87%8D"><span class="nav-number">3.2.</span> <span class="nav-text">从骨架模态中学习关节权重</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97"><span class="nav-number">3.2.1.</span> <span class="nav-text">图卷积运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%B3%E8%8A%82%E6%9D%83%E9%87%8D"><span class="nav-number">3.2.2.</span> <span class="nav-text">关节权重</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%9E%8D%E5%90%88"><span class="nav-number">3.3.</span> <span class="nav-text">基于模型的融合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="nav-number">3.4.</span> <span class="nav-text">目标函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="nav-number">3.5.</span> <span class="nav-text">训练与优化</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/01/08/MMNet_A_Model-based_Multimodal_Network_for_Human_Action_Recognition_in_RGB-D_Videos/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="《MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos》阅读笔记 | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          《MMNet: A Model-based Multimodal Network for Human Action Recognition in RGB-D Videos》阅读笔记
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-01-08 10:00:59" itemprop="dateCreated datePublished" datetime="2023-01-08T10:00:59+08:00">2023-01-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-13 12:28:00" itemprop="dateModified" datetime="2023-03-13T12:28:00+08:00">2023-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>2022 tpami</p>
<h2 id="摘要">摘要</h2>
<p>自廉价深度传感器问世以来，RGB-D视频中的人体动作识别(HAR)得到了广泛研究。目前，单模态方法(如基于骨架和基于RGB视频)已经在越来越大的数据集上实现了实质性的改进。然而，很少研究具有模型级融合的多模态方法。本文提出一种基于模型的多模态网络(MMNet)，通过一种基于模型的方法融合骨架和RGB模态。该方法的目标是通过有效地利用不同数据模态的互补信息来提高集成识别的精度。对于基于模型的融合方案，我们对骨架模态使用时空图卷积网络来学习注意力权重，并将其迁移到RGB模态的网络中。在5个基准数据集上进行了广泛的实验:NTU
RGB+D 60、NTU RGB+D 120、PKU-MMD、Northwestern-UCLA Multiview和Toyota
smarhome。在聚合多个模态的结果后，发现所提出方法在五个数据集的六个评估协议上优于最先进的方法;因此，MMNet能够有效地捕获不同RGB-D视频模态中相互补充的特征，为HAR提供更具判别力的特征。在包含更多户外动作的RGB视频数据集Kinetics
400上测试了MMNet，结果与RGB- d视频数据集的结果一致。</p>
<h2 id="序言">序言</h2>
<p>使用骨架或RGB模态的单模态方法存在障碍:</p>
<ol type="1">
<li><p>基于RGB的方法的主要限制是缺乏3D结构；</p></li>
<li><p>基于骨骼的方法也受到缺乏纹理和外观特征的限制。</p></li>
</ol>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230105201002470.png"
alt="image-20230105201002470" />
<figcaption aria-hidden="true">image-20230105201002470</figcaption>
</figure>
<p>多模态HAR方法的核心任务是数据融合，可进一步分为数据级融合、特征级融合和决策级融合</p>
<p>除了这三种之外还有一种融合方法，称为协同学习</p>
<p>为了推进之前围绕协作学习的工作，本文提出了一种新的基于模型的多模态网络(MMNet)，在融合骨架和RGB模态时建模有效的知识转换，以提高RGB-
D视频中的人体动作识别</p>
<p>通过构建感兴趣的时空区域(STROI)特征图来关注整个RGB视频帧的不同外观特征，这种策略减轻了与大量视频数据相关的挑战。</p>
<p>从所提出的MMNet的骨架关节流Skeleton
Joints中衍生出一个注意力掩码，以关注提供互补特征的ST-ROI区域，可以提高RGB-D视频中人体动作的识别。</p>
<p>贡献：</p>
<ol type="1">
<li>首先，引入了一种多模态深度学习架构，在模型层次上用注意力机制融合不同的数据模态，并使用骨架骨流Skeleton
Bones。</li>
<li>其次，通过三个基准数据集证明，所提出方法大大提高了最先进的性能</li>
<li>通过对MMNet的两个关键参数进行分析，进一步验证了该方法的有效性。</li>
</ol>
<h2 id="网络架构">网络架构</h2>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230106140433610.png"
alt="image-20230106140433610" />
<figcaption aria-hidden="true">image-20230106140433610</figcaption>
</figure>
<p><span class="math inline">\(B^{(i)}\)</span>,<span
class="math inline">\(J^{(i)}\)</span>和<span
class="math inline">\(V^{(i)}\)</span>分别代表骨骼、骨骼关节和RGB视频的输入;</p>
<p><span
class="math inline">\(w^{(i)}\)</span>是来自骨骼关节图表示的空间注意力权重<span
class="math inline">\(\hat{J}^{(i)}\)</span>，它指导从RGB视频输入<span
class="math inline">\(V^{(i)}\)</span>转换为ST-ROI的焦点;</p>
<p>在这种基于模型的数据融合之后,以骨架为中心的ST-ROI<span
class="math inline">\(R^{\prime(i)}\)</span>将被馈送到ResNet以生成特定模式的预测;</p>
<p><span class="math inline">\(\hat{y}_{c}^{J^{(i)}}\)</span>和<span
class="math inline">\(\hat{y}_{c}^{B^{(i)}}\)</span>分别表示来自skeleton
joint 和 bone流的预测,这些预测通过RGB模态的特定预测<span
class="math inline">\(\hat{y}_{c}^{V^{(i)}}\)</span>进行聚合，以提供集成识别结果。</p>
<h3 id="从rgb模态构建st-roi">从RGB模态构建ST-ROI</h3>
<p>基于视频的模比如I3D和S3D等需要大量的RAM和GPU显存的计算资源，并且需要很长时间才能收敛,而较早的模型如C3D在NTU
RGB+D上则受限于数据量并不能有好的表现.因此,作者建议从RGB模态构建ST-ROI，并使用通用CNN模型从中检索有效特征。</p>
<p>以符号<span class="math inline">\(V=\left\{V^{(i)} \mid i=1, \ldots,
N\right\}\)</span>表示为有N个视频样本进行训练的RGB模态,那么可以表示出<span
class="math inline">\(V^{(i)}=\left(f_{1}^{(i)}, \ldots, f_{t}^{(i)},
\ldots, f_{T}^{(i)}\right)\)</span></p>
<p>其中<span class="math inline">\(f_{t}^{(i)}\)</span>是<span
class="math inline">\(t\)</span>帧。给定一个RGB帧<span
class="math inline">\(f_{t}^{(i)}\)</span>，作者定义了一个函数<span
class="math inline">\(g\)</span>来构建空间ROI<span
class="math inline">\(R_{t j}^{(i)}\)</span>为 <span
class="math display">\[
R_{t j}^{(i)}=g\left(f_{t}^{(i)}, o_{t j}^{(i)}\right), j
\in\left(m_{1}, \ldots, m_{M_{O}^{\prime}}\right), M_{O}^{\prime} \leq
M_{O}
\]</span> 其中<span class="math inline">\(O_{t
j}^{(i)}\)</span>为时刻t时OpenPose骨架的第j个关节。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230106193800339.png"
alt="image-20230106193800339" />
<figcaption aria-hidden="true">image-20230106193800339</figcaption>
</figure>
<p>如上图，在<span
class="math inline">\(V^{(i)}\)</span>进行时间采样，选择L个代表帧，将它们拼接成一个方形ST-ROI，如上图中的单受试者案例所示。对于有两个主体的动作，我们裁剪每个主体的ST-ROI，如上图中两个主体的情况所示。ST-ROI显著减少了RGB视频输入的数据量，同时保留了物体的外观和动作的运动信息。在<span
class="math inline">\(τ\)</span>时刻的时域子ROI将具有<span
class="math inline">\(M′\)</span>个空间子ROI,可以垂直连接并表示为<span
class="math inline">\(R_{\tau}^{(i)}\)</span>;相反，第<span
class="math inline">\(j\)</span>个关节的空间子ROI将具有<span
class="math inline">\(L\)</span>个时间子ROI，可以水平级联并表示为<span
class="math inline">\(R^{(i)}_ j\)</span>;最后对于<span
class="math inline">\(V^{(i)}\)</span>的ST-ROI可用<span
class="math inline">\(R_{(i)}\)</span>表示,包含<span
class="math inline">\(M&#39;\times L\)</span>个子ST-ROI<span
class="math inline">\(R_{\tau j}^{(i)}\)</span></p>
<h3 id="从骨架模态中学习关节权重">从骨架模态中学习关节权重</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230106204310906.png"
alt="image-20230106204310906" />
<figcaption aria-hidden="true">image-20230106204310906</figcaption>
</figure>
<p>(a)时空骨架图结构。(b)图卷积网络的空间采样策略。不同的颜色表示不同的子集:绿色星形表示顶点本身;黄色三角形表示离心力较远的子集;蓝色方块表示更接近的向心子集。</p>
<p>符号表示:</p>
<p><span class="math inline">\(J^{(i)}=\left(J_{1}^{(i)}, \ldots,
J_{t}^{(i)}, \ldots, J_{T}^{(i)}\right)\)</span>表示从时间<span
class="math inline">\(t=1\)</span>开始到时间<span
class="math inline">\(T\)</span>结束的第<span
class="math inline">\(i\)</span>个训练样本对应的<span
class="math inline">\(T\)</span>个骨架帧序列；</p>
<p><span class="math inline">\(B^{(i)}=\left(B_{1}^{(i)}, \ldots,
B_{t}^{(i)}, \ldots,
B_{T}^{(i)}\right)\)</span>表示由骨骼关节转化而来的骨骼的相应序列；</p>
<p><span class="math inline">\(J_{t}^{(i)}=\left(J_{t 1}^{(i)}, \ldots,
J_{t j}^{(i)}, \ldots, J_{t
M}^{(i)}\right)\)</span>表示给定一组t时刻观察到的骨架中的M个关节得到的骨架数据;</p>
<p>构造一个时空图来表示<span
class="math inline">\(J^{(i)}\)</span>的时空结构,即上图(a),其中单个骨架框架的关节和骨骼分别由图顶点([a]中的橙色圆圈)及其自然连接([a]中的紫色线)表示。两个相邻的骨架通过关节之间的边连接起来([a]中的黑线虚线)。图顶点的属性可以是每个关节对应的三维坐标。骨架输入<span
class="math inline">\(J^{(i)}\)</span>的骨架图可以符号化为<span
class="math inline">\(\mathcal{G}=(\mathcal{V},
\mathcal{E})\)</span>，其中<span
class="math inline">\(\mathcal{V}\)</span>和<span
class="math inline">\(\mathcal{E}\)</span>分别表示关节和骨骼。</p>
<h4 id="图卷积运算">图卷积运算</h4>
<p>为表示卷积操作的采样区域，一个节点<span
class="math inline">\(v_{ti}\)</span>的邻居集被定义为<span
class="math inline">\(N\left(v_{t i}\right)=\left\{v_{t j} \mid
d\left(v_{t i}, v_{t j}\right) \leq D\right\}\)</span>,其中D是<span
class="math inline">\(d\left(v_{t i}, v_{t
j}\right)\)</span>的最大路径长度.该策略如图4(b)所示，其中×表示骨架的重心,采样面积<span
class="math inline">\(N\left(v_{t i}\right)\)</span>被曲线包围.</p>
<p>假设在邻居集中有固定数量的K个子集,它们将被映射为数字标记<span
class="math inline">\(l_{t i}: N\left(v_{t i}\right) \rightarrow\{0,
\ldots, K-1\}\)</span>;在时间上，邻域概念扩展到时间连接的关节，如<span
class="math inline">\(N\left(v_{t i}\right)=\left\{v_{q
j}\left|d\left(v_{t j}, v_{t i}\right) \leq K,\right| q-t \mid \leq
\Gamma / 2\right\}\)</span>,其中<span
class="math inline">\(\Gamma\)</span>是控制邻居集的时间范围的时间内核大小。这样图卷积就可以计算为
<span class="math display">\[
\hat{v}_{t i}=\sum_{v_{t j} \in N\left(v_{t i}\right)} \frac{1}{Z_{t
i}\left(v_{t j}\right)} f_{i n}\left(v_{t j}\right)
\mathbf{w}\left(l\left(v_{t j}\right)\right)
\]</span> 其中<span class="math inline">\(f_{i n}\left(v_{t
j}\right)\)</span>为获取<span
class="math inline">\(v_{tj}\)</span>的属性向量的特征映射，<span
class="math inline">\(\mathbf{w}\left(l\left(v_{t
j}\right)\right)\)</span>是权重函数<span
class="math inline">\(\mathbf{w}\left(v_{t i}, v_{t
j}\right)\)</span>:<span class="math inline">\(N\left(v_{t i}\right)
\rightarrow \mathbb{R}^{C}\)</span>可以用<span
class="math inline">\((C,K)\)</span>维张量实现;<span
class="math inline">\(Z_{t i}\left(v_{t j}\right)=\left|v_{t k}\right|
l_{t i}\left(v_{t k}\right)=l_{t i}\left(v_{t j}\right)
\mid\)</span>等于相应子集的基数，这是一个归一化项。</p>
<h4 id="关节权重">关节权重</h4>
<p>对骨架模态应用图卷积后，图上每个顶点的输出可以用来推断相应骨架节点的重要性。骨架序列的特征映射可以用(C,
T,
M)维的张量表示，其中C表示关节顶点的属性个数，T表示时间长度，M表示顶点个数。这种划分策略可以用一个邻接矩阵A来表示，矩阵A中的元素表示一个顶点<span
class="math inline">\(v_{ti}\)</span>是否属于<span
class="math inline">\(N(v_{ti})\)</span>的子集。因此，图卷积可以使用<span
class="math inline">\(1 \times
\Gamma\)</span>经典二维卷积，并通过在二维上将所得张量乘以归一化邻接矩阵<span
class="math inline">\(\boldsymbol{\Lambda}^{-\frac{1}{2}} \mathbf{A}
\boldsymbol{\Lambda}^{-\frac{1}{2}}\)</span>来实现。若采用K种分区策略<span
class="math inline">\(\sum_{k=1}^{K}
\mathbf{A}_{k}\)</span>，图卷积的公式可被转换为 <span
class="math display">\[
\hat{J}^{(i)}=\sum_{k=1}^{K} \boldsymbol{\Lambda}^{-\frac{1}{2}}
\mathbf{A} \boldsymbol{\Lambda}^{-\frac{1}{2}} f_{i
n}\left(J^{(i)}\right) \mathbf{W}_{k} \odot \mathbf{M}_{k}
\]</span></p>
<p>其中<span class="math inline">\(\boldsymbol{\Lambda}_{k}^{i
i}=\sum_{j}\left(\mathbf{A}_{k}^{i
j}\right)+\alpha\)</span>为对角矩阵且<span
class="math inline">\(\alpha\)</span>被设为0.001以避免空行；<span
class="math inline">\(\mathbf{W}_{k}\)</span>是一个具有<span
class="math inline">\((C_in, C_out, 1,1)\)</span>维的1 x
1卷积运算的权重张量，它表示方程3的权重函数；<span
class="math inline">\(\mathbf{M}_{k}\)</span>是与<span
class="math inline">\(A_k\)</span>相同大小的注意力图，表明了每个顶点的重要性;<span
class="math inline">\(\odot\)</span>表示两个矩阵的元素乘积；<span
class="math inline">\(\hat{J}^{(i)}\)</span>是一个大小为<span
class="math inline">\((c, t, M)\)</span>的张量，其中<span
class="math inline">\(c\)</span>是输出通道数，<span
class="math inline">\(t\)</span>是输出时间长度，<span
class="math inline">\(M\)</span>是顶点数。该张量可用于推断动作类别，并可转换为关节权重，为RGB模态提供注意力知识。代表其相应身体面积重要性的关节权重可以计算为
<span class="math display">\[
w^{(i)}=\frac{1}{c t} \sum_{1}^{c} \sum_{1}^{t} \sqrt{\left(\hat{J}_{c
t}^{(i)}\right)^{2}}
\]</span>
其中t和c分别为卷积图的输出维数，分别表示时间长度和输出通道。<span
class="math inline">\(w^{(i)}\)</span>是包含M个不同骨架关节权重的向量。</p>
<h3 id="基于模型的融合">基于模型的融合</h3>
<p>本文提出一种RGB帧的空间权重机制，使机器能够关注将提供判别式信息的RGB特征，更明确的是，机器将更有能力，因为它直观地模仿了人眼的动作识别。本文选择使用来自骨架模态的关节权重，并将其乘以ST-ROI来正则化RGB模态。可以将第i个训练样本的骨架聚焦的ST-ROI(记为<span
class="math inline">\(R^{&#39;(i)}\)</span>)从<span
class="math inline">\(R^{(i)}\)</span>映射出来，函数<span
class="math inline">\(h\)</span>定义为 <span class="math display">\[
R^{\prime(i)}=h\left(R_{j}^{(i)}, w_{j}^{(i)}\right), j=m_{1}^{\prime},
\ldots, m_{M^{\prime}}^{\prime}, M^{\prime}&lt;M
\]</span> 其中<span class="math inline">\(w_{j}\)</span>为第<span
class="math inline">\(j\)</span>个关节的权重，<span
class="math inline">\(R_{j}^{(i)}\)</span>为对应身体区域的子空间ROI。而<span
class="math inline">\(m_{1}^{\prime}, \ldots,
m_{M^{\prime}}^{\prime}\)</span>是<span
class="math inline">\(M’\)</span>个不同骨骼关节对应于建议关注的身体区域的指数。<span
class="math inline">\(M &#39;\)</span>的值等于公式2中<span
class="math inline">\(M &#39;
_O\)</span>的值。公式6的数据融合过程如下图所示。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230107180156852.png"
alt="image-20230107180156852" />
<figcaption aria-hidden="true">image-20230107180156852</figcaption>
</figure>
<h3 id="目标函数">目标函数</h3>
<p>用动作标签监督的损失项集合的和构建了MMNet的端到端格式，表示为 <span
class="math display">\[
\mathcal{L}=\mathcal{L}_{J}\left(\hat{y}^{J},
y\right)+\mathcal{L}_{B}\left(\hat{y}^{B},
y\right)+\mathcal{L}_{V}\left(\hat{y}^{V}, y\right)
\]</span> 其中<span
class="math inline">\(\mathcal{L}_{J},\mathcal{L}_{B},\mathcal{L}_{V}\)</span>分别为骨架关节、骨架骨骼、RGB视频输入的损失项。</p>
<p>将骨架关节输入输入到公式4中引入的图卷积模型中。因此，骨骼关节的预测可以定义为
<span class="math display">\[
\hat{y}^{J^{(i)}}=\sigma\left(G_{J}\left(\Theta_{J},
J^{(i)}\right)\right)
\]</span> 其中<span
class="math inline">\(G_J\)</span>表示式4定义的图卷积运算；<span
class="math inline">\(Θ_J\)</span>为GCN子模型的可学习参数；<span
class="math inline">\(J^{(i)}\)</span>是骨骼关节输入的数据样本。而<span
class="math inline">\(σ\)</span>表示一个线性层，将子模型输出的形状转换为one
- hot表示</p>
<p>骨骼输入基本上是骨骼关节输入的转换。将同样的图卷积运算方法应用于骨骼输入，可以表示为
<span class="math display">\[
\hat{y}^{B^{(i)}}=\sigma\left(G_{B}\left(\Theta_{B},
B^{(i)}\right)\right)
\]</span>
本文提出了ST-ROI作为RGB视频输入的转换形式，它可以大幅减少数据量，并保持HAR的核心判别信息。由于ST-ROI本质上是一个二维特征图，便采用ResNet如下
<span class="math display">\[
\hat{y}^{V^{(i)}}=\sigma\left(G_{V}\left(R^{\prime(i)},
\Theta_{V}\right)+R^{\prime(i)}\right)
\]</span> 其中<span class="math inline">\(G_{V}\left(R^{\prime(i)},
\Theta_{V}\right)\)</span>表示待学习的残差映射，ΘV表示基于ResNet层数的可学习参数</p>
<p>给定上述子模型预测的定义，根据以下目标制定优化问题: <span
class="math display">\[
\begin{array}{c}
\underset{\Theta_{B}}{\arg \min }-\sum_{i=1}^{N} \sum_{c=1}^{N_{c}}
\underbrace{y_{c} \log
\left(\hat{y}_{c}^{B^{(i)}}\right)}_{\mathcal{L}_{B}} \\
\underset{\Theta_{J}}{\arg \min }-\sum_{i=1}^{N} \sum_{c=1}^{N_{c}}
\underbrace{y_{c} \log
\left(\hat{y}_{c}^{J^{(i)}}\right)}_{\mathcal{L}_{J}} \\
\underset{\Theta_{V}}{\arg \min }-\sum_{i=1}^{N} \sum_{c=1}^{N_{c}}
\underbrace{y_{c} \log
\left(\hat{y}_{c}^{V^{(i)}}\right)}_{\mathcal{L}_{V}}
\end{array}
\]</span> 其中<span
class="math inline">\(\mathcal{L}\)</span>是交叉熵损失函数，<span
class="math inline">\(N_c\)</span>是特定数据集中动作类的数量，<span
class="math inline">\(N\)</span>表示训练集中的样本个数。</p>
<h3 id="训练与优化">训练与优化</h3>
<p>为了追求更高的识别精度，还可以采用其他几个损失项作为关节权重。但本文依旧采用了关节权重的普通实现，作为RGB模态的空间注意力，以验证新颖的基于模型的数据融合机制的有效性。给定目标函数，使用随机梯度下降(SGD)求解方程11、12和13。网络<span
class="math inline">\(G_J\)</span>可以预训练，也可以与<span
class="math inline">\(G_V\)</span>同时训练，以获得空间注意力权重以进行特征融合。子模型<span
class="math inline">\(G_J\)</span>和<span
class="math inline">\(G_V\)</span>可以通过将<span
class="math inline">\(Θ_J\)</span>和<span
class="math inline">\(Θ_V\)</span>一起调优来进行端到端训练，或者简单地通过修改<span
class="math inline">\(Θ_J\)</span>来更新<span
class="math inline">\(Θ_V\)</span>。同时，对骨骼的网络<span
class="math inline">\(G_B\)</span>进行单独训练，并将其聚合到<span
class="math inline">\(G_J\)</span>和<span
class="math inline">\(G_V\)</span>的结果中，从而实现集成预测。具体训练步骤如算法1所示。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230107205547516.png"
alt="image-20230107205547516" />
<figcaption aria-hidden="true">image-20230107205547516</figcaption>
</figure>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" rel="tag"># 动作识别</a>
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" rel="tag"># 论文笔记</a>
              <a href="/tags/RGB-D-Action-Recognition/" rel="tag"># RGB-D Action Recognition</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/10/18/%E3%80%8ADynamic-Spatio-Temporal-Specialization-Learning-for-Fine-Grained-Action-Recognition%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" rel="prev" title="《Dynamic Spatio-Temporal Specialization Learning for Fine-Grained Action Recognition》阅读笔记">
                  <i class="fa fa-chevron-left"></i> 《Dynamic Spatio-Temporal Specialization Learning for Fine-Grained Action Recognition》阅读笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/02/19/Zhang_Temporal_Query_Networks_for_Fine-Grained_Video_Understanding_CVPR_2021_paper/" rel="next" title="《Temporal Query Networks for Fine-grained Video Understanding》阅读笔记">
                  《Temporal Query Networks for Fine-grained Video Understanding》阅读笔记 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
