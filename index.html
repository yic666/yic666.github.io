<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Yic">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Yic">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yic-gdut">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Yic</title>
  





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Yic</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yic-gdut</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/22/Hypergraph-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/22/Hypergraph-Transformer/" class="post-title-link" itemprop="url">Hypergraph Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-22 14:05:24" itemprop="dateCreated datePublished" datetime="2023-08-22T14:05:24+08:00">2023-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-23 14:57:28" itemprop="dateModified" datetime="2023-08-23T14:57:28+08:00">2023-08-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" itemprop="url" rel="index"><span itemprop="name">动作识别</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>paper: https://arxiv.org/abs/2211.09590</p>
<p>code: https://github.com/ZhouYuxuanYX/Hyperformer</p>
<h1 id="摘要">摘要</h1>
<p>骨架基础的动作识别旨在给定人体关节坐标及其骨架互连来识别人类动作。通过将关节定义为顶点，其自然连接定义为边，先前的工作成功地采用了图卷积网络(GCN)来建模关节的共现，并获得了出色的性能。更近期，发现了GCN的一个限制，即拓扑结构在训练后是固定的。为了放松这种限制，采用了自注意力(SA)机制使GCN的拓扑结构对输入变得自适应，产生了目前最好的混合模型。同时，也尝试了简单的Transformer，但由于缺乏结构先验，它们仍落后于目前最好的基于GCN的方法。与混合模型不同，我们提出了一种更优雅的方法，通过图距离嵌入将骨连接性结构融入Transformer。我们的嵌入在训练期间保留了骨架结构的信息，而GCN仅将其用于初始化。更重要的是，我们揭示了图模型通常存在的一个潜在问题，即成对聚合从本质上忽略了身体关节之间的高阶运动依赖性。为弥补这一空白，我们在超图上提出了一种新的自注意力(SA)机制，称为超图自注意力(HyperSA)，以融入内在的高阶关系。我们将结果模型称为Hyperformer，它在NTU
RGB+D、NTU RGB+D
120和Northwestern-UCLA数据集上都优于目前最好的图模型，在精度和效率方面。</p>
<h1 id="引言">引言</h1>
<p>论文背景:
骨骼动作识别是一项重要的研究领域，通过使用骨骼关节坐标来识别人类动作。过去的研究主要采用图卷积网络（GCNs）来建模关节之间的关联，并取得了较好的性能。然而，GCNs存在拓扑结构固定的限制，无法灵活适应输入数据。为了解决这个问题，本研究引入了自注意机制，使得GCNs的拓扑结构能够根据输入数据自适应调整，从而提高了模型的性能。</p>
<p>过去方案:
过去的研究主要采用图卷积网络（GCNs）来建模关节之间的关联，并取得了较好的性能。然而，GCNs存在拓扑结构固定的限制，无法灵活适应输入数据。为了解决这个问题，一些研究尝试使用Transformer模型，但由于缺乏结构先验知识，其性能仍然落后于GCN-based方法。</p>
<p>论文的Motivation:
鉴于GCNs和Transformer模型在骨骼动作识别中的局限性，本研究旨在提出一种更优雅的解决方案，将骨骼关联信息引入Transformer模型中。通过引入基于图距离的相对位置嵌入和超图自注意机制，该模型能够更好地捕捉骨骼关节之间的高阶关系，并在准确性和效率方面超越现有的图模型。</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/1.png" /></p>
<p>论文的Contribution： 1.
提出通过基于图距离的相对位置嵌入将人体骨架的结构信息融入Transformer,利用了Transformer与目前最先进的混合模型之间的差距
2.
基于超图表示设计了一种新的自注意力(SA)变体,称为超图自注意力(HyperSA)。据作者所知,他们的工作是第一种将超图表示应用于基于骨架的动作识别的工作,它考虑了成对和高阶关节关系,超越了当前最先进的方法（并不是第一种）
3.
根据所提出的相对位置嵌入和HyperSA构建了一个轻量级的Transformer。它在基于骨架的动作识别基准测试中优于目前最好的图模型,无论是效率还是准确率方面。</p>
<p>在latex源码，作者在引言第三段“This can be attributed to the fact that
the formulation of the vanilla Transformer ignores the unique
characteristics of skeleton data, i.e.,”
注释掉了一部分原有的内容，注释者认为“对应这个问题的改动很小”，注释内容翻译如下：</p>
<blockquote>
<p>Transformer假设输入标记是同质的,然而人体关节本质上是异质的,例如,每个物理关节发挥独特的作用,因此与其他关节有不同的关系。这些固有关系在不同的动作中持续存在,与输入的关节坐标或动作类别无关。骨连接性:Transformer依赖排列不变的注意力操作,这会忽略位置信息。普通的注意力操作假设排列不变性,因此破坏了位置信息。为了缓解这个问题,绝对位置嵌入被广泛使用。然而,它们无法表示人体关节之间复杂的骨连接关系。与绝对位置嵌入相比,相对位置嵌入被证明在语言、视觉和图数据等各种任务上的Transformer中更优越,因为它们保留了比前者更多的结构信息。</p>
</blockquote>
<h1 id="preliminaries">Preliminaries</h1>
<h2 id="自注意力">自注意力</h2>
<p>给定输入序列<span
class="math inline">\(X=(\vec{x}_1,...,\vec{x}_n)\)</span>,每个标记<span
class="math inline">\(\vec{x}_i\)</span>先被映射到关键表示 <span
class="math inline">\(\vec{k}_i\)</span> , 查询表示 <span
class="math inline">\(\vec{q}_i\)</span> 和值表示 <span
class="math inline">\(\vec{v}_i\)</span>。然后通过<span
class="math inline">\(\vec{q}_i\)</span> 和 <span
class="math inline">\(\vec{k}j\)</span>
的点乘经softmax函数计算出两个标记间的注意力分数<span
class="math inline">\(A{ij}\)</span>: <span class="math display">\[
\begin{equation}
A_{ij} = \vec{q}_i \cdot \vec{k}_j^\top,
\end{equation}
\]</span> 每个位置的最终输出是所有值表示的加权和: <span
class="math display">\[
\begin{equation}
\vec{y}_i = \sum_{j=1}^n A_{ij}\vec{v}_j
\end{equation}
\]</span> 多头自注意力(Multi-Head Self-Attention,
MHSA)是Transformer的常用扩展,分成多个子空间进行自注意力的计算。</p>
<h2 id="超图表示">超图表示</h2>
<p>与标准图边不同,超图中的超边连接两个或多个顶点。一个无权超图定义为<span
class="math inline">\(\mathcal{H} = (\mathcal{V}, \mathcal{E})\)</span>,
由顶点集<span class="math inline">\(\mathcal{V}\)</span>和超边集<span
class="math inline">\(\mathcal{E}\)</span>组成。超图<span
class="math inline">\(\mathcal{H}\)</span>可以用一个<span
class="math inline">\(|\mathcal{V}| \times |\mathcal{E}|\)</span>
的关联矩阵<span
class="math inline">\(H\)</span>表示,其中的元素定义如下:</p>
<p>$$ <span class="math display">\[\begin{equation}

h_{v, e}=\left\{
    \begin{aligned}
    1, \quad if \quad v \in e\\
    0, \quad if \quad v \notin e\\
    \end{aligned}\right
    .
\end{equation}\]</span> $$</p>
<p>顶点<span class="math inline">\(v \in
\mathcal{V}\)</span>的度定义为<span class="math inline">\(d(v) = \sum_{e
\in \mathcal{E}} h_{v, e}\)</span>, 超边<span class="math inline">\(e
\in \mathcal{E}\)</span>的度定义为<span class="math inline">\(d(e) =
\sum_{v \in \mathcal{V}} h_{v, e}\)</span>。度矩阵<span
class="math inline">\(D_e\)</span>和<span
class="math inline">\(D_v\)</span>分别将所有超边的度和所有顶点的度设为对角元素。</p>
<p>在本工作中,考虑所有顶点<span
class="math inline">\(d(v)=1\)</span>的特殊情况,即身体关节被划分为<span
class="math inline">\(\vert \mathcal{E}
\vert\)</span>个不相交的子集,这在实践中很高效。值得注意的是,在这种情况下,关联矩阵<span
class="math inline">\(H\)</span>等效于一个分割矩阵。每行是一个one
hot向量,表示每个关节所属的组。</p>
<h1 id="方法">方法</h1>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/3.png" /></p>
<h2 id="派生超边特征">派生超边特征</h2>
<p>给定一个关联矩阵<span
class="math inline">\(H\)</span>,作者提出了一种有效的方法来获得连接到每个超边的关节子集的特征表示。令<span
class="math inline">\(C\)</span>表示特征维数,每个关节的特征<span
class="math inline">\(X \in \mathbb{R}^{\vert \mathcal{V} \vert \times
C}\)</span> 首先通过以下规则聚合为子集表示<span class="math inline">\(E
\in \mathbb{R}^{\vert \mathcal{E} \vert \times C}\)</span>: <span
class="math display">\[
\begin{equation}
     E = D_e^{-1}H^\top XW_{e},
\end{equation}
\]</span> <!-- % 解释一下HX是求和的含义， -->
<!-- % D_e-1 代表求平均 --></p>
<p>其中: 1. 关联矩阵<span class="math inline">\(H\)</span>与输入<span
class="math inline">\(X\)</span>的乘积本质上是将每个子集中属于该子集的关节特征求和。
2. 超边的度矩阵的逆用于归一化的目的。 3. 投影矩阵<span
class="math inline">\(W_{e} \in \mathbb{R}^{C\times
C}\)</span>进一步转换每个超边的特征以获得它们的最终表示。</p>
<p>然后我们通过将超边表示分配给每个关联关节的位置,构造增强的超边表示<span
class="math inline">\(E_{aug} \in \mathbb{R}^{\vert \mathcal{V} \vert
\times C}\)</span>:</p>
<p><span class="math display">\[
\begin{equation}
    E_{aug} = HD_e^{-1}H^\top XW_{e}.
\end{equation}
\]</span></p>
<h2 id="人体骨骼结构编码">人体骨骼结构编码</h2>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/18/3Mformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/18/3Mformer/" class="post-title-link" itemprop="url">3Mformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-18 12:53:38" itemprop="dateCreated datePublished" datetime="2023-07-18T12:53:38+08:00">2023-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-23 15:11:27" itemprop="dateModified" datetime="2023-08-23T15:11:27+08:00">2023-08-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%8A%A8%E4%BD%9C%E8%AF%86%E5%88%AB/" itemprop="url" rel="index"><span itemprop="name">动作识别</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>paper: https://arxiv.org/abs/2303.14474</p>
<h1 id="摘要">摘要</h1>
<p>许多骨骼动作识别模型使用图卷积网络（GCNs）通过连接身体部位的三维关节来表示人体。GCNs聚合一或少数跳的图邻域，并忽略未连接的身体关节之间的依赖关系。我们提出使用超图来建模图节点之间的超边（例如，三阶和四阶超边捕捉三个和四个节点），从而帮助捕捉身体关节组的高阶运动模式。我们将动作序列分割为时间块，Higher-order
Transformer（HoT）根据（i）身体关节，（ii）身体关节之间的成对连接，以及（iii）骨骼身体关节的高阶超边，生成每个时间块的嵌入。我们通过一种新颖的多阶多模Transformer（3Mformer）结合这些超边的HoT嵌入，该Transformer具有两个模块，可以交换顺序，实现基于“通道-时间块”、“顺序-通道-身体关节”、“通道-超边（任意阶）”和“仅通道”对上的耦合模式注意力。第一个模块称为多阶汇聚（MP），还可以学习沿着超边模式的加权汇聚，而第二个模块称为时间块汇聚（TP），则沿着时间块1模式进行汇聚。我们的端到端可训练网络相对于基于GCN、Transformer和超图的对应方法获得了最先进的结果。</p>
<h1 id="引言">引言</h1>
<p>论文背景:
骨骼动作识别在视频监控、人机交互、体育分析和虚拟现实等领域具有广泛应用。与基于视频的方法不同，骨骼序列通过表示3D身体关节的时空演变，对传感器噪声具有鲁棒性，并且在计算和存储效率上更高效。</p>
<p>过去方案:
过去的图形模型主要通过图卷积网络（GCN）或图神经网络（GNN）来处理骨骼数据。然而，这些方法忽略了非连接的关节之间的依赖关系，并且对于捕捉更高阶的运动模式有限。</p>
<p>论文的Motivation:
鉴于现有方法的局限性，本研究旨在提出一种新的模型来更好地表示骨骼数据，并捕捉关节之间的高阶动态。通过构建超图来表示骨骼数据，并使用多阶多模态变压器进行耦合模式注意力，以实现对不同模态的关注。</p>
<p>论文的Contribution：</p>
<ol type="1">
<li>将骨骼数据建模为阶数为1到r的超图（集合、图和/或超图），其中人体关节作为节点。这样形成的超边的Higher-order
Transformer嵌入表示了各种3D身体关节的组合，并捕捉了对于动作识别非常重要的各种高阶动态。</li>
<li>由于HoT嵌入表示了各个超边的阶数和时间块，引入了一种新颖的Multi-order
Multi-mode Transformer (3Mformer)。它包含两个模块，即Multi-order
Pooling和Temporal block
Pooling，其目标是形成诸如'通道-时间块'、'顺序-通道-身体关节'、'通道-超边（任意阶）'和'仅通道'等耦合模式tokens，并进行加权超边聚合和时间块聚合。</li>
</ol>
<h1 id="背景">背景</h1>
<h2 id="符号表示">符号表示</h2>
<p><span class="math inline">\(\mathcal{I}_{K}\)</span>代表索引集合<span
class="math inline">\({1,2,\cdots,K}\)</span>。大写粗体符号表示矩阵（二阶张量）或高阶张量（超过两个模式）。小写粗体符号表示向量，普通字体表示标量。
<span class="math inline">\(\mathcal{I}_{K}\)</span>代表索引集合<span
class="math inline">\({1,2,\cdots,K}\)</span>。普通字体表示标量；向量用小写粗体字母表示，例如<span
class="math inline">\(\textbf{x}\)</span>；矩阵用大写粗体字母表示，例如<span
class="math inline">\(\textbf{M}\)</span>；张量用花体字母表示，例如<span
class="math inline">\(\vec{\mathcal{M}}\)</span>。 <span
class="math inline">\(r\)</span>阶张量表示为<span
class="math inline">\(\vec{\mathcal{M}} \in \mathbb{R}^{I_1 \times I_2
\times \cdots \times I_r}\)</span>， <span
class="math inline">\(\vec{\mathcal{M}}\)</span>的第<span
class="math inline">\(m\)</span>模式矩阵化表示为<span
class="math inline">\(\vec{\mathcal{M}}_{(m)}\in \mathbb{R}^{I_m \times
(I_1 \cdots I_{m-1} I_{m+1} \cdots I_{r})}\)</span>。</p>
<h2 id="transformer">Transformer</h2>
<p>Transformer编码器层<span class="math inline">\(f : \mathbb{R}^{J
\times d} \rightarrow \mathbb{R}^{J \times d}\)</span>包含两个子层：(i)
自注意力<span class="math inline">\(a : \mathbb{R}^{J \times d}
\rightarrow \mathbb{R}^{J \times d}\)</span>和(ii) 逐元素的前馈网络<span
class="math inline">\(\text{MLP} :\mathbb{R}^{J \times d} \rightarrow
\mathbb{R}^{J \times d}\)</span>。对于具有<span
class="math inline">\(J\)</span>个节点的集合，其中<span
class="math inline">\({\bf X} \in \mathbb{R}^{J \times
d}\)</span>，<span class="math inline">\({\bf x}_i\)</span>是节点<span
class="math inline">\(i\)</span>的特征向量，一个Transformer层(为简洁起见，省略了<span
class="math inline">\(a(\cdot)\)</span>和MLP<span
class="math inline">\((\cdot)\)</span>后的归一化)计算如下： <span
class="math display">\[
\begin{align}
    &amp; a({\bf x}_i) = {\bf x}_i +
\sum_{h=1}^H\sum_{j=1}^J\alpha_{ij}^h{\bf x}_j{\bf W}_h^V{\bf W}_h^O, \\
    &amp; f({\bf x}_i) = a({\bf x}_i) + \text{MLP}(a({\bf X}))_i,
\end{align}
\]</span> 其中<span class="math inline">\(H\)</span>和<span
class="math inline">\(d_H\)</span>分别表示注意力头的数量和头的大小，<span
class="math inline">\({\boldsymbol{\alpha}}^h = \sigma\big({\bf X}{\bf
W}_h^Q({\bf X}{\bf W}_h^{K})^\top\big)\)</span>是注意力系数，<span
class="math inline">\({\bf W}_h^O \in \mathbb{R}^{d_H \times
d}\)</span>，<span class="math inline">\({\bf W}_h^V\)</span>，<span
class="math inline">\({\bf W}_h^K\)</span>，<span
class="math inline">\({\bf W}_h^Q \in \mathbb{R}^{d \times
d_H}\)</span>。</p>
<h2 id="higher-order-transformer">Higher-order transformer</h2>
<p>设HoT层为<span class="math inline">\(f_{m\rightarrow n}
:\mathbb{R}^{J^m \times d} \rightarrow \mathbb{R}^{J^n \times
d}\)</span>，其中包含两个子层：(i) 高阶自注意力<span
class="math inline">\(a_{m\rightarrow n} :\mathbb{R}^{J^m \times d}
\rightarrow \mathbb{R}^{J^n \times d}\)</span>和(ii) 前馈网络<span
class="math inline">\(\text{MLP}_{n\rightarrow n} :\mathbb{R}^{J^n
\times d} \rightarrow \mathbb{R}^{J^n \times
d}\)</span>。此外，引入索引向量<span class="math inline">\({\bf
i}\in\mathcal{I}_{J}^m\equiv\mathcal{I}_{J} \times \mathcal{I}_{J}
\times \cdots \times \mathcal{I}_{J}\)</span>（<span
class="math inline">\(m\)</span>个模式）和<span
class="math inline">\({\bf j}\in\mathcal{I}_{J}^n\equiv\mathcal{I}_{J}
\times \mathcal{I}_{J} \times \cdots \times
\mathcal{I}_{J}\)</span>（<span
class="math inline">\(n\)</span>个模式）。对于输入张量<span
class="math inline">\({\bf X} \in \mathbb{R}^{J^m \times
d}\)</span>，其中超边的阶数为<span
class="math inline">\(m\)</span>，HoT层的计算如下：</p>
<p><span class="math display">\[
\begin{align}
    &amp; a_{m \rightarrow n}(\mathbf{X})_{j}=\sum_{h=1}^{H} \sum_{\mu}
\sum_{i} \alpha_{i, j}^{h, \mu} \mathbf{X}_{i} \mathbf{W}_{h, \mu}^{V}
\mathbf{W}_{h, \mu}^{o} \\
    &amp; \operatorname{MLP}_{n \rightarrow n}\left(a_{m \rightarrow
n}(\mathbf{X})\right)=\mathrm{L}_{n \rightarrow
n}^{2}\left(\operatorname{ReLU}\left(\mathrm{L}_{n \rightarrow
n}^{1}\left(a_{m \rightarrow n}(\mathbf{X})\right)\right)\right), \\
    &amp; f_{m \rightarrow n}(\mathbf{X})=a_{m \rightarrow
n}(\mathbf{X})+\operatorname{MLP}_{n \rightarrow n}\left(a_{m
\rightarrow n}(\mathbf{X})\right),
\end{align}
\]</span> 其中<span class="math inline">\({\boldsymbol \alpha}^{h, \mu}
\in
\mathbb{R}^{J^{m+n}}\)</span>是具有多个头部的所谓注意力系数张量，<span
class="math inline">\({\boldsymbol \alpha}^{h,
\mu}_{\mathbf{i},\mathbf{j}} \in
\mathbb{R}^{J}\)</span>是一个向量，<span class="math inline">\({\bf
W}_{h, \mu}^V \in \mathbb{R}^{d \times d_H}\)</span>和<span
class="math inline">\({\bf W}_{h, \mu}^O \in \mathbb{R}^{d_H \times
d}\)</span>是可学习的参数。此外，<span
class="math inline">\(\mu\)</span>在相同节点分区中的阶-<span
class="math inline">\((m+n)\)</span>的等价类上进行索引，<span
class="math inline">\(\text{L}_{n\rightarrow n}^1 :\mathbb{R}^{J^n
\times d}\rightarrow \mathbb{R}^{J^n \times d_F}\)</span>和<span
class="math inline">\(\text{L}_{n\rightarrow n}^2 :\mathbb{R}^{J^n
\times d_F}\rightarrow \mathbb{R}^{J^n \times
d}\)</span>是等变线性层，<span
class="math inline">\(d_F\)</span>是隐藏维度。</p>
<p>为了从阶数为<span class="math inline">\(m\)</span>的输入张量<span
class="math inline">\({\bf X} \in \mathbb{R}^{J^m \times
d}\)</span>中计算每个注意力张量<span class="math inline">\({\boldsymbol
\alpha}^{h,\mu} \in
\mathbb{R}^{J^{m+n}}\)</span>，根据高阶query和key，我们有： $$ <span
class="math display">\[\begin{equation}
  {\boldsymbol{\alpha}_{\boldsymbol i, \boldsymbol j}^{h,\mu}}  =
    \begin{cases}
      \frac{\sigma({\bf Q}_{\boldsymbol j}^{h,\mu}, {\bf K}_{\boldsymbol
i}^{h,\mu})}{Z_{\boldsymbol j}}\;\quad({\boldsymbol i}, {\boldsymbol
j})  \in  \mu\\
      \quad\quad 0 \quad\quad\;\text{otherwise},
    \end{cases}
    
\end{equation}\]</span> $$ 其中<span class="math inline">\({\bf Q}^\mu =
\text{L}_{m\rightarrow n}^\mu({\bf X})\)</span>，<span
class="math inline">\({\bf K}^\mu = \text{L}_{m\rightarrow m}^\mu({\bf
X})\)</span>，归一化常数<span class="math inline">\(Z_{\boldsymbol j} =
\sum_{\boldsymbol i:({\boldsymbol i}, {\boldsymbol j})\in
\mu}\sigma({\bf Q}_{\boldsymbol j}^\mu, {\bf K}_{\boldsymbol
i}^\mu)\)</span>。最后，可以将Eq(6)中的核注意力近似为具有RKHS特征映射<span
class="math inline">\(\psi\in\mathbb{R}_{+}^{d_K}\)</span>以提高效率，其中<span
class="math inline">\(d_K\ll d_H\)</span>。具体而言，有<span
class="math inline">\(\sigma({\bf Q}_{\boldsymbol j}^{h,\mu}, {\bf
K}_{\boldsymbol i}^{h,\mu})\approx{\boldsymbol \psi}({\bf
Q}_{\boldsymbol j}^{h,\mu})^\top{\boldsymbol \psi}({\bf K}_{\boldsymbol
i}^{h,\mu})\)</span>。选择了performer核，因为它在理论和实证上都有保证。</p>
<p>由于query和key张量是使用等变线性层从输入张量<span
class="math inline">\({\bf
X}\)</span>计算得到的，因此Transformer编码器层<span
class="math inline">\(f_{m\rightarrow n}\)</span>满足排列等变性。</p>
<h1 id="方法">方法</h1>
<h2 id="模型概览">模型概览</h2>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image.png" /></p>
<p>如图所示，框架包含一个简单的三层MLP单元（全连接层，ReLU激活函数，全连接层，ReLU激活函数，Dropout层，全连接层），三个针对每种输入类型（即，身体关节特征集、身体关节的图和超图）的HoT块，接着是具有两个模块（i）多阶池化（MP）和（ii）时间块池化（TP）的多阶多模态Transformer（3Mformer）。</p>
<p>3Mformer的目标是形成耦合模式tokens（稍后会解释其含义），例如“通道-时间块”、“序号-通道-身体关节”、“通道-超边（任意序号）”和“仅通道”，并进行加权超边聚合和时间块聚合。它们的输出进一步连接并传递到一个全连接层进行分类。</p>
<p><strong>MLP 单元</strong>:MLP单元接受<span
class="math inline">\(T\)</span>个相邻的帧，每个帧具有<span
class="math inline">\(J\)</span>个2D/3D骨骼身体关节，形成一个时间块。总共，取决于步长<span
class="math inline">\(S\)</span>，我们得到一些<span
class="math inline">\(\tau\)</span>个时间块（一个块捕获短期时间演变）。相比之下，长期时间演变由HoT和3Mformer建模。每个时间块由MLP编码成一个<span
class="math inline">\(d\times J\)</span>维的特征图。</p>
<p><strong>HoT 分支</strong>：我们将 HoT（Hypergraph on Transformer） 的
<span class="math inline">\(r\)</span>
个分支堆叠在一起，每个分支接收维度为 <span class="math inline">\({\bf
X}_t\in\mathbb{R}^{d \times J}\)</span> 的嵌入，其中 <span
class="math inline">\(t\in\mathcal{I}_{\tau}\)</span> 表示时间块。每个
HoT 分支输出大小为 <span
class="math inline">\(m\in\mathcal{I}_{r}\)</span> 的超边特征表示，记为
<span class="math inline">\({\bf\Phi}&#39;_m\in\mathbb{R}^{J^m \times
d&#39;}\)</span>，其中 <span
class="math inline">\(m\in\mathcal{I}_{r}\)</span> 表示阶数。</p>
<p>对于一阶、二阶和更高阶的流输出 <span
class="math inline">\({\bf\Phi}&#39;_1,\cdots,{\bf\Phi}&#39;_r\)</span>，我们进行以下步骤：(i)
交换特征通道和超边模式，(ii)
提取张量的上三角部分，然后在块-时间模式上进行连接，这样我们得到 <span
class="math inline">\({\bf\Phi}_m\in\mathbb{R}^{d&#39;\times
N_{E_m}\times\tau}\)</span>，其中 <span class="math inline">\(N_{E_m} =
\binom{J}{m}\)</span>。随后，我们沿着超边模式连接 <span
class="math inline">\({\bf\Phi}_1,\cdots,{\bf\Phi}_r\)</span>，得到一个多阶特征张量
<span class="math inline">\(\vec{\mathcal{M}} \in \mathbb{R}^{d&#39;
\times N \times \tau}\)</span>，其中所有阶数的超边总数为 <span
class="math inline">\(N=\sum_{m=1}^r\binom{J}{m}\)</span>。</p>
<p><strong>3Mformer</strong>：我们使用具有耦合模式自注意力（CmSA）的多阶多模式Transformer（3Mformer）来融合多阶特征张量
<span class="math inline">\(\vec{\mathcal{M}}\)</span>
中的信息流，并最终将3Mformer的输出传递给分类器进行分类。</p>
<h2 id="coupled-mode-self-attention">Coupled-mode Self-Attention</h2>
<h3 id="耦合模式tokenscoupled-mode-tokens">耦合模式tokens(Coupled-mode
tokens)</h3>
<p>我们受到标准Vision
Transformer（ViT）中单类别tokens的注意区域的启发，这些区域可以用来形成一个与类别无关的本地化映射(参考
https://zhuanlan.zhihu.com/p/481304916)。研究了Transformer模型是否也能够有效地捕捉耦合模式注意力，用于更具有区分性的分类任务，例如通过学习Transformer内的耦合模式tokens来进行基于张量骨架的动作识别。为此，提出了一个多阶多模式Transformer（3Mformer），它使用耦合模式tokens来共同学习通道模式、块-时间模式、身体关节模式和阶数模式之间的各种高阶运动动态。3Mformer能够成功地从CmSA机制中生成对应于不同tokens的耦合模式关系。接下来，介绍CmSA机制。</p>
<p>给定阶数为 <span class="math inline">\(r\)</span> 的张量 <span
class="math inline">\(\vec{\mathcal{M}} \in \mathbb{R}^{I_1 \times I_2
\times \cdots \times I_r}\)</span>，为了形成耦合模式tokens，我们对 <span
class="math inline">\(\vec{\mathcal{M}}\)</span> 进行模式-<span
class="math inline">\(m\)</span>的矩阵化，得到 <span
class="math inline">\(\textbf{M} \equiv \vec{\mathcal{M}}_{(m)}^\top \in
\mathbb{R}^{(I_1 \cdots I_{m-1} I_{m+1} \cdots I_{r}) \times
I_m}\)</span>，然后从 <span class="math inline">\(\textbf{M}\)</span>
形成耦合tokens。</p>
<p>举例来说，对于一个给定的三阶张量，它具有特征通道模式、超边模式和时间块模式，我们可以形成以下tokens对：</p>
<ol type="1">
<li>`channel-temporal block'：特征通道-时间块对</li>
<li>`channel-hyper-edge (any order)'：特征通道-超边 (任意阶数)对</li>
<li>`channel-only'：仅特征通道对</li>
</ol>
<p>另外，如果给定的张量被用作输入并输出一个产生新模式（例如，身体关节模式）的新张量，我们可以形成以下tokens：</p>
<p>`order-channel-body joint'：阶数-特征通道-身体关节对</p>
<p>在接下来的部分，为了简化起见，使用“reshape”来进行张量的矩阵化，以形成不同类型的联合模式令牌。</p>
<p>联合模式自注意力（JmSA）定义如下： <span class="math display">\[
\begin{equation}
      a(\mathbf{Q}, \mathbf{K},
\mathbf{V})=\text{SoftMax}\left(\frac{\mathbf{Q
K}^{\top}}{\sqrt{d_{K}}}\right) \mathbf{V}
\end{equation}
\]</span></p>
<p>其中，<span class="math inline">\(\sqrt{d_{K}}\)</span>
是缩放因子，<span class="math inline">\({\bf Q} = {\bf W}^q{\bf
M}\)</span>，<span class="math inline">\({\bf K} = {\bf W}^k{\bf
M}\)</span> 和 <span class="math inline">\({\bf V} = {\bf W}^v{\bf
M}\)</span> 分别是查询（query）、键（key）和值（value）向量，而 <span
class="math inline">\(\textbf{M} \equiv
\vec{\mathcal{M}}{(m)}^\top\)</span>。此外，<span
class="math inline">\({\bf Q}\)</span>、<span class="math inline">\({\bf
K}\)</span>、<span class="math inline">\({\bf V} \in \mathbb{R}^{(I_1
\cdots I_{m-1} I_{m+1} \cdots I_{r}) \times I_m}\)</span>，<span
class="math inline">\({\bf W}^q\)</span>、<span
class="math inline">\({\bf W}^k\)</span>、<span
class="math inline">\({\bf W}^v \in \mathbb{R}^{(I_1 \cdots I_{m-1}
I_{m+1} \cdots I_{r}) \times (I_1 \cdots I_{m-1} I_{m+1} \cdots
I_{r})}\)</span>
是可学习的权重。我们注意到不同的联合模式标记具有不同的“注意力焦点”机制，并且我们在我们的3Mformer中应用它们来融合多阶特征表示。</p>
<h2 id="multi-order-multi-mode-transformer">Multi-order Multi-mode
Transformer</h2>
<h3 id="multi-order-pooling-mp-module">Multi-order Pooling (MP)
Module</h3>
<p><strong>CmSA in MP</strong>：我们将多阶特征表示<span
class="math inline">\(\vec{\mathcal{M}} \in \mathbb{R}^{d&#39; \times N
\times \tau}\)</span>重塑为<span class="math inline">\({\bf M} \in
\mathbb{R}^{d&#39;\tau \times
N}\)</span>（或者将后面TP中解释的输出重塑为<span
class="math inline">\({\bf M}&#39; \in \mathbb{R}^{d&#39; \times
N}\)</span>），从而使模型能够关注不同类型的特征表示。让我们简单地记<span
class="math inline">\(d&#39;&#39; = d&#39;\tau\)</span>（或者<span
class="math inline">\(d&#39;&#39; =
d&#39;\)</span>，具体取决于输入的来源）。我们形成了一个耦合模式的self-attention（如果<span
class="math inline">\(d&#39;&#39;=d&#39;\tau\)</span>，我们有"channel-temporal
block"的token；如果<span
class="math inline">\(d&#39;&#39;=d&#39;\)</span>，我们有"channel-only"的token）。
<span class="math display">\[
\begin{equation}
  a_{\mathrm{MP}}\left(\mathbf{Q}_{\mathrm{MP}},
\mathbf{K}_{\mathrm{MP}},
\mathbf{V}_{\mathrm{MP}}\right)=\operatorname{SoftMax}\left(\frac{\mathbf{Q}_{\mathrm{MP}}
\mathbf{K}_{\mathrm{MP}}^{\top}}{\sqrt{d_{K_{\mathrm{MP}}}}}\right)
\mathbf{V}_{\mathrm{MP}} \text {, }
\end{equation}
\]</span></p>
<p>其中，<span class="math inline">\(\sqrt{d_{K_\text{MP}}}\)</span>
是缩放因子，<span class="math inline">\({\bf Q}_\text{MP}\!=\!{\bf
W}_\text{MP}^q{\bf M}\)</span>，<span class="math inline">\({\bf
K}_\text{MP}\!=\!{\bf W}_\text{MP}^k{\bf M}\)</span> 和 <span
class="math inline">\({\bf V}_\text{MP}\!=\!{\bf W}_\text{MP}^v{\bf
M}\)</span>（我们可以使用 <span class="math inline">\({\bf M}\)</span>
或者 <span class="math inline">\({\bf
M}&#39;\)</span>）分别是查询、键和值。此外，<span
class="math inline">\({\bf Q}_\text{MP}\)</span>，<span
class="math inline">\({\bf K}_\text{MP}\)</span>，<span
class="math inline">\({\bf V}_\text{MP}\!\in\!
\mathbb{R}^{d&#39;&#39;\times N}\)</span> 和 <span
class="math inline">\({\bf W}_\text{MP}^q\)</span>，<span
class="math inline">\({\bf W}_\text{MP}^k\)</span>，<span
class="math inline">\({\bf W}_\text{MP}^v\!\in\!
\mathbb{R}^{d&#39;&#39;\times d&#39;&#39;}\)</span>
是可学习的权重。方程式(8)是一种自注意层，它基于所谓的耦合模式令牌的
<span class="math inline">\({\bf Q}_\text{MP}\)</span> 和 <span
class="math inline">\({\bf K}_\text{MP}\)</span> 令牌嵌入之间的相关性对
<span class="math inline">\({\bf V}_\text{MP}\)</span>
进行重新加权。</p>
<p><strong>Weighted pooling</strong>:(8)中的注意力层产生特征表示 <span
class="math inline">\({\bf O}_\text{MP}\!\in\!
\mathbb{R}^{d&#39;&#39;\times
N}\)</span>，以增强例如特征通道与身体关节之间的关系。随后，我们通过对多个阶数
<span class="math inline">\(m\in\mathcal{I}_{r}\)</span>
的超边进行加权池化来处理多个阶数的超边的影响： $$ <span
class="math display">\[\begin{align}
    &amp; {\bf O}_\text{MP}^{*(m)}\!=\!{\bf O}_\text{MP}^{(m)}{\bf
H}^{(m)}\!\in\!  \mathbb{R}^{d&#39;&#39;\times J}, %~(\text{or}~{\bf
O}_\text{MP}^{*&#39;(k)}\!=\!{\bf O}_\text{MP}^{&#39;(k)}{\bf
H}^{(k)}\!\in\! \mathbb{R}^{d&#39;\times J})

\end{align}\]</span> $$</p>
<p>其中，<span class="math inline">\({\bf O}_\text{MP}^{(m)}\!\in\!
\mathbb{R}^{d&#39;&#39;\times N_{E_m}}\)</span> 是从 <span
class="math inline">\({\bf O}_\text{MP}\)</span> 中简单地提取出阶数为
<span class="math inline">\(m\)</span> 的超边的特征表示，矩阵 <span
class="math inline">\({\bf H}^{(m)}\!\in\! \mathbb{R}^{N_{E_m}\times
J}\)</span> 是可学习的权重，用于对阶数为 <span
class="math inline">\(m\)</span>
的超边进行加权池化。最后，通过简单地连接 <span
class="math inline">\({\bf O}_\text{MP}^{*(1)},\cdots,{\bf
O}_\text{MP}^{*(r)}\)</span>，我们得到 <span class="math inline">\({\bf
O}_\text{MP}^{*}\!\in\! \mathbb{R}^{r{d&#39;&#39;\times
J}}\)</span>。如果我们使用了从 TP 到 MP 的输入，则将 MP 的输出表示为
<span
class="math inline">\({\mathbf{O}&#39;}_\text{MP}^{*}\)</span>。</p>
<h3 id="temporal-block-pooling-tp-module">Temporal block Pooling (TP)
Module</h3>
<p><strong>CmSA in TP</strong>：首先，我们将多阶特征表示 <span
class="math inline">\(\vec{\mathcal{M}}\!\in\!
\mathbb{R}^{d&#39;\!\times\!N\!\times\!\tau}\)</span> 重新整形为 <span
class="math inline">\({\bf M}\!\in\!
\mathbb{R}^{d&#39;N\!\times\!\tau}\)</span>（或者将来自 MP
的输出重新整形为 <span class="math inline">\({\bf M}&#39;&#39;\!\in\!
\mathbb{R}^{rd&#39;J\!\times\!\tau}\)</span>）。为简单起见，我们在第一种情况下记
<span
class="math inline">\(d&#39;&#39;&#39;\!=\!d&#39;N\)</span>，在第二种情况下记
<span
class="math inline">\(d&#39;&#39;&#39;\!=\!rd&#39;J\)</span>。在第一种情况下，重新整形后的输入的第一模式用于形成令牌，它们再次是耦合模式令牌，例如“通道-超边”和“阶-通道-身体关节”令牌，分别对应不同的表示意义。
此外，TP（可能是指某种处理方式或模块）还沿着块-时间模式（沿 <span
class="math inline">\(\tau\)</span>
方向）执行池化操作。我们形成一个耦合模式自注意力： <span
class="math display">\[
\begin{equation}
    a_\text{TP}({\bf Q}_\text{TP}, {\bf K}_\text{TP}, {\bf V}_\text{TP})
=\text{SoftMax}\left(\frac{\mathbf{Q}_\text{TP}\mathbf{K}_\text{TP}^\top}{\sqrt{d_{K_\text{TP}}}}\right)\mathbf{V}_\text{TP},
\end{equation}
\]</span> 这里，<span
class="math inline">\(\sqrt{d_{K_\text{TP}}}\)</span> 是缩放因子，<span
class="math inline">\({\bf Q}_\text{TP} = {\bf W}_\text{TP}^q{\bf
M}\)</span>，<span class="math inline">\({\bf K}_\text{TP} = {\bf
W}_\text{TP}^k{\bf M}\)</span> 和 <span class="math inline">\({\bf
V}_\text{TP} = {\bf W}_\text{TP}^v{\bf M}\)</span>（我们可以使用 <span
class="math inline">\({\bf M}\)</span> 或者 <span
class="math inline">\({\bf
M}&#39;&#39;\)</span>）分别是查询、键和值。此外，<span
class="math inline">\({\bf Q}_\text{TP}\)</span>，<span
class="math inline">\({\bf K}_\text{TP}\)</span>，<span
class="math inline">\({\bf V}_\text{TP} \in
\mathbb{R}^{d&#39;&#39;&#39;\times \tau}\)</span> （或者 <span
class="math inline">\(\mathbb{R}^{3d&#39;J\!\times\!\tau}\)</span>） 和
<span class="math inline">\({\bf W}_\text{TP}^q\)</span>，<span
class="math inline">\({\bf W}_\text{TP}^k\)</span>，<span
class="math inline">\({\bf W}_\text{TP}^v \in
\mathbb{R}^{d&#39;&#39;&#39;\times d&#39;&#39;&#39;}\)</span> （或者
<span class="math inline">\(\mathbb{R}^{3d&#39;J\times
3d&#39;J}\)</span>）是可学习的权重。方程式(10)重新加权 <span
class="math inline">\({\bf
V}_\text{TP}\)</span>，其基于联合模式令牌（例如“通道-超边”或“阶-通道-身体关节”）的
<span class="math inline">\({\bf Q}_\text{TP}\)</span> 和 <span
class="math inline">\({\bf K}_\text{TP}\)</span>
令牌嵌入之间的相关性。注意力的输出是时间表示 <span
class="math inline">\({\bf O}_\text{TP} \in
\mathbb{R}^{d&#39;&#39;&#39;\times \tau}\)</span>。如果我们使用 <span
class="math inline">\({\bf M}&#39;&#39;\)</span>
作为输入，则将输出表示为 <span class="math inline">\({\bf
O}&#39;&#39;_\text{TP}\)</span>。</p>
<p><strong>Pooling step</strong>:在给定时间表示 <span
class="math inline">\({\bf
O}_\text{TP}\!\in\!\mathbb{R}^{d&#39;&#39;&#39;\!\times\!\tau}\)</span>（或者
<span class="math inline">\({\bf
O}&#39;&#39;_\text{TP}\)</span>）后，我们在块-时间模式（即 <span
class="math inline">\(\tau\)</span>
方向）上应用池化操作，以获得与骨骼序列长度（块数量 <span
class="math inline">\(\tau\)</span>）无关的紧凑特征表示。有许多池化操作(我们没有提出池化算子，而是选择了一些流行的算子，以比较它们对
TP
的影响)。，包括一阶的（例如平均池化、最大池化、求和池化）、二阶的（如注意力池化）、高阶的（三线性池化）
和排序池化。</p>
<p>池化后的输出是 <span class="math inline">\({\bf
O}^*_\text{TP}\!\in\!\mathbb{R}^{d&#39;&#39;&#39;}\)</span>（或者 $</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/28/SVFormer-Semi-supervised-Video-Transformer-for-Action-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/28/SVFormer-Semi-supervised-Video-Transformer-for-Action-Recognition/" class="post-title-link" itemprop="url">SVFormer: Semi-supervised Video Transformer for Action Recognition</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-28 09:24:39" itemprop="dateCreated datePublished" datetime="2023-06-28T09:24:39+08:00">2023-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-18 13:04:17" itemprop="dateModified" datetime="2023-07-18T13:04:17+08:00">2023-07-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>SVFormer: Semi-supervised Video Transformer for Action
Recognition</p>
<p>paper：http://arxiv.org/abs/2211.13222</p>
<p>code：https://github.com/ChenHsing/SVFormer</p>
<h1 id="摘要">摘要</h1>
<p>半监督动作识别是一项具有挑战性但又至关重要的任务，因为视频注释的成本很高。现有方法主要使用卷积神经网络，然而当前的革命性视觉Transformer模型尚未得到充分探索。在本文中，我们研究了在半监督学习（SSL）设置下使用Transformer模型进行动作识别的方法。为此，我们引入了SVFormer，它采用了稳定的伪标签框架（即EMATeacher）来处理无标签视频样本。虽然广泛的数据增强方法已被证明对于半监督图像分类是有效的，但对于视频识别而言，它们通常产生有限的结果。因此，我们引入了一种针对视频数据的新型增强策略，称为Tube
Token-Mix，其中视频剪辑通过掩码和一致的遮蔽标记在时间轴上混合。此外，我们提出了一种时域扭曲增强方法，用于覆盖视频中复杂的时域变化，它将选定的帧在剪辑中拉伸到不同的时间长度。对三个数据集Kinetics-400、UCF-101和HMDB-51进行了大量实验证实了SVFormer的优势。特别是，在Kinetics-400的1%标注率下，SVFormer在较少的训练周期内比现有技术提升了31.5%。我们的方法有望作为一个强有力的基准，并鼓励未来在使用Transformer网络进行半监督动作识别方面的研究。</p>
<h1 id="引言">引言</h1>
<p>论文背景：目前，视频在互联网上逐渐取代了图像和文字，并以指数级的速度增长。有监督的视频理解已经取得了巨大的成功，但这些工作依赖于大规模手工标注，因此利用现成的无标签视频来更好地理解视频是非常重要的。</p>
<p>过去方案：半监督方法通常基于伪标签的模式，通常是用已标记数据来预训练网络，然后利用预训练的模型为未标记的数据生成伪标签，最后使用伪标签进一步改进预训练模型。学界通常采用额外模态（比如光流）或者辅助网络的方式提高伪标签的质量，但这类方法会带来额外的训练或推理成本。</p>
<p>论文的Motivation：鉴于TransFormer架构在视频领域的巨大成功，而原有的SSL方法（如Mixup和CutMix）并不适用于TransFormer架构，本文旨在提出一种基于TransFormer的半监督动作识别方法，并提出一种适用于TransFormer架构的增强方法，能够更好地对token之间的时间相关性进行建模。另外，本文还提出一种时间扭曲增强方法可以覆盖视频中复杂的时间变化。</p>
<p>论文的Contribution：</p>
<ol type="1">
<li>率先探索了半监督视频识别的变压器模型。</li>
<li>提出了一种token级增强方法Tube
Token-Mix，它比像素级混合策略更适合视频Transformer。</li>
<li>在三个基准数据集上进行了广泛的实验，达到了SOTA。</li>
</ol>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230628113047.png" /></p>
<h1 id="方法">方法</h1>
<h2 id="ssl设置">SSL设置</h2>
<p>假设我们有N个训练视频样本，包括N_L个带有标签的视频样本<span
class="math inline">\((x_l,y_l) \in \mathcal{D}_L\)</span>和<span
class="math inline">\(N_U\)</span>个无标签的视频样本<span
class="math inline">\(x_u \in \mathcal{D}_U\)</span>，其中<span
class="math inline">\(x_l\)</span>是带有类别标签<span
class="math inline">\(y_l\)</span>的标记视频样本，<span
class="math inline">\(x_u\)</span>是无标签的视频样本。通常情况下，<span
class="math inline">\(N_U \gg N_L\)</span>。半监督学习的目标是利用<span
class="math inline">\(\mathcal{D}_L\)</span>和<span
class="math inline">\(\mathcal{D}_U\)</span>来训练模型。</p>
<h2 id="pipeline">Pipeline</h2>
<p>SVFormer采用了流行的半监督学习框架FixMatch，该框架利用两个不同增强视图之间的一致性损失。训练范式分为两个部分。</p>
<p>对于标记集合<span class="math inline">\(\{(x_l,
y_l)\}_{l=1}^{N_L}\)</span>，模型优化有监督损失<span
class="math inline">\({\cal{L}}_{s}\)</span>： $$ <span
class="math display">\[\begin{equation}

{\cal{L}}_{s}=\frac{1}{N_L}\sum_{}^{N_L}{\cal{H}}({\cal{F}}
({x_l}),y_l),
\end{equation}\]</span> $$ 其中<span
class="math inline">\({\cal{F}}(\cdot)\)</span>表示模型产生的预测，<span
class="math inline">\({\cal{H}}\)</span>是标准的交叉熵损失函数。</p>
<p>对于无标签样本<span
class="math inline">\(x_u\)</span>，我们首先使用弱增强（例如，随机水平翻转、随机缩放和随机裁剪）和强增强（例如，AutoAugment或Dropout）分别生成两个视图，<span
class="math inline">\(x_w={\cal{A}}_{weak}(x_u)\)</span>，<span
class="math inline">\(x_s={\cal{A}}_{strong}(x_u)\)</span>。然后，利用模型生成的弱视图的伪标签<span
class="math inline">\({\hat{y}_w}=\arg\max({\cal{F}}(x_w))\)</span>来监督强视图，使用以下无监督损失：
<span class="math display">\[
\begin{equation}
{\cal{L}}_{un}=\frac{1}{N_U}\sum^{N_U}{\mathbb{I}} (\max({\cal{F}}(x_w))
&gt; \delta){\cal{H}}({\cal{F}}({x_s}),\hat{y}_w),
\end{equation}
\]</span> 其中<span
class="math inline">\(\delta\)</span>是预定义的阈值，<span
class="math inline">\({\mathbb{I}}\)</span>是指示函数，当最大类别概率超过<span
class="math inline">\(\delta\)</span>时，它的值为1，否则为0。置信度指标用于过滤嘈杂的伪标签。</p>
<p>在FixMatch中，两个增强的输入共享同一个模型，这容易导致模型容易崩溃
。因此，论文采用了指数移动平均（EMA）-Teacher，这是FixMatch的改进版本。
伪标签是由EMA-Teacher模型生成的，该模型的参数通过对学生参数进行指数移动平均来更新，具体表示为：
<span class="math display">\[
\begin{equation}
{\theta}_t \gets m {\theta}_t +(1- m) {\theta}_s,
\end{equation}
\]</span> 其中<span
class="math inline">\(m\)</span>是一个动量系数，<span
class="math inline">\(\theta_{t}\)</span>和<span
class="math inline">\(\theta_{s}\)</span>分别是教师模型和学生模型的参数。</p>
<h2 id="tube-tokenmix">Tube TokenMix</h2>
<p>在半监督框架中，一个核心问题是如何使用高质量的伪标签丰富数据集。
Mixup
是一种广泛采用的数据增强策略，它通过以下方式在样本和标签之间执行凸组合：
<span class="math display">\[
\begin{equation}
  \hat{x} = \lambda \cdot x_1 + (1-\lambda) \cdot x_2,
\end{equation}
\]</span> <span class="math display">\[
\begin{equation}
  \hat{y} = \lambda \cdot y_1 + (1-\lambda) \cdot y_2,
\end{equation}
\]</span> 其中比例 <span class="math inline">\(\lambda\)</span>
是一个符合贝塔分布的标量。Mixup及其变种（例如CutMix）在低数据情况下的许多任务中取得了成功，例如长尾分类、域自适应、少样本学习等等。对于半监督学习，Mixup
通过在图像分类中混合无标签样本的伪标签也表现良好。</p>
<h3 id="视频mixing">视频Mixing</h3>
<p>对于基于TransFormer的方法，原本像素级的mixing增强方法（Mixup
或CutMix）并不适用于token级的模型。因此论文提出三种用于视频数据的mixing增强方法：Rand
TokenMix、Frame TokenMix和Tube TokenMix。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230629104912.png"
alt="Tube TokenMix训练框架" />
<figcaption aria-hidden="true">Tube TokenMix训练框架</figcaption>
</figure>
<p>给定未标记的视频片段<span class="math inline">\(x_a, x_b\in
\mathbb{R}^{H\times W\times T}\)</span>，使用一个基于标记的掩码<span
class="math inline">\(\textbf{M}\in\{0,1\}^{H\times W\times
T}\)</span>来执行样本混合。这里<span
class="math inline">\(H\)</span>和<span
class="math inline">\(W\)</span>分别表示经过分块标记化后的帧的高度和宽度，<span
class="math inline">\(T\)</span>表示片段长度。为了生成一个新的样本<span
class="math inline">\(x_{mix}\)</span>，在进行强数据增强<span
class="math inline">\({\cal{A}}_{strong}\)</span>后，按照以下方式混合<span
class="math inline">\(x_a\)</span>和<span
class="math inline">\(x_b\)</span>： <span class="math display">\[
\begin{equation}
    x_{mix} = {\cal{A}}_{strong}(x_a) \odot \textbf{M} +
{\cal{A}}_{strong}(x_b) \odot (\textbf{1}-\textbf{M}),
\end{equation}
\]</span> 其中<span
class="math inline">\(\odot\)</span>表示逐元素相乘，<span
class="math inline">\(\textbf{1}\)</span>是一个全为1的二值掩码。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230706152632.png"
alt="Masks增强例子" />
<figcaption aria-hidden="true">Masks增强例子</figcaption>
</figure>
<p>在三种增强方法中，掩码<span
class="math inline">\(\textbf{M}\)</span>的处理方式不同，如图所示。
对于Rand TokenMix，被掩盖的标记是从整个视频剪辑中（<span
class="math inline">\(H\times W\times T\)</span>个标记）随机选择的。
对于Frame TokenMix，从<span
class="math inline">\(T\)</span>个帧中随机选择一些帧，并将这些帧中的所有标记进行掩盖。
对于Tube
TokenMix，采用了管道式的掩盖策略，即不同帧共享相同的空间掩码矩阵。在这种情况下，掩码<span
class="math inline">\(\textbf{M}\)</span>在时间轴上具有一致的掩盖标记。</p>
<p>利用上述的增强方法可以掩码混合两个片段并合成一个新的混合数据样本，然后，混合样本<span
class="math inline">\(x_{mix}\)</span>被馈送到学生模型<span
class="math inline">\({\cal{F}}_{s}\)</span>，得到模型预测<span
class="math inline">\(y_{mix} = {\cal{F}}_{s}(x_{mix})\)</span>。
此外，通过将弱增强样本<span
class="math inline">\({\cal{A}}_{weak}(x_a)\)</span>和<span
class="math inline">\({\cal{A}}_{weak}(x_b)\)</span>输入到教师模型<span
class="math inline">\({\cal{F}}_{t}\)</span>中，产生<span
class="math inline">\(x_a\)</span>和<span
class="math inline">\(x_b\)</span>的伪标签<span
class="math inline">\(\hat{y}_a,\hat{y}_b\)</span>： <span
class="math display">\[
\begin{equation}
    \hat{y}_a = \arg\max({\cal{F}}_{t}({\cal{A}}_{weak}(x_a))),
\end{equation}
\]</span> <span class="math display">\[
\begin{equation}
    \hat{y}_b = \arg\max({\cal{F}}_{t}({\cal{A}}_{weak}(x_b))).   
\end{equation}
\]</span> 注意，如果<span
class="math inline">\(\max({\cal{F}}_{t}({\cal{A}}_{weak}(x)))&lt;\delta\)</span>，则伪标签<span
class="math inline">\(\hat{y}\)</span>保持软标签<span
class="math inline">\({\cal{F}}_{t}({\cal{A}}_{weak}(x))\)</span>不变。
对于<span class="math inline">\(x_{mix}\)</span>，伪标签<span
class="math inline">\(\hat{y}_{mix}\)</span>通过使用掩码比例<span
class="math inline">\(\lambda\)</span>混合<span
class="math inline">\(\hat{y}_a\)</span>和<span
class="math inline">\(\hat{y}_b\)</span>生成： <span
class="math display">\[
\begin{equation}
    \hat{y}_{mix} = \lambda \cdot \hat{y}_a + (1-\lambda) \cdot
\hat{y}_b.
\end{equation}
\]</span> 最后，学生模型通过以下一致性损失进行优化： $$ <span
class="math display">\[\begin{equation}

    {\cal{L}}_{mix}= \frac{1}{N_{m}} \sum^{N_{m}}(\hat{y}_{mix} -
y_{mix})^2,
\end{equation}\]</span> $$ 其中<span
class="math inline">\(N_{m}\)</span>是混合样本的数量。
TTMix的一致性损失算法显示在算法:Consistency loss for Tube
TokenMix中。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230706153406.png"
alt="Consistency loss for Tube TokenMix" />
<figcaption aria-hidden="true">Consistency loss for Tube
TokenMix</figcaption>
</figure>
<h3 id="时间扭曲增强-temporal-warping-augmentation">时间扭曲增强
Temporal Warping Augmentation</h3>
<p>作者提出扭曲每个帧的时间持续性，从而将更高的随机性引入数据中。时间扭曲增强（TWAug）可以将一个帧的时间长度拉伸到各种不同的值。
给定一个包含<span
class="math inline">\(T\)</span>帧（例如，8帧）的提取视频片段，随机决定保留所有帧，或者选择一小部分帧（例如，2或4帧），并遮盖其他帧。
然后，被遮盖的帧会用随机的相邻可见（未遮盖）帧进行填充。
请注意，在进行时间填充后，帧的顺序仍然保持不变。
下图分别显示了选择2、4和8帧的三个示例。
提出的TWAug可以帮助模型在训练过程中学习灵活的时间动态。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230706160835.png"
alt="时间扭曲增强" />
<figcaption aria-hidden="true">时间扭曲增强</figcaption>
</figure>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230706161103.png"
alt="TTMix中使用时间扭曲增强和空间增强示例" />
<figcaption
aria-hidden="true">TTMix中使用时间扭曲增强和空间增强示例</figcaption>
</figure>
<h2 id="训练模式">训练模式</h2>
<p>SVFormer的训练由三个部分组成：由式（1）给出的有监督损失、由式（2）给出的无监督伪标签一致性损失以及由式（10）给出的TTMix一致性损失。最终的损失函数如下所示：
<span class="math display">\[
\begin{equation}
{\cal{L}}_{all}= {\cal{L}}_{s} + {\gamma}_1  {\cal{L}}_{un} +
{\gamma}_2  {\cal{L}}_{mix},
\end{equation}
\]</span> 其中<span class="math inline">\({\gamma}_1\)</span>和<span
class="math inline">\({\gamma}_2\)</span>是平衡损失项的超参数。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/06/26/Video-Test-Time-Adaptation-for-Action-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/26/Video-Test-Time-Adaptation-for-Action-Recognition/" class="post-title-link" itemprop="url">Video Test-Time Adaptation for Action Recognition</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-26 10:56:29" itemprop="dateCreated datePublished" datetime="2023-06-26T10:56:29+08:00">2023-06-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-28 10:13:19" itemprop="dateModified" datetime="2023-06-28T10:13:19+08:00">2023-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Video Test-Time Adaptation for Action Recognition</p>
<p>paper：http://arxiv.org/abs/2211.15393</p>
<p>code： https://github.com/wlin-at/ViTTA</p>
<h1 id="摘要">摘要</h1>
<p>尽管在同一分布的测试数据上评估时，动作识别系统可以达到最佳性能，但它们对于测试数据中的意外分布变化很容易受到攻击。然而，迄今为止尚未展示视频动作识别模型的测试时间自适应能力。我们提出了一种针对时空模型的方法，可以在单个视频样本的每一步上进行自适应。该方法通过一种特征分布对齐技术，将在线估计的测试集统计数据与训练统计数据进行对齐。我们进一步通过对同一测试视频样本进行时间增强视图的预测一致性来强化。在三个基准动作识别数据集上的评估结果表明，我们提出的技术不依赖于具体的架构，能够显著提高最先进的卷积架构TANet和Video
Swin
Transformer的性能。我们的方法在单个分布变化的评估和随机分布变化的挑战性情况下都表现出了显著的性能提升。</p>
<h1 id="背景信息">背景信息:</h1>
<p>论文背景:
行为识别系统在对分布内的测试数据进行评估时可以达到最佳性能，但对于测试数据中的未预期分布变化却很脆弱。例如，用于识别机动或行人交通事件的摄像头可能会记录罕见的天气状况，如冰雹;体育运动识别系统可能会受到体育场馆观众产生的干扰的影响，如照明弹的烟雾。然而，迄今为止，尚未展示出针对常见分布变化的视频行为识别模型的测试时间自适应方法。</p>
<p>过去方案:
图像分类中的分布变化可以通过测试时间自适应（TTA）来缓解，使用未标记的测试数据来调整模型以适应数据分布的变化。但这些方法不适用于行为识别。大多数行为识别应用需要在线运行内存和计算资源消耗大的时间模型，并且需要在硬件限制下实现最小延迟。此外，视频比图像更容易受到分布变化的影响。现有的TTA算法在处理视频数据时效果不佳。</p>
<p>论文的Motivation:
鉴于现有方法的局限性，本文旨在提出一种在线测试时间自适应的行为识别方法，能够适应测试数据中的分布变化，并且适用于不同的行为识别模型。通过特征对齐技术和预测一致性约束，本文的方法能够在单个视频样本上进行自适应，显著提高行为识别性能。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230626114107.png"
alt="ViTTA" />
<figcaption aria-hidden="true">ViTTA</figcaption>
</figure>
<h1 id="方法">方法</h1>
<p>优化目的：</p>
<p>给定一个在视频序列训练集<span
class="math inline">\(S\)</span>训练好的多层神经网络<span
class="math inline">\(\phi\)</span>以及网络的最优参数<span
class="math inline">\(\hat{\theta}\)</span>。在测试的时候，该网络被用于未标签的测试集<span
class="math inline">\(T\)</span>，而<span
class="math inline">\(T\)</span>可能与<span
class="math inline">\(S\)</span>中的数据分布不同。该方法的目的是为了能使得网络<span
class="math inline">\(\phi\)</span>能够适应这种分布变化，以最大限度地提高在测试视频数据集上的性能。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230626211328.png"
alt="Pipeline of ViTTA" />
<figcaption aria-hidden="true">Pipeline of ViTTA</figcaption>
</figure>
<h2 id="特征分布对齐">特征分布对齐</h2>
<p>论文通过对齐为训练和测试视频计算的特征映射的分布来执行自适应。根据最近对TTA的研究，采用均衡特征映射的均值和方差的方法做到对齐分布。用<span
class="math inline">\(\phi_{l}(\mathbf{x}; \theta)\)</span>表示网络<span
class="math inline">\(\phi\)</span>的第<span
class="math inline">\(l\)</span>层对视频<span
class="math inline">\(\mathbf{x}\)</span>计算得到的特征图，其中<span
class="math inline">\(\mathbf{\theta}\)</span>是用于计算的参数向量。该特征图是一个尺寸为<span
class="math inline">\(({c_l},t_l,h_l,w_l)\)</span>的张量，其中<span
class="math inline">\(c_l\)</span>表示第<span
class="math inline">\(l\)</span>层的通道数，<span
class="math inline">\(t_l\)</span>、<span
class="math inline">\(h_l\)</span>和<span
class="math inline">\(w_l\)</span>分别表示其时间和空间维度。用<span
class="math inline">\(V={[1,t_l]\times[1,h_l]\times[1,w_l]}\)</span>表示特征图的时空范围，并用<span
class="math inline">\(\phi_{l}(\mathbf{x};\mathbf{\theta})[v]\)</span>表示特征图中体素<span
class="math inline">\(v \in V\)</span>上的一个<span
class="math inline">\(c_l\)</span>元素特征向量。 数据集<span
class="math inline">\(D\)</span>中第<span
class="math inline">\(l\)</span>层特征的均值向量可以计算为样本期望值：
<span class="math display">\[
\begin{equation}
    \mu_{l} (D;\mathbf{\theta}) =
        \mathbb{E}_{\substack{\mathbf{x} \in D\\ v \in V}}
            \big[ \phi_{l} (\mathbf{x};\mathbf{\theta})[v] \big] ,
\end{equation}
\]</span> 第<span
class="math inline">\(l\)</span>层特征的方差向量可以计算为： <span
class="math display">\[
\begin{equation}
    {\sigma^2}_{l} (D;\mathbf{\theta}) =
        \mathbb{E}_{\substack{\mathbf{x} \in D\\ v \in V}}
            \left[ \big( \phi_{l} (\mathbf{x};\mathbf{\theta})[v] -
\mu_{l} (D;\mathbf{\theta}) \big)^2 \right]
\end{equation}
\]</span> 为了简化方程的表达，将训练统计量的符号表示缩短为<span
class="math inline">\({\hat{\mu}}_l=\mu_{l}(\mathit{S};\hat{\mathbf{\theta}})\)</span>和<span
class="math inline">\(\hat{\sigma}^2_l={\sigma^2}_l(\mathit{S};\hat{\mathbf{\theta}})\)</span>。在论文的实验中，在训练数据上预先计算样本期望和方差。当这些数据不再可用时，它们可以被另一个未标记数据集的统计数据所取代，该数据集已知是由类似分布生成的。对于具有批归一化层的网络，可以使用这些层中累积的运行均值和方差来代替计算训练集的统计量，虽然会有轻微的性能损失。</p>
<p>总体方法是通过迭代更新参数向量<span
class="math inline">\(\mathbf{\theta}\)</span>，以使选定层的测试统计与计算的训练数据统计相一致。这可以形式化为最小化对齐目标
<span class="math display">\[
\begin{equation}
\mathcal{L}_{\text {align }}(\theta)=\sum_{l \in L}\left|\mu_{l}(T ;
\theta)-\hat{\mu}_{l}\right|+\left|\sigma_{l}^{2}(T ;
\theta)-\hat{\sigma}_{l}^{2}\right|
\end{equation}
\]</span> 关于参数向量<span
class="math inline">\(\mathbf{\theta}\)</span>的最小化问题，其中<span
class="math inline">\(L\)</span>是要对齐的层的集合，<span
class="math inline">\(|\cdot|\)</span>表示向量的<span
class="math inline">\(l_1\)</span>范数，<span
class="math inline">\(\mathit{T}\)</span>表示测试集。论文的方法是通过训练网络来对齐分布，与基于特征对齐的TTA技术在思路上有所不同，后者仅调整归一化层中训练期间累积的运行统计量，因此在测试时并没有真正学习。更新整个参数向量的事实使该方法与仅更新仿射变换层参数的现有算法有所区别，并且在自适应过程中更具灵活性。即使方法调整了完整的参数向量，在持续自适应实验中发现，论文的方法可以快速适应周期性的分布变化。当测试数据流中的分布变化被移除时，网络可以迅速恢复到原始性能。论文还发现，对于TANet和Video
Swin
Transformer这两种架构，通过对齐四个块中的最后两个块的特征分布可以实现最佳性能。因此，论文将<span
class="math inline">\(L\)</span>设置为包含这两个块中的层。</p>
<h2 id="在线自适应">在线自适应</h2>
<p>在公式(3)中优化目标需要迭代估计测试集的统计量。在在线视频识别系统中，通常需要以最小的延迟处理数据流，因此这是不可行的。因此，需要将特征对齐方法适应到在线场景中。假设测试数据以视频序列的形式逐步展示给自适应算法，表示为<span
class="math inline">\(\mathbf{x}\)</span>，其中<span
class="math inline">\(i\)</span>是测试视频的索引。对序列中的每个元素执行一步自适应。单个测试样本上计算的特征统计量不能代表整个测试集上的特征分布，因此不能仅仅依靠它们来对齐分布。因此，通过对连续测试视频上计算的统计量进行指数移动平均来近似测试集的统计量，并将其用于对齐。定义第<span
class="math inline">\(i\)</span>次迭代中的均值估计为： <span
class="math display">\[
\begin{equation}
    {\mu_{l}}^{(i)}(\mathbf{\theta})=\alpha\cdot
\mu_{l}(\mathbf{x}_i;\mathbf{\theta}) + (1-\alpha)\cdot
{\mu_{l}}^{(i-1)}(\mathbf{\theta})
\end{equation}
\]</span> 其中，<span
class="math inline">\(1-\alpha\)</span>是动量项，通常设置为常见的选择<span
class="math inline">\(0.9\)</span>（<span
class="math inline">\(\alpha=0.1\)</span>）。 类似地，定义第<span
class="math inline">\(i\)</span>次迭代中的方差估计为： <span
class="math display">\[
\begin{equation}
    {\sigma^2_{l}}^{(i)}(\mathbf{\theta})=\alpha\cdot
\sigma^2_{l}(\mathbf{x}_i;\mathbf{\theta}) + (1-\alpha)\cdot
{\sigma^2_{l}}^{(i-1)}(\mathbf{\theta}).
\end{equation}
\]</span> 为了适应在线自适应，第<span
class="math inline">\(i\)</span>次对齐迭代中的目标函数被近似为： <span
class="math display">\[
\begin{equation}
\mathcal{L}^{(i)}_\mathrm{align}(\mathbf{\theta})=\sum_{l \in ls} \lvert
{\mu_{l}}^{(i)}(\mathbf{\theta}) - \hat{\mu}_l \rvert
                                                                 +
\lvert  {\sigma^2_{l}}^{(i)}(\mathbf{\theta}) -  \hat{\sigma}^2_l \rvert
\end{equation}
\]</span>
这种方法同时减小了估计量的方差，并让网络持续适应测试数据分布的变化。</p>
<h2 id="时序增强">时序增强</h2>
<p>为了进一步提高该方法的有效性，利用数据的时间性质创建了相同视频的<span
class="math inline">\(M\)</span>个重新采样视图。论文用<span
class="math inline">\(\mathbf{x}^{(m)}_i\)</span>表示输入视频的时间增强视图，其中<span
class="math inline">\(1\le m \le M\)</span>。 论文计算视频<span
class="math inline">\(\mathbf{x}_i\)</span>在这<span
class="math inline">\(M\)</span>个视图上的均值和方差向量，以提高单个视频上的统计量的准确性：
<span class="math display">\[
\begin{equation}
    \mu_{l} (\mathbf{x}_i;\mathbf{\theta}) =
        \mathbb{E}_{\substack{m \in M\\ v \in V}}
            \big[ \phi_{l} (\mathbf{x}^{(m)}_i;\mathbf{\theta})[v] \big]
,
\end{equation}
\]</span> <span class="math display">\[
\begin{equation}
    \sigma^2_{l} (\mathbf{x}_i;\mathbf{\theta}) =
        \mathbb{E}_{\substack{m \in M\\ v \in V}}
            \left[ \big( \phi_{l}
(\mathbf{x}^{(m)}_i;\mathbf{\theta})[v] - \mu_{l}
(\mathbf{x}_i;\mathbf{\theta}) \big)^2 \right] .
\end{equation}
\]</span> 在第<span class="math inline">\(i\)</span>次迭代中，<span
class="math inline">\(\mu_{l}
(\mathbf{x}_i;\mathbf{\theta})\)</span>和<span
class="math inline">\(\sigma^2_{l}
(\mathbf{x}_i;\mathbf{\theta})\)</span>用于计算迭代<span
class="math inline">\(i\)</span>中的均值和方差估计值。</p>
<p>此外，论文要求<span
class="math inline">\(M\)</span>个视图之间的相应预测具有一致性。
论文通过对网络对输入视图进行预测的类别概率进行平均来建立伪标签，即<span
class="math inline">\(y(\mathbf{x})=\frac{1}{M} \sum_{m=1}^{M}
\phi(\mathbf{x}^{(m)}_i;\mathbf{\theta})\)</span>，并定义第<span
class="math inline">\(i\)</span>次迭代中的一致性目标为 <span
class="math display">\[
\begin{equation}
\mathcal{L}^{(i)}_\mathrm{cons} (\mathbf{\theta})=  \sum_{m=1}^{M}
\lvert \phi (\mathbf{x}^{(m)}_i;\mathbf{\theta}) - y(\mathbf{x}) \rvert.
\end{equation}
\]</span> 在第<span
class="math inline">\(i\)</span>次对齐迭代中，论文通过以下梯度更新网络参数：
<span class="math display">\[
\begin{equation}
\min_{\mathbf{\theta}} \mathcal{L}^{(i)}_\mathrm{align}(\mathbf{\theta})
+ \lambda\cdot \mathcal{L}^{(i)}_\mathrm{cons} (\mathbf{\theta}),
\end{equation}
\]</span> 其中<span
class="math inline">\(\lambda\)</span>是系数，论文将其设置为0.1。在消融研究中，论文展示了将<span
class="math inline">\(M=2\)</span>设定为足以显著提升性能的方法，以及均匀等距重新采样输入视频可以获得最佳结果。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/01/Efficient-Video-Understanding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/04/01/Efficient-Video-Understanding/" class="post-title-link" itemprop="url">Efficient Video Understanding</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-04-01 14:25:58" itemprop="dateCreated datePublished" datetime="2023-04-01T14:25:58+08:00">2023-04-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-18 12:52:42" itemprop="dateModified" datetime="2023-06-18T12:52:42+08:00">2023-06-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1
id="prompting-visual-language-models-for-efficient-video-understanding">Prompting
Visual-Language Models for Efficient Video Understanding</h1>
<p>ECCV2022</p>
<p>paper: https://arxiv.org/abs/2112.04478</p>
<p>code: https://github.com/ju-chen/Efficient-Prompt</p>
<h2 id="基于图像的视觉语言模型">基于图像的视觉语言模型</h2>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230406151516.png" /></p>
<h3 id="pre-training">Pre-training</h3>
<p>给定一个batch的N对(图像，文本)，使用两个单独的编码器计算图像和文本的feature
embeddings，并计算所有N个可能的(图像，文本)对之间的密集余弦相似度矩阵。训练目标是联合优化图像和文本编码器，通过最大化N对正确的(图像，文本)关联之间的相似性，同时通过密集矩阵上的对称交叉熵最小化N
×(N−1)错误对的相似性，即噪声对比学习。这两个编码器都包含一个tokeniser，用于将图像补丁或语言单词转换为向量，也就是会有visual
tokens和textual tokens。</p>
<h3 id="inference">Inference</h3>
<p>训练完成之后，I-VL模型便可以部署在开放词汇表上的图像分类任务，通过使用从文本编码器<span
class="math inline">\(\Phi_{\text{text}}\)</span>生成相应的视觉分类器完成分类任务。举例来说，如果为了分辨一张图片是猫还是狗，那分类器（<span
class="math inline">\(c_{cat}\)</span>和<span
class="math inline">\(c_{dog}\)</span>）可以生成为: <span
class="math display">\[
\begin{aligned}c_{\mathrm{cat}}&amp;=\Phi_{\mathrm{text}}(\mathrm{TOKENISER}(\text{“this
is a photo of}[\underline{cat}]\text{&#39;&#39;}))\\
c_{\mathrm{dog}}&amp;=\Phi_{\operatorname{text}}(\mathrm{TOKENISER}(\text{“this
is a photo of}[\underline{dog}]\text{&#39;&#39;}))\end{aligned}
\]</span> 而“this is a photo of
[·]”是手工制作的提示模板，可以被有效地用于图像分类。</p>
<h3 id="discussion">Discussion</h3>
<blockquote>
<p>尽管在零样本图像分类方面取得了巨大的成功，I-VL模型也显示出对手工制作的提示模板很敏感，显然对其对新的下游任务的有效适应造成了限制，在这些任务中，专家知识可能很难浓缩或不可用。因此，考虑将这种提示设计过程自动化，探索有效的方法，以使预训练的基于图像的视觉-语言模型适应新的下游任务，而训练数量最少。</p>
</blockquote>
<h2 id="视频理解prompting-clip">视频理解Prompting CLIP</h2>
<p>作者认为I-VL模型上的prompt
learning会在视频领域大放异彩主要有两个原因：（实际上感觉和用图像理解的模型去预训练视频理解模型类似）
1.
视频任务需要大量的资源。具体来说，视频-文本对更难收集且训练计算成本更高，因此通过训练大规模的基于图像的视觉文本模型（I-VL），并且prompt进行高效的视频理解更好（类似于图像迁移到视频？）。
2.
视频由帧序列组成，在强大的基于图像的模型上建立时间依赖性是一个自然且经济的选择。</p>
<h3 id="问题设想">问题设想</h3>
<p>数据集表示为<span class="math inline">\(\mathcal{D} = \{
\mathcal{D}_{\text{train}}, \mathcal{D}_{\text{val}}
\}\)</span>，e.g.<span class="math inline">\(\mathcal{D}_{\text{train}}
= \{(\mathcal{V}_1, y_1), \dots, (\mathcal{V}_n, y_n)
\}\)</span>，其中视频为<span class="math inline">\(\mathcal{V}_i \in
\mathbb{R}^{T \times H \times W \times 3}\)</span>，而标签<span
class="math inline">\(y_i\)</span>根据下游任务的不同而有不同的情况：识别任务为<span
class="math inline">\(c_{train}\)</span>中的动作标签、定位任务为密集的动作类别标签的T时间戳、检索任务为细粒度文本描述。</p>
<p>在closed-set的情况下，训练和验证的动作类别是相同的；零样本情况下，训练和验证的动作类别是不相关的。</p>
<h3 id="基于提示学习的模型自适应">基于提示学习的模型自适应</h3>
<p>主要目的是引导预训练只需最少的训练即可执行各种视频任务。具体而言，通过将连续随机向量序列(“提示向量”)与文本标记预先/追加，实现高效的模型适应。在训练时，CLIP的图像和文本编码器都保持冻结，梯度将通过文本编码器，只更新提示向量。最终，这些可学习的向量将构建文本编码器可以理解的“虚拟”提示模板，并生成所需的分类器或查询嵌入，详细内容如下</p>
<ol type="1">
<li>动作识别：为了生成动作分类器，我们通过将标记化的类别名称输入预训练的文本编码器<span
class="math inline">\(\mathrm{\Phi}_{\text{text}}\)</span>来构造“虚拟”提示模板，如下式，其中<span
class="math inline">\(a_i \in \mathbb{R}
^{D}\)</span>表示第i个提示向量，由几个可学习参数组成，<span
class="math inline">\(D\)</span>是向量维度。提示向量<span
class="math inline">\(\{a_i\}\)</span>会与所有的动作类别共享，也就是只是对于任务是专有的。
<span class="math display">\[
\begin{align*}
&amp;c_{\text{archery}} = \mathrm{\Phi}_{\text{text}}(a_{1}, \dots,
\mathrm{TOKENISER}(\text{&#39;&#39;}\text{\underline{archery}&#39;&#39;}),
\dots, a_{k}) \\
&amp;c_{\text{bowling}} = \mathrm{\Phi}_{\text{text}}(a_{1},
\dots,  \mathrm{TOKENISER}(\text{&#39;&#39;}\text{\underline{bowling}&#39;&#39;}),
\dots, a_{k})
\end{align*}
\]</span></li>
<li>动作定位：采用two-stage范式，首先检测潜在的类别未知动作建议(详见第4.1节)，然后对这些检测到的建议执行动作分类。</li>
<li>视频文字提取：类似地将整个句子标记化，并将标记化的结果与可学习的提示向量馈送到文本编码器，以生成每个句子的查询嵌入。</li>
<li>总结：一般来说，模型适应的学习提示有以下好处:
1）所有任务都可以使用同一个共享的backbone，并且能达到有竞争力的性能；2）适应新任务只需要优化少数提示向量，便于实现few-shot问题；3）能够更好地利用丰富的训练数据，并进一步泛化到封闭集类别之外。</li>
</ol>
<h3 id="时序建模">时序建模</h3>
<p>作者通过使用一个简单而轻量级的时序建模模块来弥补图像到视频之间的差距。具体来说，通过在冻结图像编码器的帧级特征上附加一个Transformer编码器将CLIP的图像encoder升级为视频encoder：
<span class="math display">\[
\begin{align*}
v_i = \mathrm{\Phi}_{\text{video}}(\mathcal{V}_i) =
\mathrm{\Phi}_(\{
\mathrm{\Phi}_{\text{image}}(I_{i1}), \dots,
\mathrm{\Phi}_{\text{image}}(I_{iT})\})
\end{align*}
\]</span> 为了表示时间顺序，图像特征上添加了可学习的时间位置编码。<span
class="math inline">\(v_i \in \mathbb{R} ^{T \times D}\)</span>是<span
class="math inline">\(T\)</span>帧的密集特征嵌入。</p>
<h3 id="训练loss">训练loss</h3>
<p>给定一批(视频，文本)训练对，视觉流最终得到密集的帧级特征嵌入~(<span
class="math inline">\(v_i\)</span>);而对于文本流，根据考虑的下游任务，它最终会得到一组操作分类器(<span
class="math inline">\(c_i \in
\mathcal{C}_{\text{action}}\)</span>)或文本查询嵌入(<span
class="math inline">\(c_i \in
\mathcal{C}_{\text{query}}\)</span>)。对于动作识别和文本-视频检索，通过取密集特征的均值池来进一步计算视频片段级特征:
<span class="math display">\[
\begin{align}
    \overline{v}_i = \mathrm{\Phi}_{POOL}(v_i) \in \mathbb{R}^{1 \times
D}
\end{align}
\]</span>
对于动作定位，对每个检测到的动作建议中的密集特征进行平均池化，以获得提案级特征。为了简单起见，还将这个提议级别的特性表示为<span
class="math inline">\(\overline{v}_i\)</span>。
训练过程中，共同优化文本提示向量和时间Transformer，使得视频片段(提案)特征及其配对分类器或文本查询嵌入在其他特征中发出最高的相似性分数。这是通过简单的NCE损失实现的
<span class="math display">\[
\begin{align}
\mathcal{L} = &amp;- \sum_i \big( \log \frac{\exp(\overline{v}_i \cdot
c_{i} / \tau)}{\sum\limits_{j} \exp(\overline{v}_i \cdot c_j / \tau)}
\big)
\end{align}
\]</span></p>
<h1 id="dual-path-adaptation-from-image-to-video-transformers">Dual-path
Adaptation from Image to Video Transformers</h1>
<p>CVPR 2023</p>
<p>paper: https://arxiv.org/abs/2303.09857</p>
<p>code: https://github.com/park-jungin/dualpath</p>
<h2 id="baseline">Baseline</h2>
<p>Parameter-efficient transfer learning
(PETL),参数高效迁移学习，在自然语言处理(NLP)中首次被使用，用于解决全/部分微调的内存和参数效率低下的问题。主要目标是通过仅使用少量可训练参数进行微调，在下游任务上获得相当或超过的性能。</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421132548.png" /></p>
<ol type="1">
<li>Visual prompt tuning (VPT) 在Transformer块的Input Tokens前添加<span
class="math inline">\(K\)</span>个可训练的prompt
token，同时冻结预训练的参数。Input Tokens为<br />
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421133836.png" /></li>
<li>AdaptFormer学习一个可训练的bottleneck模块。这个模块会与Transformer块中的MLP层平行，即输入为中间特征<span
class="math inline">\(z\)</span>，AdaptFormer block的输出可表示为 <img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421134535.png" /></li>
<li>Pro-tuning使用连续的2D卷积层从每个Transformer块的输出中预测特定于任务的视觉提示v。每个块的输出会被reshaped为
<span class="math inline">\(P \times P \times C\)</span>以应用于2D卷积
。 <img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421140433.png" /></li>
<li>ST-adapter采用的adapter结构为在向下投影层和激活函数之间插入深度方向的3D卷积层。相较于AdaptFormer，ST-adapter会接受所有帧的token使得模型可以捕捉到视频中的时间特征。输出可表示为：
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421141151.png" /></li>
</ol>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421142701.png" /></p>
<h2 id="spatial-adaptation">Spatial adaptation</h2>
<p>即使在视频中，已经在大量的数据集上训练过的图像模型也可能对空间contexts进行编码，具有突出的空间建模能力。为了更适合视频理解，作者采用了两个平行于transformer块的MHA和MLP的adapter。平行adapter可以允许模型在保持用于识别对象的原始contexts的同时，能在视频的空间信息中学得可以用于动作识别的contexts。</p>
<p>空间token的集<span
class="math inline">\(\mathbf{X}^{\text{SP}}_t\)</span>包括可学习的位置编码<span
class="math inline">\(\mathbf{p}^{SP}\)</span>和空间class token <span
class="math inline">\(\mathbf{x}_{t}^{\mathrm{SP}}\{[\mathrm{CLS}]\}\)</span>。第<span
class="math inline">\(l\)</span>个transformer块的Spatial
adaptation可由下列公式表示：</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421150350.png" /></p>
<p>接着对来自最后一个transformer块的空间分类token进行平均以获得全局空间表示<span
class="math inline">\(y^{sp}\)</span></p>
<h2 id="temporal-adaptation">Temporal adaptation</h2>
<p>虽然Spatial
adaptation能够使得图像模型适应视频数据中的空间contexts，但图像模型仍然无法对时间信息进行建模。使视频transformer能够建模时间context的关键是能够学习到视频中跨帧的局部patch之间的关系。因此，作者提出了一种新的类网格帧集转换技术，将多个帧聚合成一个统一的类网格帧集，能模拟空间和时间建模且大大减少计算量。在每个transformer块中，分别为MHA和MLP采用了两个额外的串行adapter。</p>
<p>在视频中采样<span class="math inline">\(T\)</span>帧后，使用<span
class="math inline">\(w\)</span>和<span
class="math inline">\(h\)</span>的因子进行缩放，缩放后的帧大小为<span
class="math inline">\([W/w \times H/h \times
3]\)</span>。接着根据时间顺序堆叠<span class="math inline">\(w\times
h\)</span>个缩放帧，并重构堆叠帧，以与原始帧大小相同的网格形式构造一组帧，从而得到的类网格帧集的总数为<span
class="math inline">\(T_G = T/wh\)</span>。按同样的方式获得第<span
class="math inline">\(g\)</span>个帧集的时间token<span
class="math inline">\(\mathbf{X}_g^{\text{TP}}\)</span>，并与可学习的时间分类token<span
class="math inline">\(\mathbf{x}_{g}^{\mathrm{SP}}\{[\mathrm{CLS}]\}\)</span>相结合。与Spatial
adaptation不同，使用了固定3D位置编码<span
class="math inline">\(\mathbf{p}^{TP}\)</span>，考虑到patch的绝对时间顺序和空间位置。作者依次将适配器附加到每个变压器块中的MHA和MLP层的顶部，第<span
class="math inline">\(l\)</span>个transformer块的temporal
adaptation可由下列公式表示：</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230421154936.png" /></p>
<p>接着对来自最后一个transformer块的时间分类token进行平均以获得全局空间表示<span
class="math inline">\(y^{sp}\)</span>。对于最后的预测，将全局空间和时间表示连接起来，并将它们输入两个FC层之间具有GeLU激活的分类器。</p>
<h1
id="aim-adapting-image-models-for-efficient-video-action-recognition">AIM:
Adapting Image Models for Efficient Video Action Recognition</h1>
<p>ICLR 2023</p>
<p>paper: https://arxiv.org/abs/2302.03024</p>
<p>code: https://github.com/taoyang1122/adapt-image-models</p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230407140936.png" /></p>
<p><img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230407151431.png" /></p>
<h2 id="空间-adaptation">空间 Adaptation</h2>
<p>Adapter是在NLP上被广泛使用的一种高效微调技术。从图中（b）可以看出，Adapter是一个bottleneck架构，由两个全连接(FC)层和中间的激活层组成。为了使预训练的空间特征适应目标视频数据，空间Adaptation通过在自注意层后添加Adapter实现。在训练过程中，只更新Adapter的参数而冻结其他部分。</p>
<h2 id="时间-adaptation">时间 Adaptation</h2>
<p>作者提出了一个新的方法：复用图像模型中预先训练好的自注意层进行时间建模。
具体地说，他们将原始的自注意层表示为S-MSA用于空间建模，将复用的自注意层表示为T-MSA用于时间建模。可以在图中（c）看到，他们将T-MSA放在S-MSA前面，然后通过reshape的方法调整输入的embedding，使得同样的MSA可以用于不同的维度。与空间Adaptation一样，在MSA后加上了一个时间Adapter，但不同的是这个Adapter没有skip
connection，原因是希望将适应模型初始化为接近原始模型。</p>
<h2 id="联合-adaptation">联合 Adaptation</h2>
<p>联合Adaptation就是在最后的MLP处加上了一个与其并行的Adapter，而这个Adapter的结构与时间Adaptation的一样，最后adapted
block可被写为 <span class="math display">\[
\begin{aligned}
    z^T_{l} &amp;= z_{l-1} +
\operatorname{Adapter}(\operatorname{T-MSA}(\operatorname{LN}(z_{l-1})))\\
    z^S_{l} &amp;= z^T_{l} +
\operatorname{Adapter}(\operatorname{S-MSA}(\operatorname{LN}(z^T_{l})))\\
    z_{l} &amp;= z^S_{l} +
\operatorname{MLP}(\operatorname{LN}(z^S_{l})) + s \cdot
\operatorname{Adapter}(\operatorname{LN}(z^S_{l}))
    \end{aligned}
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/28/Video-Swin-Transformers/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/28/Video-Swin-Transformers/" class="post-title-link" itemprop="url">Video-Swin-Transformers</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-28 19:49:09" itemprop="dateCreated datePublished" datetime="2023-03-28T19:49:09+08:00">2023-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-27 10:20:20" itemprop="dateModified" datetime="2023-04-27T10:20:20+08:00">2023-04-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>paper: https://arxiv.org/abs/2106.13230</p>
<p>code: https://github.com/SwinTransformer/Video-Swin-Transformer</p>
<h1 id="摘要">摘要</h1>
<p>视觉社区正在见证从CNN到Transformer的建模转变，其中纯Transformer架构在主要视频识别基准上获得了最高准确性。这些视频模型都建立在Transformer层上，它们在空间和时间维度上全局连接补丁。在本文中，我们反而提倡在视频变换器中引入局部性的归纳偏差，与先前计算全局自我关注甚至具有空间-时间因子分解的方法相比，这导致了更好的速度-准确性折衷。所提出的视频架构的局部性是通过适应为图像域设计的Swin
Transformer实现的，同时继续利用预训练图像模型的能力。我们的方法在广泛的视频识别基准上实现了最先进的准确性，包括动作识别（Kinetics-400上84.9的top-1准确性和Kinetics-600上85.9的top-1准确性，预训练数据约少20倍，模型大小约小3倍）和时间建模（Something-Something
v2上69.6的top-1准确性）。</p>
<h1 id="方法">方法</h1>
<h2 id="总体架构">总体架构</h2>
<p>Video Swin
Transformer的总体架构如图所示，它展示了其tiny版本（Swin-T）。输入视频被定义为大小为<span
class="math inline">\(T×H×W×3\)</span>，由<span
class="math inline">\(T\)</span>帧组成，每帧包含<span
class="math inline">\(H×W×3\)</span>个像素。在Video Swin
Transformer中，将每个大小为<span class="math inline">\(2\times 4 \times
4 \times 3\)</span>的3D块视为一个token。因此，3D patch partitioning
layer获得<span
class="math inline">\(\frac{T}{2}×\frac{H}{4}×\frac{W}{4}\)</span>个3D标记，每个块/token由96维特征组成。然后应用线性embedding层将每个标记的特征投影到一个任意维度<span
class="math inline">\(C\)</span>。</p>
<p>根据先前的工作，时间维度不进行下采样，这样就能直接参考Swin
Transformer的架构，包含4个阶段，在每个阶段的Patch Merging层执行<span
class="math inline">\(2\times\)</span>空间下采样（将每组2 <span
class="math inline">\(\times\)</span>
2空间相邻补丁的特征拼接起来，并应用一个线性层将拼接后的特征投影到其维数的一半）。</p>
<p>该架构的主要组成部分是视频Swin
Transformer块，该块是通过将标准Transformer层中的多头自注意(MSA)模块替换为基于3D移动窗口的多头自注意模块，并保持其他组件不变的方式构建的。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230329134824.png"
alt="20230329134824" />
<figcaption aria-hidden="true">20230329134824</figcaption>
</figure>
<h2 id="基于3d移位窗口的msa模块">基于3D移位窗口的MSA模块</h2>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230329145740.png"
alt="20230329145740" />
<figcaption aria-hidden="true">20230329145740</figcaption>
</figure>
<p><strong>非重叠3D窗口的多头注意力机制</strong>：直接扩展2D的方法去处理视频输入。给定一个由<span
class="math inline">\(T’\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(H’\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(W’\)</span> 3D token组成的视频和一个<span
class="math inline">\(P\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(M\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(M\)</span>的3D窗口大小，窗口被安排以非重叠的方式均匀划分视频输入。也就是说，输入token被划分为<span
class="math inline">\(\lceil\frac{T’}{P}\rceil\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(\lceil\frac{H’}{M}\rceil\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(\lceil\frac{W’}{M}\rceil\)</span>个不重叠的3D窗口。例如，如图所示，对于输入大小为<span
class="math inline">\(8\times8\times8\)</span>个token和窗口大小为<span
class="math inline">\(4\times 4 \times 4\)</span>的情况，第<span
class="math inline">\(l\)</span>层的窗口数量将为<span
class="math inline">\(2\times 2 \times
2\)</span>=8,并且在每个3D窗口内执行多头自注意力。</p>
<p><strong>3D移位窗口</strong>：同样，类似于Swin
Transformer，不同窗口之间缺乏连接，因此将Swin
Transformer的2D移位窗口机制扩展到3D，使得可以实现跨窗口的信息连接。鉴于输入3D令牌的数量为<span
class="math inline">\(T’\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(H’\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(W’\)</span>，每个3D窗口的大小为<span
class="math inline">\(P\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(M\)</span><span
class="math inline">\(\times\)</span><span
class="math inline">\(M\)</span>，对于两个连续的层，第一层中的自注意力模块使用常规窗口划分策略，第二层则是把窗口划分配置沿时间轴、高度轴和宽度轴分别移动(<span
class="math inline">\(\frac{P}{2}\)</span>,<span
class="math inline">\(\frac{M}{2}\)</span>,<span
class="math inline">\(\frac{M}{2}\)</span>)个token，也就是相比于Swin
Transformer多了时间轴的移动。</p>
<p>通过移动窗口划分方法，两个连续的视频Swin变换器块被计算为</p>
<p><span class="math display">\[
    { {\hat{\bf{z} } }^{l} } = \text{3DW-MSA}\left( {\text{LN}\left( { {
{\bf{z} }^{l - 1} } } \right)} \right) + {\bf{z} }^{l - 1},\\\\
    { {\bf{z} }^l} = \text{FFN}\left( {\text{LN}\left( { { {\hat{\bf{z}
} }^{l} } } \right) } \right) + { {\hat{\bf{z} } }^{l} },\\\\
    { {\hat{\bf{z} } }^{l+1} } = \text{3DSW-MSA}\left( {\text{LN}\left(
{ { {\bf{z} }^{l} } } \right)} \right) + {\bf{z} }^{l}, \\\\
    { {\bf{z} }^{l+1} } = \text{FFN}\left( {\text{LN}\left( { {
{\hat{\bf{z} } }^{l+1} } } \right)} \right) + { {\hat{\bf{z} } }^{l+1}
},
\]</span></p>
<p>其中，<span class="math inline">\({\hat{\bf{z}}}^l\)</span>和<span
class="math inline">\({\bf{z}}^l\)</span>分别表示第<span
class="math inline">\(l\)</span>块的3D(S)W-MSA模块和FFN模块的输出特征；<span
class="math inline">\(\text{3DW-MSA}\)</span>和<span
class="math inline">\(\text{3DSW-MSA}\)</span>分别表示使用常规和移动窗口划分配置的基于3D窗口的多头自注意力。</p>
<p><strong>3D相对位置偏置</strong>：与大多数工作类似，作者也在每个注意力头加了3D相对位置偏置<span
class="math inline">\(B \in \mathbb{R}^{P^2 \times M^2 \times
M^2}\)</span>如下</p>
<p><span class="math display">\[
    \text{Attention}(Q, K, V) = \text{SoftMax}(QK^T/\sqrt{d}+B)V,
\]</span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/23/MViT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/23/MViT/" class="post-title-link" itemprop="url">MViT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-23 21:11:34" itemprop="dateCreated datePublished" datetime="2023-03-23T21:11:34+08:00">2023-03-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-28 19:49:53" itemprop="dateModified" datetime="2023-03-28T19:49:53+08:00">2023-03-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>paper: https://arxiv.org/abs/2104.11227</p>
<p>code: https://github.com/facebookresearch/SlowFast</p>
<h1 id="摘要">摘要</h1>
<p>我们提出了用于视频和图像识别的多尺度视觉Transformer（MViT），通过将多尺度特征层次结构的开创性想法与Transformer模型相连接。多尺度Transformer具有多个通道-分辨率尺度阶段。从输入分辨率和小通道维度开始，这些阶段在减小空间分辨率的同时分层扩展通道容量。这创建了一个多尺度特征金字塔，早期层以高空间分辨率操作，以模拟简单的低层次视觉信息，而深层以空间粗糙但复杂的高维特征操作。我们评估了这个基本的架构先验来模拟视觉信号的密集性质，针对多种视频识别任务进行了评估，其中它的表现优于依赖于大规模外部预训练的并且计算和参数成本高5-10倍的同时期视觉Transformer。我们进一步去除了时间维度，并将我们的模型应用于图像分类，其中它比视觉Transformer之前的工作中表现更好。</p>
<h1 id="方法">方法</h1>
<p>通用多尺度变压器架构是建立在stages得核心概念上的。每一个stage由多个具有特定时空分辨率和channel维度的Transformer块组成。主要思想是逐步扩大信道容量，同时汇集网络从输入到输出的分辨率。</p>
<h2 id="多头池化注意力">多头池化注意力</h2>
<p>多头池化注意力（Multi Head Pooling
Attention,MHPA）是一种可以在Transformer块中灵活建模不同分辨率的自注意力算子，使得多尺度Transformer可以在不断变化的时空分辨率下运行。与原始的多头注意力算子相比，MHPA将潜在张量序列进行池化，以减少参与输入的序列长度(分辨率)，如下图。
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230326125003.png"
alt="20230326125003" /> 输入<span class="math inline">\(X \in
\mathbb{R}^{L \times D}\)</span>是一个序列长度为<span
class="math inline">\(L\)</span>的<span
class="math inline">\(D\)</span>维输入张量，与MHA一样得到query、key、value：</p>
<p><span class="math display">\[
\hat{Q} = XW_{Q} \quad \hat{K} = XW_{K} \quad \hat{V} = XW_{V}
\]</span></p>
<p><strong>池化算子</strong>：<span
class="math inline">\(\mathcal{P}(\cdot ;
\mathbf{\Theta})\)</span>沿着每个维度对输入张量执行池化核计算。<span
class="math inline">\(\mathbf{\Theta}\)</span>内的参数为<span
class="math inline">\(\mathbf{\Theta} := (\mathbf{k}, \mathbf{s},
\mathbf{p})\)</span>，分别对应池化核大小、步长、padding。因此注意力机制的计算会变为：</p>
<p><span class="math display">\[
\operatorname{PA}(\cdot) = \operatorname{Softmax}(\mathcal{P}(Q;
\mathbf{\Theta}_Q)\mathcal{P}(K;
\mathbf{\Theta}_K)^T/\sqrt{d})\mathcal{P}(V; \mathbf{\Theta}_V),
\]</span></p>
<p><strong>计算复杂度</strong>：用<span
class="math inline">\(f_Q\)</span>，<span
class="math inline">\(f_K\)</span>和<span
class="math inline">\(f_V\)</span>表示序列长度缩小因子有，</p>
<p><span class="math display">\[ f_j = s^j_T \cdot s^j_H \cdot s^j_W, \
\forall \ j \in \{Q,K,V\}. \]</span></p>
<p>考虑将<span class="math inline">\(\mathcal{P}(;
\Theta)\)</span>的输入张量的维度表示为<span class="math inline">\(D
\times T \times H \times W\)</span>，每个MHPA头部的运行时复杂度为<span
class="math inline">\(O(THW D/h
(D+THW/f_Qf_K))\)</span>，存储复杂度为<span
class="math inline">\(O(THWh(D/h + THW/f_Qf_K))\)</span>。</p>
<h2 id="多尺度transformer网络">多尺度Transformer网络</h2>
<h3 id="vit">ViT</h3>
<p>Vision Transformer（ViT）架构通过将分辨率为<span
class="math inline">\(T\times H \times
W\)</span>的输入视频分成大小为<span class="math inline">\(1\times 16
\times
16\)</span>的不重叠补丁，并对展平的图像补丁应用逐点线性层，将它们投影到Transformer的潜在维度<span
class="math inline">\(D\)</span>中。这相当于使用大小和步长均为<span
class="math inline">\(1\times 16 \times
16\)</span>的卷积，将其显示为模型定义中的patch<span
class="math inline">\(_1\)</span>阶段。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230327181457.png"
alt="20230327181457" />
<figcaption aria-hidden="true">20230327181457</figcaption>
</figure>
<p>接下来，对于长度为<span class="math inline">\(L\)</span>、维度为<span
class="math inline">\(D\)</span>的投影序列的每个元素都加上一个位置嵌入<span
class="math inline">\(\mathbf{E} \in \mathbb{R}^{L \times
D}\)</span>，以编码位置信息并打破排列不变性。一个可学习的类别嵌入被添加到投影的图像补丁中。</p>
<p>将结果长度为<span
class="math inline">\(L+1\)</span>的序列依次输入到一个由<span
class="math inline">\(N\)</span>个transformer块构成的堆叠中进行处理，每个块都执行注意力（<span
class="math inline">\(\operatorname{MHA}\)</span>）、多层感知机（<span
class="math inline">\(\operatorname{MLP}\)</span>）和层归一化（<span
class="math inline">\(\operatorname{LN}\)</span>）操作。假设<span
class="math inline">\(X\)</span>是块的输入，则单个transformer块的输出<span
class="math inline">\(\operatorname{Block}(X)\)</span>的计算方式为：</p>
<p><span class="math display">\[
X_1 = \operatorname{MHA}(\operatorname{LN}(X)) + X \\
\operatorname{Block}(X) = \operatorname{MLP}(\operatorname{LN}(X_1)) +
X_1
\]</span></p>
<p>经过 <span class="math inline">\(N\)</span>
个连续的块处理后，得到的序列进行层归一化，然后提取类嵌入并通过线性层传递以预测所需的输出（例如类别）。默认情况下，MLP
的隐藏维度为 <span class="math inline">\(4D\)</span>。</p>
<h3 id="mvit">MViT</h3>
<p>MViT的关键是在整个网络中同时逐步“增加”通道分辨率（即维度）并“降低”时空分辨率（即序列长度）。MViT架构在早期层中具有细节的时空（和“粗略”的通道）分辨率，这些分辨率在后期层中进行上/下采样以获得粗略的时空（和“精细”的通道）分辨率。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230327193043.png"
alt="20230327193043" />
<figcaption aria-hidden="true">20230327193043</figcaption>
</figure>
<p>一个scale stage被定义为在相同scale上操作的一组<span
class="math inline">\(N\)</span>个transformer块，其在通道和时空维度上具有相同的分辨率<span
class="math inline">\(D\times T\times H\times
W\)</span>。在输入（cube<span
class="math inline">\(_1\)</span>）中，将图像块（或立方体）投影到较小的通道维度（例如，比典型的ViT模型小8倍），但有更长的序列（例如，比典型的ViT模型密集16倍）。</p>
<p>在一个阶段的“过渡”（例如从scale<span
class="math inline">\(_1\)</span>到scale<span
class="math inline">\(_2\)</span>），被处理的序列的通道维度被上采样，而序列的长度则被下采样。这有效地降低了底层视觉数据的时空分辨率，同时允许网络在更复杂的特征中同化处理的信息。</p>
<p><strong>通道扩展</strong>：通过将前一个阶段的最终MLP层的输出增加一个相对于引入的分辨率变化因子来扩展通道维度。具体来说，如果我们将时空分辨率下采样4倍，则将通道维度增加2倍。例如，从scale<span
class="math inline">\(_3\)</span>到scale<span
class="math inline">\(_4\)</span>的分辨率从<span
class="math inline">\(2D\times \frac{T}{s_T}\times \frac{H}{8}\times
\frac{T}{8}\)</span>变为<span class="math inline">\(4D\times
\frac{T}{s_T}\times \frac{H}{16}\times
\frac{T}{16}\)</span>，类似于卷积。</p>
<p><strong>Query
池化</strong>：池化注意力操作不仅可以灵活地调整Key和value的长度，还可以调整query及输出序列的长度。将查询向量<span
class="math inline">\(\mathcal{P}(Q; \mathbf{k}; \mathbf{p};
\mathbf{s})\)</span>与核<span class="math inline">\(\mathbf{s} \equiv
(s^Q_T,s^Q_H,s^Q_W)\)</span>进行池化操作，可以将序列的长度缩小<span
class="math inline">\(s^Q_T \cdot s^Q_H \cdot
s^Q_W\)</span>倍。由于是在每个阶段开始时减小分辨率，然后在整个阶段中保持这种分辨率，因此每个阶段只有第一个池化注意力操作为<span
class="math inline">\(\mathbf{s}^Q &gt; 1\)</span>，而所有其他的<span
class="math inline">\(\mathbf{s}^Q\)</span>均为1</p>
<p><strong>Key-Value 池化</strong>：与查询池化不同，更改<span
class="math inline">\(K\)</span> 和 <span
class="math inline">\(V\)</span>
的序列长度不会改变输出序列长度，因此也不会改变空时分辨率。作者将<span
class="math inline">\(K,V\)</span>和<span
class="math inline">\(Q\)</span>池化的使用解耦，<span
class="math inline">\(Q\)</span>池化用于每个阶段的第一层，而<span
class="math inline">\(K,V\)</span>池化则在所有其他层中使用。由于需要保证键和值张量的序列长度相同才能进行注意力权重计算，所以用于<span
class="math inline">\(K\)</span>和<span
class="math inline">\(V\)</span>张量的池化步长需要相同。在默认设置中，所有池化参数（<span
class="math inline">\(\mathbf{k}; \mathbf{p};
\mathbf{s}\)</span>）限制是相同的，即<span
class="math inline">\(\Theta_K \equiv
\Theta_V\)</span>在每个阶段内，但是根据不同阶段的尺度，自适应地变化<span
class="math inline">\(\mathbf{s}\)</span>。</p>
<p><strong>跳过连接</strong>：由于维度问题，需要对查询池化算子 <span
class="math inline">\(\mathcal{P}(\cdot ; \mathbf{\Theta}_Q)\)</span>
来处理残差连接，需要将池化后的输入 <span
class="math inline">\(X\)</span> 加入到输出中，而不是直接将输入 <span
class="math inline">\(X\)</span>
加入到输出中，从而使分辨率匹配注意力查询 <span
class="math inline">\(Q\)</span>。</p>
<p>为了处理阶段之间通道维度的不匹配，作者使用了一个额外的线性层，对MHPA操作的层归一化输出进行操作。请注意，这与其他（保留分辨率）的跳跃连接不同，它们在未标准化的信号上进行操作。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com//img/20230327212411.png"
alt="20230327212411" />
<figcaption aria-hidden="true">20230327212411</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/17/XVit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/17/XVit/" class="post-title-link" itemprop="url">XVit</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-17 10:44:02" itemprop="dateCreated datePublished" datetime="2023-03-17T10:44:02+08:00">2023-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-04-27 10:27:27" itemprop="dateModified" datetime="2023-04-27T10:27:27+08:00">2023-04-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>paper: https://arxiv.org/abs/2106.05968</p>
<p>code: https://github.com/1adrianb/video-transformers</p>
<h1 id="摘要">摘要</h1>
<p>本文研究的是利用Transformer进行视频识别。最近在这一领域的尝试在识别精度方面已经证明了有希望的结果，但在许多情况下，由于对时间信息的额外建模，它们也被证明会导致显著的计算开销。在这项工作中，我们提出了一个视频Transformer模型，其复杂性与视频序列中的帧数成线性比例，因此与基于图像的Transformer模型相比没有开销。为了实现这一点，我们的模型对视频Transformer中使用的全时空注意力做了两个近似:(a)它将时间注意力限制在局部时间窗口，并利用Transformer的深度来获得视频序列的全时间覆盖。(b)它使用高效的时空混合来联合关注空间和时间位置，而不会在纯空间注意力模型的基础上产生任何额外的成本。我们还展示了如何集成2个非常轻量级的全局时间关注机制，以最小的计算成本提供额外的精度改进。我们证明了我们的模型在最流行的视频识别数据集上产生非常高的识别精度，同时比其他视频转换器模型更有效。代码将被提供。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230317104614654.png"
alt="Different approaches to space-time self-attention for video recognition." />
<figcaption aria-hidden="true">Different approaches to space-time
self-attention for video recognition.</figcaption>
</figure>
<h1 id="主要贡献">主要贡献</h1>
<ol type="1">
<li>提出一种复杂度为<span
class="math inline">\(O(TS^2)\)</span>的视频Transformer模型，与基线模型一样高效。它在效率方面（即准确度/FLOP）比最近/同时提出的工作优越很多；</li>
<li>对视频Transformer的全时空注意力进行了两次近似：
<ol type="1">
<li>将时间注意力限制在局部时间窗口内，并利用Transformer的深度获得视频序列的完整时间覆盖；</li>
<li>使用高效的时空混合，在不增加空间唯一注意力模型额外成本的情况下，共同关注空间和时间位置；</li>
</ol></li>
<li>展示了如何整合两种非常轻量级的全局时间唯一注意力机制，它们以最小的计算成本提供额外的准确性改进。</li>
</ol>
<h1 id="方法">方法</h1>
<h2 id="video-transformer">Video Transformer</h2>
<p>给定视频片段：<span
class="math inline">\(\mathbf{X}\in\mathbb{R}^{T\times H \times W \times
C }\)</span></p>
<p>根据ViT，每帧将被分割为<span class="math inline">\(K\times
K\)</span>个非重叠补丁并通过一个线性embedding层<span
class="math inline">\(\mathbf{E}\in\mathbb{R}^{3K^2 \times
d}\)</span>映射为visual
token。另外需要学习两个位置embedding加载初始化的visual token，一是<span
class="math inline">\(\mathbf{p}_{s}\in\mathbb{R}^{1 \times S\times
d}\)</span>，二是<span
class="math inline">\(\mathbf{p}_{t}\in\mathbb{R}^{T\times 1 \times
d}\)</span>。最后token序列会被L层Transformer处理。第<span
class="math inline">\(l\)</span>层的visual token被定义为： <span
class="math display">\[
\mathbf{z}^l_{s,t}\in\mathbb{R}^d, \;\;\; l=0,\dots,L-1, \;\;
s=0,\dots,S-1, \;\; t=0,\dots,T-1
\]</span> 同样，cls token<span
class="math inline">\(\mathbf{z}^l_{cls}\in\mathbb{R}^{d}\)</span>会被加在token序列。第<span
class="math inline">\(l\)</span>层变换器使用一系列多头自我注意力（MSA）、层归一化（LN）和MLP（<span
class="math inline">\(\mathbb{R}^d \rightarrow \mathbb{R}^{4d}
\rightarrow \mathbb{R}^d\)</span>）层来处理前一层的视觉标记<span
class="math inline">\(\mathbf{Z}^l\in\mathbb{R}^{(TS+1) \times
d}\)</span>，如下所示： <span class="math display">\[
\mathbf{Y}^{l}  =  \textrm{MSA}(\textrm{LN}(\mathbf{Z}^{l-1})) +
\mathbf{Z}^{l-1},\\
\mathbf{Z}^{l+1}  =  \textrm{MLP}(\textrm{LN}(\mathbf{Y}^{l})) +
\mathbf{Y}^{l}.
\]</span> 单个时空注意力的计算可被表示为 <span class="math display">\[
\mathbf{y}^{l}_{s,t} = \sum_{t&#39;=0}^{T-1} \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}, \;
\]</span> 最后，整个模型的复杂度为: <span
class="math inline">\(O(3hTSdd_h)\)</span> (<span
class="math inline">\(qkv\)</span> projections) <span
class="math inline">\(+ O(2hT^2S^2d_h)\)</span> (MSA for <span
class="math inline">\(h\)</span> attention heads) <span
class="math inline">\(+ O(TS(hd_h)d)\)</span> (multi-head projection)
<span class="math inline">\(+ O(4TSd^2)\)</span> (MLP).</p>
<h2 id="近似全时空注意力">近似全时空注意力</h2>
Baseline是一个通过在每个Transformer层应用纯空间注意力来执行对全时空注意力的简单近似的模型:
$$ <sup>{l}<em>{s,t} = </em>{s'=0}</sup>{S-1} {(^{l}<em>{s,t}
^{l}</em>{s',t})/} ^{l}_{s',t}, ;{
<span class="math display">\[\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\]</span>
<p>}</p>
<p>$$ 复杂度为<span
class="math inline">\(O(TS^2)\)</span>。在仅spatial-only注意力之后，对cls-token执行简单的时间平均
<span class="math inline">\(\mathbf{z}_{final} =
\frac{1}{T}\sum\limits_{t}
\mathbf{z}^{L-1}_{t,cls}\)</span>以获得一个特征，该特征被馈送到线性分类器。</p>
而TimeSFormer提出的factorised attention如下： $$
<span class="math display">\[\begin{split}
        \tilde{\mathbf{y} }^{l}_{s,t} = \sum_{t&#39;=0}^{T-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s,t&#39;})/\sqrt{d_h}\} \mathbf{v}^{l}_{s,t&#39;}, \\
        \mathbf{y}^{l}_{s,t} = \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{\tilde{\mathbf{q} }^{l}_{s,t} \cdot \tilde{\mathbf{k}
}^{l}_{s&#39;,t})/\sqrt{d_h}\} \tilde{\mathbf{v} }^{l}_{s&#39;,t},
    \end{split}
    \quad
    \begin{split}
         \; \begin{Bmatrix}
          s=0,\dots,S-1\\
          t=0,\dots,T-1
        \end{Bmatrix},
    \end{split}\]</span>
<p>$$ 并且把复杂度降低到<span class="math inline">\(O(T^2S +
TS^2)\)</span>​。然而，时间注意是对固定的空间位置进行的，当有相机或物体运动以及帧间存在空间错位时，时间注意是无效的。</p>
<p><strong>模型</strong>旨在更好地近似完整的时空自注意力（SA），同时将复杂度保持在<span
class="math inline">\(O(TS^2)\)</span>，即不对spatial-only模型产生进一步的复杂性。为了达到这个目的，论文提出了<strong>第一次近似</strong>，以执行全时空注意力，但仅限于局部时间窗口<span
class="math inline">\([-t_w, t_w]\)</span>： <span
class="math display">\[
\mathbf{y}^{l}_{s,t} = \sum_{t&#39;=t-t_w}^{t+t_w} \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{(\mathbf{q}^{l}_{s,t} \cdot
\mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}= \sum_{t&#39;=t-t_w}^{t+t_w}
\mathbf{V}^{l}_{t&#39;} \mathbf{a}^l_{t&#39;},
\;\big\{\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\big\}
\]</span> 其中<span
class="math inline">\(\mathbf{V}^{l}_{t&#39;}=[\mathbf{v}^{l}_{0,t&#39;};
\mathbf{v}^{l}_{1,t&#39;}; \dots;
\mathbf{v}^{l}_{S-1,t&#39;}]\in\mathbb{R}^{d_h \times S}\)</span>，<span
class="math inline">\(\mathbf{a}^l_{t&#39;}=[a^l_{0,t&#39;},
a^l_{1,t&#39;}, \dots,
a^l_{S-1,t&#39;}]\in\mathbb{R}^{S}\)</span>是向量与相应的注意权重。对于单个
Transformer 层，<span
class="math inline">\(\mathbf{y}^{l}_{s,t}\)</span> 是局部窗口 <span
class="math inline">\([-t_w, t_w]\)</span>
中视觉标记的时空组合。因此，在 <span class="math inline">\(k\)</span> 个
Transformer 层之后，<span
class="math inline">\(\mathbf{y}^{l+k}_{s,t}\)</span> 将是局部窗口 <span
class="math inline">\([-kt_w, kt_w]\)</span>
中视觉标记的时空组合，这反过来方便地允许对整个剪辑执行时空注意力。例如，对于
<span class="math inline">\(t_w=1\)</span> 和 <span
class="math inline">\(k=4\)</span>，局部窗口变为 <span
class="math inline">\([-4,
4]\)</span>，它在典型情况下覆盖整个视频剪辑（<span
class="math inline">\(T=8\)</span>）。</p>
<p>如上的局部自注意力的复杂度为 <span
class="math inline">\(O((2t_w+1)TS^2)\)</span>。为了进一步降低这个复杂度，在第一次近似之上进行<strong>第二次近似</strong>，如下所示：在空间位置
<span class="math inline">\(s\)</span> 和 <span
class="math inline">\(s’\)</span> 之间的注意力是 <span
class="math display">\[
\sum_{t&#39;=t-t_w}^{t+t_w}  \textrm{Softmax}\{(\mathbf{q}^{l}_{s,t}
\cdot \mathbf{k}^{l}_{s&#39;,t&#39;})/\sqrt{d_h}\}
\mathbf{v}^{l}_{s&#39;,t&#39;}
\]</span> 即它需要计算 <span class="math inline">\(2t_w+1\)</span>
个注意力，每个时间位置在 <span class="math inline">\([-t_w,
t_w]\)</span> 上计算一个。相反，论文建议在 <span
class="math inline">\([-t_w, t_w]\)</span> 上计算一个注意力，这可以通过
<span class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 关注 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w} \triangleq
[\mathbf{k}{l}_{s’,t-t_w};\dots;\mathbf{k}{l}_{s’,t+t_w}] \in
\mathbb{R}^{(2t_w+1)d_h}\)</span> 来实现。注意，为了匹配 <span
class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 和 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
的维度，通常需要对 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
进行进一步投影到 <span
class="math inline">\(\mathbb{R}^{d_h}\)</span>，其复杂度为 <span
class="math inline">\(O((2t_w+1)d_h^2)\)</span>。为了缓解这种情况，
使用``移位技巧’‘（类似TSM），它允许在 <span
class="math inline">\(O(d_h)\)</span>
内同时执行零成本降维、时空混合和注意力（在 <span
class="math inline">\(\mathbf{q}^{l}_{s,t}\)</span> 和 <span
class="math inline">\(\mathbf{k}^{l}_{s’,-t_w:t_w}\)</span>
之间）。具体来说，每个 <span class="math inline">\(t’ \in [-t_w,
t_w]\)</span> 被分配 <span class="math inline">\(d_h^{t’}\)</span>
个来自 <span class="math inline">\(d_h\)</span> 的通道（即 <span
class="math inline">\(\sum_{t’} d_h^{t’} = d_h\)</span>）。设 <span
class="math inline">\(\mathbf{k}^{l}_{s’,t’}(d_h^{t’})\in
\mathbb{R}^{d_h^{t’} }\)</span> 表示索引 <span
class="math inline">\(\mathbf{k}^{l}_{s’,t’}\)</span> 中的 <span
class="math inline">\(d_h^{t’}\)</span>
个通道的运算符。然后，构造一个新的key向量：</p>
<p><span class="math display">\[
\tilde{\mathbf{k} }^{l}_{s&#39;,-t_w:t_w} \triangleq
[\mathbf{k}^{l}_{s&#39;,t-t_w}(d_h^{t-t_w}), \dots,
\mathbf{k}^{l}_{s&#39;,t+t_w}(d_h^{t+t_w})]\in \mathbb{R}^{d_h}
\]</span></p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230317163928156.png"
alt="Detailed self-attention computation graph" />
<figcaption aria-hidden="true">Detailed self-attention computation
graph</figcaption>
</figure>
<p>上图展示了如何构造<span class="math inline">\(\tilde{\mathbf{k}
}^{l}_{s&#39;,-t_w:t_w}\)</span>，按照同样的方式可以构造一个新的value向量<span
class="math inline">\(\tilde{\mathbf{v}
}^{l}_{s&#39;,-t_w:t_w}\)</span>。最后，提出的对全时空注意力的近似为:</p>
<p><span class="math display">\[
\mathbf{y}^{l_s}_{s,t} = \sum_{s&#39;=0}^{S-1}
\textrm{Softmax}\{(\mathbf{q}^{l_s}_{s,t} \cdot \tilde{\mathbf{k}
}^{l}_{s&#39;,-t_w:t_w})/\sqrt{d_h}\} \tilde{\mathbf{v}
}^{l}_{s&#39;,-t_w:t_w},
\;\big\{\begin{smallmatrix}
  s=0,\dots,S-1\\
  t=0,\dots,T-1
\end{smallmatrix}\big\}.  
\]</span></p>
<h3 id="temporal-attention-aggregation">Temporal Attention
aggregation</h3>
<p>最后一组cls-token<span
class="math inline">\(\mathbf{z}^{L-1}_{t,cls}, 0 \leq t \leq
L-1\)</span>会被用于生成预测结果，为此，论文提出了两个方案：</p>
<ol type="1">
<li>在论文的baseline下，实验简单的时序平均<span
class="math inline">\(\mathbf{z}_{final} = \frac{1}{T}\sum_{t}
\mathbf{z}^{L-1}_{t,cls}\)</span></li>
<li>时间平均显然忽略了时序信息，因此论文提出使用轻量级的时间注意(TA)机制，该机制将参与<span
class="math inline">\(T\)</span> 个cls-token。具体来说，token <span
class="math inline">\(\mathbf{z}_{final}\)</span>
使用时序Transformer处理序列<span
class="math inline">\([\mathbf{z}^{L-1}_{0,cls}, \ldots ,
\mathbf{z}^{L-1}_{T-1,cls}]\)</span>。这类似于ViViT的(并发)工作，不同之处在于，在模型中，发现一个单一的TA层就足够了，而ViViT使用<span
class="math inline">\(L_t\)</span>层。</li>
</ol>
<h3 id="summary-token">Summary token</h3>
<p>作为TA的替代方案，论文还提出了一种简单的轻量级机制，用于在网络中间层的不同帧之间进行信息交换。给定每一帧<span
class="math inline">\(t\)</span>的token集，<span
class="math inline">\(\mathbf{Z}_{t}^{l-1}\in\mathbb{R}^{(S+1)\times
d_h}\)</span>（通过连接所有token<span
class="math inline">\(\mathbf{z}_{s,t}^{l-1},
s=0,\dots,S\)</span>来构造），计算得到<span
class="math inline">\(R\)</span>个token<span
class="math inline">\(\mathbf{Z}^{l}_{r, t} =
\phi(\mathbf{Z}_{t}^{l-1})\in\mathbb{R}^{R\times
d_h}\)</span>组成的一个新token集，这样的一个token集总结了帧信息，因此被命名为“摘要”token。然后将这些token附加到所有帧的visual
token中，以计算key和value，以便query向量处理原始key和Summary标记。论文探讨了<span
class="math inline">\(\phi(.)\)</span>执行简单空间平均的情况，即<span
class="math inline">\(\mathbf{z}^{l}_{0, t} = \frac{1}{S}\sum_{s}
\mathbf{z}^{l}_{s,t}\)</span>在每一帧的token上（对于这种情况，<span
class="math inline">\(R=1\)</span>）。请注意，对于<span
class="math inline">\(R=1\)</span>，Summary token引起的额外成本是<span
class="math inline">\(O(TS)\)</span>。</p>
<!-- # 实验

## 网络架构等细节 -->
<h1 id="代码">代码</h1>
<h2 id="模型整体">模型整体</h2>
<p>模型部分由ViT作为Base_model，在ViT的基础上主要修改了Attention部分，取其中一个<code>Transformer_block</code>打印如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">(0): Block(</span><br><span class="line">  (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">  (attn): Attention(</span><br><span class="line">    (qkv): Linear(in_features=768, out_features=2304, bias=True)</span><br><span class="line">    (attn_drop): Dropout(p=0.0, inplace=False)</span><br><span class="line">    (proj): Linear(in_features=768, out_features=768, bias=True)</span><br><span class="line">    (proj_drop): Dropout(p=0, inplace=False)</span><br><span class="line">    (control_point): AfterReconstruction()</span><br><span class="line">    (control_point_query): TemporalShift(</span><br><span class="line">      (net): AfterReconstruction()</span><br><span class="line">    )</span><br><span class="line">    (control_point_value): TemporalShift(</span><br><span class="line">      (net): AfterReconstruction()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (drop_path): Identity()</span><br><span class="line">  (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)</span><br><span class="line">  (mlp): Mlp(</span><br><span class="line">    (fc1): Linear(in_features=768, out_features=3072, bias=True)</span><br><span class="line">    (act): GELU(approximate=&#x27;none&#x27;)</span><br><span class="line">    (fc2): Linear(in_features=3072, out_features=768, bias=True)</span><br><span class="line">    (drop): Dropout(p=0, inplace=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>构建模型的代码主要位于<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/video_model_builder.py">video_model_builder.py</a>，采用的是在base_model的基础上做修改的方式（基础模型应该是类似于TimesFormer中的联合时空的方式），make_temporal_shift应该就是论文提到的“移位技巧”的具体代码实现。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> timm.models <span class="keyword">import</span> create_model</span><br><span class="line">self.base_model = create_model()</span><br><span class="line"><span class="keyword">if</span> self.cfg.XVIT.USE_XVIT:</span><br><span class="line">  make_temporal_shift(</span><br><span class="line">      self.base_model,</span><br><span class="line">      self.cfg.XVIT.NUM_SEGMENTS,</span><br><span class="line">      n_div=self.cfg.XVIT.SHIFT_DIV,</span><br><span class="line">      locations_list=self.cfg.XVIT.LOCATIONS_LIST,</span><br><span class="line">  )</span><br></pre></td></tr></table></figure></p>
<h2 id="注意力">注意力</h2>
<p>而注意力部分就是这篇论文的最主要的部分，相关代码在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/transformers/transformer_block.py">transformer_block.py</a>，可以看到大多数的代码与ViT的一样，主要在<code>Attention</code>类做出了修改，对k和v做了构造，以达到对全时空注意力的近似：
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.insert_control_point:</span><br><span class="line">    k = self.control_point_query(k)</span><br><span class="line">    v = self.control_point_value(v)</span><br></pre></td></tr></table></figure>
其中的<code>control_point_query</code>和<code>control_point_value</code>是一个继承<code>nn.Identity</code>类的名为<code>AfterReconstruction</code>的类，起到一个占位的作用，而在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/temporal_shift.py">temporal_shift.py</a>中的make_temporal_shift函数会用<code>TemporalShift</code>类去替代这个<code>AfterReconstruction</code>。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_temporal_shift</span>(<span class="params">net, n_segment, n_div=<span class="number">8</span>, locations_list=[]</span>):</span><br><span class="line">    n_segment_list = [n_segment] * <span class="number">20</span></span><br><span class="line">    <span class="keyword">assert</span> n_segment_list[-<span class="number">1</span>] &gt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> idx, block <span class="keyword">in</span> <span class="built_in">enumerate</span>(net.blocks):</span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> locations_list:</span><br><span class="line">            net.blocks[idx].attn.control_point_query = TemporalShift(</span><br><span class="line">                net.blocks[idx].attn.control_point_query,</span><br><span class="line">                n_segment=n_segment_list[counter + <span class="number">2</span>],</span><br><span class="line">                n_div=n_div,</span><br><span class="line">            )</span><br><span class="line">            net.blocks[idx].attn.control_point_value = TemporalShift(</span><br><span class="line">                net.blocks[idx].attn.control_point_value,</span><br><span class="line">                n_segment=n_segment_list[counter + <span class="number">2</span>],</span><br><span class="line">                n_div=n_div,</span><br><span class="line">            )</span><br><span class="line">            counter += <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TemporalShift</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, net, n_segment=<span class="number">3</span>, n_div=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(TemporalShift, self).__init__()</span><br><span class="line">        self.net = net</span><br><span class="line">        self.n_segment = n_segment</span><br><span class="line">        self.fold_div = n_div</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        nt, num_heads, d, c = x.size()</span><br><span class="line">        n_batch = nt // self.n_segment</span><br><span class="line">        x = x.view(n_batch, self.n_segment, num_heads, d, c)</span><br><span class="line">        fold = c * num_heads // self.fold_div</span><br><span class="line"></span><br><span class="line">        x = (</span><br><span class="line">            x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">            .view(n_batch, self.n_segment, num_heads * c, d)</span><br><span class="line">        )</span><br><span class="line">        out = torch.zeros_like(x)</span><br><span class="line">        out[:, :-<span class="number">1</span>, :fold] = x[:, <span class="number">1</span>:, :fold]  <span class="comment"># shift left</span></span><br><span class="line">        out[:, <span class="number">1</span>:, fold : <span class="number">2</span> * fold] = x[:, :-<span class="number">1</span>, fold : <span class="number">2</span> * fold]  <span class="comment"># shift right</span></span><br><span class="line">        out[:, :, <span class="number">2</span> * fold :] = x[:, :, <span class="number">2</span> * fold :]  <span class="comment"># not shift</span></span><br><span class="line"></span><br><span class="line">        out = (</span><br><span class="line">            out.view(n_batch, self.n_segment, num_heads, c, d)</span><br><span class="line">            .permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">            .contiguous()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out.view(nt, num_heads, d, c)</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>TemporalShift</code>部分代码就是TSM的代码<a
target="_blank" rel="noopener" href="https://github.com/mit-han-lab/temporal-shift-module">temporal-shift-module</a>，也就是说XViT类似于ViT以TSM的方式去获得时序信息。</p>
<h2 id="temporal-attention-aggregation-1">Temporal Attention
aggregation</h2>
<p>论文采用轻量级的时间注意(TA)机制处理最后一层注意力层所输出的cls——token，相关代码在<a
target="_blank" rel="noopener" href="https://github.com/1adrianb/video-transformers/blob/main/slowfast/models/head_helper.py">head_helper.py</a>，如下所示
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, position_ids</span>):</span><br><span class="line">    <span class="comment"># temporal encoder (Longformer)</span></span><br><span class="line">    B, D, E = x.shape</span><br><span class="line"></span><br><span class="line">    cls_tokens = self.cls_token.expand(</span><br><span class="line">        B, -<span class="number">1</span>, -<span class="number">1</span></span><br><span class="line">    )  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">    x = self.temporal_encoder(x)</span><br><span class="line">    <span class="comment"># MLP head</span></span><br><span class="line">    x = self.mlp_head(x[:, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<h1 id="from-chatgpt-just-for-fun">From ChatGPT (Just for fun)</h1>
<p>Summary: The paper proposes a video recognition model using
Transformers that scales linearly with the number of frames and avoids
computational overhead by approximating space-time attention with local
temporal windows and efficient space-time mixing.</p>
<p>Core Problems and Solutions:</p>
<ul>
<li>Computational overhead induced by modeling temporal information in
video recognition with Transformers. (Solution: proposing a model that
scales linearly with the number of frames by approximating space-time
attention with local temporal windows and efficient space-time
mixing).</li>
</ul>
<p>Main Insights and Lessons Learned:</p>
<ul>
<li>The proposed video recognition model achieves state-of-the-art
results while avoiding computational overheads.</li>
<li>The use of local temporal windows and efficient space-time mixing
can significantly reduce the complexity of video recognition
models.</li>
</ul>
<p>Questions:</p>
<ol type="1">
<li>How does the proposed video recognition model compare to existing
models in terms of recognition accuracy?</li>
<li>How does the model's efficiency compare to other approaches that
avoid computational overhead in video recognition?</li>
<li>Can the proposed approach be extended to other applications that use
video data, such as action recognition or video captioning?</li>
</ol>
<p>Future Research Directions:</p>
<ol type="1">
<li>Investigating the impact of the proposed model on different video
recognition tasks and datasets.</li>
<li>Exploring the use of local temporal windows and efficient space-time
mixing in other areas of video analysis, such as action recognition or
video captioning.</li>
<li>Extending the proposed approach to incorporate other types of
attention mechanisms or to combine it with other efficient architectures
for video recognition.</li>
</ol>
<p>Relevant Documents:</p>
<ol type="1">
<li>"Attention is All You Need" by Vaswani et al.</li>
<li>"Temporal Segment Networks: Towards Good Practices for Deep Action
Recognition" by Wang et al.</li>
<li>"Deep Residual Learning for Image Recognition" by He et al.</li>
<li>"A Closer Look at Spatiotemporal Convolutions for Action
Recognition" by Tran et al.</li>
<li>"Convolutional Sequence to Sequence Learning" by Gehring et
al.✏</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/03/13/TimesFormer%E4%B8%8EViViT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/03/13/TimesFormer%E4%B8%8EViViT/" class="post-title-link" itemprop="url">TimeSFormer与ViViT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-03-13 12:24:22" itemprop="dateCreated datePublished" datetime="2023-03-13T12:24:22+08:00">2023-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-03-22 13:09:57" itemprop="dateModified" datetime="2023-03-22T13:09:57+08:00">2023-03-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="timesformer">TimeSFormer</h1>
<p>paper: https://arxiv.org/abs/2102.05095</p>
<p>code: https://github.com/facebookresearch/TimeSformer</p>
<h2 id="摘要">摘要</h2>
<p>我们提出了一种基于空间和时间上的自注意力机制的无卷积视频分类方法。我们的方法，命名为“TimeSformer”，通过从一系列帧级别的图像块直接进行时空特征学习，将标准的Transformer架构适应到视频上。我们的实验研究比较了不同的自注意力方案，并发现“分割注意力”架构，在每个网络块中分别应用时间注意力和空间注意力，能够在我们考虑的设计选择中获得最佳的视频分类准确率。尽管设计完全不同，TimeSformer在几个动作识别基准上都达到了最先进的结果，包括在Kinetics-400和Kinetics-600上获得了最佳的准确率。最后，与3D卷积网络相比，我们的模型训练速度更快，可以实现更高的测试效率（以较小的准确率损失为代价），并且可以应用于更长的视频片段（超过一分钟）。</p>
<h2 id="整体架构">整体架构</h2>
<p><strong>输入视频</strong>：TimeSformer的输入为<span
class="math inline">\(X \in \mathbb{R}^{H \times W \times 3 \times
F}\)</span>，表示<span class="math inline">\(F\)</span>个size为$HW
$的RGB帧。</p>
<p><strong>转换为Patch</strong>：与ViT一样，将每一帧分解为N个不重叠的Patch，每一个patch的大小都为<span
class="math inline">\(P \times P\)</span>，因此<span
class="math inline">\(N = HW/P^2\)</span>。把patch展开为向量<span
class="math inline">\(\mathbf{x}_{(p, t)} \in \mathbb{R}^{3
P^{2}}\)</span>，其中<span class="math inline">\(p = 1, \dots ,
N\)</span>表示空间位置，<span class="math inline">\(t = 1, \dots
,F\)</span>为时间帧的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">            B, C, T, H, W = x.shape</span><br><span class="line">            x = rearrange(x, <span class="string">&#x27;b c t h w -&gt; (b t) c h w&#x27;</span>)</span><br><span class="line">            x = self.proj(x) <span class="comment"># ((bt), dim, h//p, w//p)</span></span><br><span class="line">            W = x.size(-<span class="number">1</span>)</span><br><span class="line">            x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)<span class="comment"># ((bt), (hw)//(p^2), dim)</span></span><br><span class="line">            <span class="keyword">return</span> x, T, W</span><br></pre></td></tr></table></figure>
<p><strong>Linear embedding</strong>：将<span
class="math inline">\(\mathbf{x}_{(p, t)} \in \mathbb{R}^{3
P^{2}}\)</span>线性映射到<span class="math inline">\(\mathbf{z}_{(p,
t)}^{(0)} \in \mathbb{R}^{D}\)</span>： <span class="math display">\[
\mathbf{z}_{(p, t)}^{(0)}=E \mathbf{x}_{(p, t)}+\mathbf{e}_{(p, t)}^{p o
s}
\]</span> 其中，<span class="math inline">\(E \in \mathbb{R}^{D \times 3
P^{2}}\)</span>为可学习的线性映射系数矩阵，<span
class="math inline">\(\mathbf{e}^{pos}_{(p,t)} \in \mathbb{R} ^
D\)</span>为可学习的空间位置编码，<span
class="math inline">\(\mathbf{z}_{(p, t)}\)</span>
序列是Transformer的输入。与ViT一样，在序列的第一个位置加入一个可学习的向量<span
class="math inline">\(\mathbf{z}_{(0, 0)}\)</span>作为cls-token。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">B = x.shape[<span class="number">0</span>]</span><br><span class="line">x, T, W = self.patch_embed(x)</span><br><span class="line">cls_tokens = self.cls_token.expand(x.size(<span class="number">0</span>), -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">x = x + self.pos_embed <span class="comment"># pose embeding</span></span><br></pre></td></tr></table></figure>
<p><strong>QKV计算</strong>：Transformer包含<span
class="math inline">\(L\)</span>层encoding
blocks，每个block的query、key、value表达为 <span class="math display">\[
\begin{array}{l}
\mathbf{q}_{(p, t)}^{(\ell, a)}=W_{Q}^{(\ell, a)}
\operatorname{LN}\left(\mathbf{z}_{(p, t)}^{(\ell-1)}\right) \in
\mathbb{R}^{D_{h}} \\
\mathbf{k}_{(p, t)}^{(\ell, a)}=W_{K}^{(\ell, a)}
\operatorname{LN}\left(\mathbf{z}_{(p, t)}^{(\ell-1)}\right) \in
\mathbb{R}^{D_{h}} \\
\mathbf{v}_{(p, t)}^{(\ell, a)}=W_{V}^{(\ell, a)}
\operatorname{LN}\left(\mathbf{z}_{(p, t)}^{(\ell-1)}\right) \in
\mathbb{R}^{D_{h}}
\end{array}
\]</span> 其中，<span class="math inline">\(a = 1 ,
\dots,A\)</span>代表注意力头的数量，<span
class="math inline">\(D_h\)</span>代表每个head的维度</p>
<p><strong>自注意力计算</strong>：对于自注意力计算部分，论文给出了五种不同的方式，将在下一节详细介绍。对于query
patch<span class="math inline">\((p,t)\)</span>的自注意力权重<span
class="math inline">\(\boldsymbol{\alpha}_{(p, t)}^{(\ell, a)} \in
\mathbb{R}^{N F+1}\)</span>通用的给出如下： <span
class="math display">\[
\boldsymbol{\alpha}_{(p, t)}^{(\ell,
a)}=\operatorname{SM}\left(\frac{\mathbf{q}_{(p, t)}^{(\ell,
a)}}{\sqrt{D_{h}}} \cdot\left[\mathbf{k}_{(0,0)}^{(\ell,
a)}\left\{\mathbf{k}_{\left(p^{\prime}, t^{\prime}\right)}^{(\ell,
a)}\right\}_{\substack{p^{\prime}=1, \ldots, N \\ t^{\prime}=1, \ldots,
F}}\right]\right)
\]</span> <strong>编码</strong>：第<span
class="math inline">\(\ell\)</span>个编码<span
class="math inline">\(\mathbf{z}_{(p,
t)}^{(\ell)}\)</span>是利用每个注意头的自注意系数计算值向量的加权和得到：
<span class="math display">\[
\mathbf{s}_{(p, t)}^{(\ell, a)}=\alpha_{(p, t),(0,0)}^{(\ell, a)}
\mathbf{v}_{(0,0)}^{(\ell, a)}+\sum_{p^{\prime}=1}^{N}
\sum_{t^{\prime}=1}^{F} \alpha_{(p, t),\left(p^{\prime},
t^{\prime}\right)}^{(\ell, a)} \mathbf{v}_{\left(p^{\prime},
t^{\prime}\right)}^{(\ell, a)}
\]</span>
然后，将来自所有head的这些向量的拼接映射并通过MLP，这两个操作都具有残差连接：
<span class="math display">\[
\begin{array}{l}
\mathbf{z}_{(p, t)}^{\prime(\ell)}=W_{O}\left[\begin{array}{c}
\mathbf{s}_{(p, t)}^{(\ell, 1)} \\
\vdots \\
\mathbf{s}_{(p, t)}^{(\ell, \mathcal{A})}
\end{array}\right]+\mathbf{z}_{(p, t)}^{(\ell-1)} \\
\mathbf{z}_{(p,
t)}^{(\ell)}=\operatorname{MLP}\left(\operatorname{LN}\left(\mathbf{z}_{(p,
t)}^{\prime(\ell)}\right)\right)+\mathbf{z}_{(p, t)}^{\prime(\ell)} .
\end{array}
\]</span> <strong>分类 embedding</strong> 取出cls-token用作最终的分类：
<span class="math display">\[
y = MLP(LN(\mathbf{z}^{(L)}_{(0, 0)}))
\]</span></p>
<h2 id="自注意力机制">自注意力机制</h2>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230313191932395.png"
alt="论文研究的视频自注意力块" />
<figcaption aria-hidden="true">论文研究的视频自注意力块</figcaption>
</figure>
<p>通过对输入图像进行分块，论文中一共研究了五种不同的注意力机制：</p>
<ol type="1">
<li>空间注意力机制（S)：只取同一帧内的图像块进行自注意力机制</li>
<li>时空共同注意力机制（ST）：取所有帧中的所有图像块进行注意力机制</li>
<li>分开的时空注意力机制（T+S）：先对同一帧中的所有图像块进行自注意力机制，然后对不同帧中<strong>对应位置</strong>的图像块进行注意力机制</li>
<li>稀疏局部全局注意力机制（L+G）：先利用所有帧中，相邻的 H/2 和 W/2
的图像块计算局部的注意力，然后在空间上，使用2个图像块的步长，在整个序列中计算自注意力机制，这个可以看做全局的时空注意力更快的近似</li>
<li>轴向的注意力机制（T+W+H）：先在时间维度上进行自注意力机制，然后在纵坐标相同的图像块上进行自注意力机制，最后在横坐标相同的图像块上进行自注意力机制</li>
</ol>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230313192113759.png"
alt="论文研究的五种时空自注意力机制方案的可视化" />
<figcaption
aria-hidden="true">论文研究的五种时空自注意力机制方案的可视化</figcaption>
</figure>
<h3 id="space-attention">Space Attention</h3>
<p>Space
Attention就是标准的Transformer结构，只计算空间注意力，跟ViT一样，只有<span
class="math inline">\(N+1\)</span>个query-key对。 <span
class="math display">\[
\boldsymbol{\alpha}_{(p, t)}^{(\ell, a) \text { space
}}=\operatorname{SM}\left(\frac{\mathbf{q}_{(p, t)}^{(\ell,
a)}}{\sqrt{D_{h}}} \cdot\left[\mathbf{k}_{(0,0)}^{(\ell,
a)}\left\{\mathbf{k}_{\left(p^{\prime}, t\right)}^{(\ell,
a)}\right\}_{p^{\prime}=1, \ldots, N}\right]\right)
\]</span> 代码部分，向量加上空间位置编码后，就可以输入到Attention
blocks，而由于在转换patch的时候batchsize和t是并在一起的，所以需要转换回去，并多帧取平均。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Attention blocks</span></span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">    x = blk(x, B, T, W)</span><br><span class="line"></span><br><span class="line"><span class="comment">### Predictions for space-only baseline</span></span><br><span class="line"><span class="keyword">if</span> self.attention_type == <span class="string">&#x27;space_only&#x27;</span>:</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;(b t) n m -&gt; b t n m&#x27;</span>,b=B,t=T)</span><br><span class="line">    x = torch.mean(x, <span class="number">1</span>) <span class="comment"># averaging predictions for every frame</span></span><br></pre></td></tr></table></figure>
<p>self.blocks如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_spatial_tokens = (x.size(<span class="number">1</span>) - <span class="number">1</span>) // T</span><br><span class="line">H = num_spatial_tokens // W</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> self.attention_type <span class="keyword">in</span> [<span class="string">&#x27;space_only&#x27;</span>, <span class="string">&#x27;joint_space_time&#x27;</span>]:</span><br><span class="line">    x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">    x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="joint-space-time-attention">Joint Space-Time Attention</h3>
<p><span class="math display">\[
\boldsymbol{\alpha}_{(p, t)}^{(\ell,
a)}=\operatorname{SM}\left(\frac{\mathbf{q}_{(p, t)}^{(\ell,
a)}}{\sqrt{D_{h}}} \cdot\left[\mathbf{k}_{(0,0)}^{(\ell,
a)}\left\{\mathbf{k}_{\left(p^{\prime}, t^{\prime}\right)}^{(\ell,
a)}\right\}_{\substack{p^{\prime}=1, \ldots, N \\ t^{\prime}=1, \ldots,
F}}\right]\right)
\]</span></p>
<p>Joint Space-Time
Attention需要在输入Transformer前先加上TimeEmbeeding，相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Time Embeddings</span></span><br><span class="line"><span class="keyword">if</span> self.attention_type != <span class="string">&#x27;space_only&#x27;</span>:</span><br><span class="line">    cls_tokens = x[:B, <span class="number">0</span>, :].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    x = x[:,<span class="number">1</span>:]</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;(b t) n m -&gt; (b n) t m&#x27;</span>,b=B,t=T)</span><br><span class="line">	x = x + self.time_embed</span><br><span class="line">    x = self.time_drop(x)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;(b n) t m -&gt; b (n t) m&#x27;</span>,b=B,t=T)</span><br><span class="line">    x = torch.cat((cls_tokens, x), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="divided-space-time-attention">Divided Space-Time Attention</h3>
<p>对于Divided Space-Time Attention，先计算时间自注意力如下： <span
class="math display">\[
\boldsymbol{\alpha}_{(p, t)}^{(\ell, a) \text { time
}}=\operatorname{SM}\left(\frac{\mathbf{q}_{(p, t)}^{(\ell,
a)^{\top}}}{\sqrt{D_{h}}} \cdot\left[\mathbf{k}_{(0,0)}^{(\ell,
a)}\left\{\mathbf{k}_{\left(p, t^{\prime}\right)}^{(\ell,
a)}\right\}_{t^{\prime}=1, \ldots, F}\right]\right)
\]</span>
得到时间注意力权重后，同样经过编码的操作，但不通过MLP，得到时间编码<span
class="math inline">\(z&#39;^{(\ell )time}_{(p,t)}\)</span>。</p>
<p>通过时间编码可计算出响应的Q、K、V，计算空间自注意力如下： <span
class="math display">\[
\boldsymbol{\alpha}_{(p, t)}^{(\ell, a) \text { space
}}=\operatorname{SM}\left(\frac{\mathbf{q}_{(p, t)}^{(\ell,
a)}}{\sqrt{D_{h}}} \cdot\left[\mathbf{k}_{(0,0)}^{(\ell,
a)}\left\{\mathbf{k}_{\left(p^{\prime}, t\right)}^{(\ell,
a)}\right\}_{p^{\prime}=1, \ldots, N}\right]\right)
\]</span> 这样得到的自注意力权重就会包含时空信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">elif</span> self.attention_type == <span class="string">&#x27;divided_space_time&#x27;</span>:</span><br><span class="line">    <span class="comment">## Temporal</span></span><br><span class="line">    xt = x[:,<span class="number">1</span>:,:]</span><br><span class="line">    xt = rearrange(xt, <span class="string">&#x27;b (h w t) m -&gt; (b h w) t m&#x27;</span>,b=B,h=H,w=W,t=T)</span><br><span class="line">    res_temporal = self.drop_path(self.temporal_attn(self.temporal_norm1(xt)))</span><br><span class="line">    res_temporal = rearrange(res_temporal, <span class="string">&#x27;(b h w) t m -&gt; b (h w t) m&#x27;</span>,b=B,h=H,w=W,t=T)</span><br><span class="line">    res_temporal = self.temporal_fc(res_temporal)</span><br><span class="line">    xt = x[:,<span class="number">1</span>:,:] + res_temporal</span><br><span class="line"></span><br><span class="line">    <span class="comment">## Spatial</span></span><br><span class="line">    init_cls_token = x[:,<span class="number">0</span>,:].unsqueeze(<span class="number">1</span>)</span><br><span class="line">    cls_token = init_cls_token.repeat(<span class="number">1</span>, T, <span class="number">1</span>)</span><br><span class="line">    cls_token = rearrange(cls_token, <span class="string">&#x27;b t m -&gt; (b t) m&#x27;</span>,b=B,t=T).unsqueeze(<span class="number">1</span>)</span><br><span class="line">    xs = xt</span><br><span class="line">    xs = rearrange(xs, <span class="string">&#x27;b (h w t) m -&gt; (b t) (h w) m&#x27;</span>,b=B,h=H,w=W,t=T)</span><br><span class="line">    xs = torch.cat((cls_token, xs), <span class="number">1</span>)</span><br><span class="line">    res_spatial = self.drop_path(self.attn(self.norm1(xs)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### Taking care of CLS token</span></span><br><span class="line">    cls_token = res_spatial[:,<span class="number">0</span>,:]</span><br><span class="line">    cls_token = rearrange(cls_token, <span class="string">&#x27;(b t) m -&gt; b t m&#x27;</span>,b=B,t=T)</span><br><span class="line">    cls_token = torch.mean(cls_token,<span class="number">1</span>,<span class="literal">True</span>) <span class="comment">## averaging for every frame</span></span><br><span class="line">    res_spatial = res_spatial[:,<span class="number">1</span>:,:]</span><br><span class="line">    res_spatial = rearrange(res_spatial, <span class="string">&#x27;(b t) (h w) m -&gt; b (h w t) m&#x27;</span>,b=B,h=H,w=W,t=T)</span><br><span class="line">    res = res_spatial</span><br><span class="line">    x = xt</span><br><span class="line"></span><br><span class="line">    <span class="comment">## Mlp</span></span><br><span class="line">    x = torch.cat((init_cls_token, x), <span class="number">1</span>) + torch.cat((cls_token, res), <span class="number">1</span>)</span><br><span class="line">    x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h1 id="vivit">ViViT</h1>
<p>paper: https://arxiv.org/abs/2103.15691</p>
<p>code:
https://github.com/google-research/scenic/tree/main/scenic/projects/vivit</p>
<h2 id="摘要-1">摘要</h2>
<p>这篇文章介绍了一种基于纯变换器的视频分类模型，该模型受到图像分类中此类模型的最近成功的启发。为了有效处理视频中可能遇到的大量时空token，我们提出了几种沿着空间和时间维度分解我们模型的方法，以提高效率和可扩展性。此外，为了在较小的数据集上有效地训练我们的模型，我们展示了如何在训练过程中对模型进行正则化并利用预训练的图像模型。我们进行了彻底的消融研究，并在多个视频分类基准测试中取得了最先进的结果，包括
Kinetics 400 和 600、Epic Kitchens、Something-Something v2 和 Moments in
Time，优于基于深度 3D 卷积网络的先前方法。</p>
<h2 id="vit概述">ViT概述</h2>
<p>视觉变换器 (ViT) 适应了变换器架构，以最小的改动来处理2D图像。
具体来说，ViT 提取 <span class="math inline">\(N\)</span>
个不重叠的图像块，<span class="math inline">\(x_i \in \mathbb{R}^{h
\times w}\)</span>，执行线性投影，然后将它们栅格化为1Dtoken <span
class="math inline">\(z_i \in
\mathbb{R}^d\)</span>。输入到下面的变换器编码器的token序列为 <span
class="math display">\[
\mathbf{z} = [z_{cls}, \mathbf{E}x_1, \mathbf{E}x_2, \ldots ,
\mathbf{E}x_N] + \mathbf{p}
\]</span> 其中，<span class="math inline">\(\mathbf{E}\)</span>
的投影相当于2D卷积。一个可选的学习分类token <span
class="math inline">\(z_{cls}\)</span>
被添加到这个序列的前面，它在编码器的最后一层的表示作为分类层使用的最终表示。
此外，一个学习位置嵌入，<span class="math inline">\(\mathbf{p} \in
\mathbb{R}^{N \times
d}\)</span>，被添加到token中以保留位置信息，因为变换器中后续的自注意力操作是排列不变的。
然后将token传递到由 <span class="math inline">\(L\)</span>
个变换器层组成的编码器中。 每一层 <span
class="math inline">\(\ell\)</span> 都包括多头自注意力、层归一化 (LN) 和
MLP 块，如下所示： <span class="math display">\[
\mathbf{y}^{\ell} = \text{MSA}(\text{LN}(\mathbf{z}^\ell))
+\mathbf{z}^\ell  \\
\mathbf{z}^{\ell + 1} = \text{MLP}(\text{LN}(\mathbf{y}^\ell)) +
\mathbf{y}^\ell
\]</span> MLP 由两个线性投影组成，它们之间由 GELU 非线性分隔，token维度
<span class="math inline">\(d\)</span> 在所有层中保持不变。
最后，使用线性分类器根据 <span class="math inline">\(z_{cls}^L \in
\mathbb{R}^d\)</span>
对编码输入进行分类，如果它被添加到输入的前面，或者对所有token进行全局平均池化
<span class="math inline">\(\mathbf{z}^{L}\)</span>。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314154425261.png"
alt="ViViT架构" />
<figcaption aria-hidden="true">ViViT架构</figcaption>
</figure>
<h2 id="视频片段-embeeding">视频片段 Embeeding</h2>
<p>论文给出了两种方法把视频<span class="math inline">\(\mathbf{V} \in
\mathbb{R}^{T \times H \times W \times
C}\)</span>映射到一个token序列<span
class="math inline">\(\mathbf{\tilde{z}} \in \mathbb{R}^{n_t \times n_h
\times n_w \times d}\)</span>，接着加上位置embedding并reshape为<span
class="math inline">\(\mathbb{R}^{N \times
d}\)</span>以得到transformer的输入<span
class="math inline">\(\mathbf{z}\)</span>。</p>
<h3 id="uniform-frame-sampling">Uniform frame sampling</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314162311574.png"
alt="Uniform frame sampling" />
<figcaption aria-hidden="true">Uniform frame sampling</figcaption>
</figure>
<p>如图所示，标记输入视频的一种简单方法是从输入视频剪辑中均匀采样 <span
class="math inline">\(n_t\)</span> 帧，使用与 ViT 相同的方法独立嵌入每个
2D 帧，并将所有这些token连接在一起。 具体地，如果从每个帧中提取 <span
class="math inline">\(n_h \cdot n_w\)</span> 个不重叠的图像块，则总共有
<span class="math inline">\(n_t \cdot n_h \cdot n_w\)</span>
个token将通过变换器编码器转发。
直观地，这个过程可以看作是简单地构造一个大的 2D 图像来按照 ViT
进行标记。 这种方法跟TimeSformer的一样。</p>
<h3 id="tubelet-embedding">Tubelet embedding</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314162920579.png"
alt="Tubelet embedding" />
<figcaption aria-hidden="true">Tubelet embedding</figcaption>
</figure>
<p>另一种方法，如图所示，是从输入体积中提取不重叠的时空“tubes”，并将其线性投影到
<span class="math inline">\(\mathbb{R}^d\)</span>。 这种方法是 ViT
embedding的 3D 扩展，类似于 3D 卷积。 对于一个维度为 <span
class="math inline">\(t \times h \times w\)</span> 的tubelet，<span
class="math inline">\(n_t =
\left\lfloor\frac{T}{t}\right\rfloor\)</span>，<span
class="math inline">\(n_h =
\left\lfloor\frac{H}{h}\right\rfloor\)</span> 和 <span
class="math inline">\(n_w =
\left\lfloor\frac{W}{w}\right\rfloor\)</span>，分别从时间、高度和宽度维度提取token。
较小的tubelet尺寸因此会导致更多的token，增加了计算量。直观地说，这种方法在标记过程中融合了时空信息，而与“均匀帧采样”不同，在那里来自不同帧的时间信息由变换器融合。</p>
<h2 id="视频transformer-模型">视频Transformer 模型</h2>
<h3 id="model-1-spatio-temporal-attention">Model 1: Spatio-temporal
attention</h3>
<p>这个模型跟TimeSformer中的Joint Space-Time
Attention基本一致，简单地将所有的时空token<span
class="math inline">\(\mathbf{z}^{0}\)</span>输入到Transformer的编码器。而由于token的数量会随着采样帧的变多而变多，这样就会带来更大的计算复杂度。</p>
<h3 id="model-2-factorised-encoder">Model 2: Factorised encoder</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314170832424.png"
alt="Model 2: Factorised encoder" />
<figcaption aria-hidden="true">Model 2: Factorised encoder</figcaption>
</figure>
<p>如图所示，该模型由两个独立的Transformer编码器组成。首先是空间编码器，仅以同一帧中提取的token为输入在<span
class="math inline">\(L_s\)</span>层后获得每帧的表示。<span
class="math inline">\(z_{cls}^{L_s}\)</span>是空间编码的cls-token，用于表达的空间特征。将每帧的特征cat以得到<span
class="math inline">\(\mathbf{H} \in \mathbb{R}^{n_t \times
d}\)</span>输入到<span
class="math inline">\(L_t\)</span>个Transformer组成的时间编码器，以建模来自不同帧的token之间的特征交互，最后该编码器的cls-token用于分类。相比于模型1，计算复杂度从<span
class="math inline">\(\mathcal{O}((n_t \cdot n_h \cdot
n_w)^2)\)</span>减低到<span class="math inline">\(\mathcal{O}({(n_h
\cdot n_w)^2 + n_t^2)}\)</span> 。</p>
<p>代码来自https://github.com/rishikksh20/ViViT-pytorch/blob/master/vivit.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = self.to_patch_embedding(x)</span><br><span class="line">    b, t, n, _ = x.shape</span><br><span class="line">    cls_space_tokens = repeat(self.space_token, <span class="string">&#x27;() n d -&gt; b t n d&#x27;</span>, b = b, t=t) <span class="comment"># 按维度扩展</span></span><br><span class="line">    x = torch.cat((cls_space_tokens, x), dim=<span class="number">2</span>)</span><br><span class="line">    x += self.pos_embedding[:, :, :(n + <span class="number">1</span>)]</span><br><span class="line">    x = self.dropout(x)</span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b t n d -&gt; (b t) n d&#x27;</span>)</span><br><span class="line">    x = self.space_transformer(x)</span><br><span class="line">    x = rearrange(x[:, <span class="number">0</span>], <span class="string">&#x27;(b t) ... -&gt; b t ...&#x27;</span>, b=b) <span class="comment">#提取每帧计算得到的cls-token</span></span><br><span class="line">    cls_temporal_tokens = repeat(self.temporal_token, <span class="string">&#x27;() n d -&gt; b n d&#x27;</span>, b=b)</span><br><span class="line">    x = torch.cat((cls_temporal_tokens, x), dim=<span class="number">1</span>)</span><br><span class="line">    x = self.temporal_transformer(x)        </span><br><span class="line">    x = x.mean(dim = <span class="number">1</span>) <span class="keyword">if</span> self.pool == <span class="string">&#x27;mean&#x27;</span> <span class="keyword">else</span> x[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> self.mlp_head(x)</span><br></pre></td></tr></table></figure>
<h3 id="model-3-factorised-self-attention">Model 3: Factorised
self-attention</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314203922539.png"
alt="Model 3: Factorised self-attention" />
<figcaption aria-hidden="true">Model 3: Factorised
self-attention</figcaption>
</figure>
<p>模型3与TimeSFormer的Divided
Space-Time基本一致，不同的是没有使用cls-token，以避免在空间和时间维度之间重新构造输入token时产生歧义，而且他们的方法验证出无论是先空间自注意力还是时间自注意力结果是一样的。</p>
<h3 id="model-4-factorised-dot-product-attention">Model 4: Factorised
dot-product attention</h3>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230314204755483.png"
alt="Model 4: Factorised dot-product attention" />
<figcaption aria-hidden="true">Model 4: Factorised dot-product
attention</figcaption>
</figure>
<p>模型4一个与模型2和模型3具有相同计算复杂度的模型，同时保留了与未分解的模型1相同的参数数量。具体而言，模型4采用了不同的注意力头分别在空间和时间维度上计算每个token的注意权重，自注意操作被定义为
<span class="math display">\[
Attention(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
Softmax\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt{d_k}} \right)
\mathbf{V}. \label{eq:selfattn}
\]</span> 在自注意力中，查询<span class="math inline">\(\mathbf{Q} =
\mathbf{X} \mathbf{W}_q\)</span>，键<span
class="math inline">\(\mathbf{K} = \mathbf{X}
\mathbf{W}_k\)</span>和值<span class="math inline">\(\mathbf{V}=
\mathbf{X} \mathbf{W}_v\)</span>是输入<span
class="math inline">\(\mathbf{X}\)</span>的线性投影，其中<span
class="math inline">\(\mathbf{X}, \mathbf{Q}, \mathbf{K}, \mathbf{V} \in
\mathbb{R}^{N \times d}\)</span>。
注意，在未分解的情况下（模型1），空间和时间维度合并为<span
class="math inline">\(N = n_t \cdot n_h \cdot n_w\)</span>。</p>
<p>这里的主要思想是通过构造<span class="math inline">\(\mathbf{K}_s,
\mathbf{V}_s \in \mathbb{R}^{n_h \cdot n_w \times d}\)</span>和<span
class="math inline">\(\mathbf{K}_t, \mathbf{V}_t \in \mathbb{R}^{n_t
\times
d}\)</span>，即与这些维度对应的键和值，修改每个查询的键和值，以仅关注来自相同空间和时间索引的令牌。然后，对于一半的注意力头，通过计算<span
class="math inline">\(\mathbf{Y}_s = Attention(\mathbf{Q}, \mathbf{K}_s,
\mathbf{V}_s)\)</span>来关注来自空间维度的token，对于其余部分，通过计算<span
class="math inline">\(\mathbf{Y}_t = Attention(\mathbf{Q}, \mathbf{K}_t,
\mathbf{V}_t)\)</span>来关注时间维度。</p>
<p>鉴于只是为每个查询更改注意力邻域，注意力操作与未分解情况下具有相同的维度，即<span
class="math inline">\(\mathbf{Y}_s, \mathbf{Y}_t \in\mathbb {R}^{N\times
d}\)</span>。 然后通过连接它们并使用线性投影来组合多个头的输出， $
=Concat（ _s, _t） _O $。</p>
<p>代码来自：https://github.com/noureldien/vivit_pytorch/blob/master/modules/vivit.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_space</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: (b, t, n, d)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    t = self.num_patches_time</span><br><span class="line">    n = self.num_patches_space</span><br><span class="line"></span><br><span class="line">    <span class="comment"># hide time dimension into batch dimension</span></span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b t n d -&gt; (b t) n d&#x27;</span>)  <span class="comment"># (bt, n, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply self-attention</span></span><br><span class="line">    out = self.forward_attention(x)  <span class="comment"># (bt, n, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># recover time dimension and merge it into space</span></span><br><span class="line">    out = rearrange(out, <span class="string">&#x27;(b t) n d -&gt; b (t n) d&#x27;</span>, t=t, n=n)  <span class="comment"># (b, tn, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward_time</span>(<span class="params">self, x</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x: (b, t, n, d)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    t = self.num_patches_time</span><br><span class="line">    n = self.num_patches_space</span><br><span class="line"></span><br><span class="line">    <span class="comment"># hide time dimension into batch dimension</span></span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)  <span class="comment"># (b, n, t, d)</span></span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b n t d -&gt; (b n) t d&#x27;</span>)  <span class="comment"># (bn, t, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># apply self-attention</span></span><br><span class="line">    out = self.forward_attention(x)  <span class="comment"># (bn, t, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># recover time dimension and merge it into space</span></span><br><span class="line">    out = rearrange(out, <span class="string">&#x27;(b n) t d -&gt; b (t n) d&#x27;</span>, t=t, n=n)  <span class="comment"># (b, tn, d)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">    t = self.num_patches_time</span><br><span class="line">    n = self.num_patches_space</span><br><span class="line"></span><br><span class="line">    <span class="comment"># reshape to reveal dimensions of space and time</span></span><br><span class="line">    x = rearrange(x, <span class="string">&#x27;b (t n) d -&gt; b t n d&#x27;</span>, t=t, n=n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.attn_type == <span class="string">&#x27;space&#x27;</span>:</span><br><span class="line">        out = self.forward_space(x) <span class="comment"># (b, tn, d)</span></span><br><span class="line">    <span class="keyword">elif</span> self.attn_type == <span class="string">&#x27;time&#x27;</span>:</span><br><span class="line">        out = self.forward_time(x) <span class="comment"># (b, tn, d)</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&#x27;Unknown attention type: %s&#x27;</span> % (self.attn_type))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="利用预训练模型进行初始化">利用预训练模型进行初始化</h2>
<p>由于Transformer缺乏CNN那样的归纳偏置，因此往往需要大规模的数据集作为训练集。为了规避这个问题，类似于3D-CNN的方法用ImageNet上预训练的2D-CNN网络（如Resnet等），使用了在大规模图片数据集上预训练的ViT迁移到ViViT的方法。</p>
<h3 id="positional-embeddings">Positional embeddings</h3>
<p>位置嵌入<span
class="math inline">\(\mathbf{p}\)</span>被添加到每个输入token。
但是，视频模型比预训练的图像模型多<span
class="math inline">\(n_t\)</span>倍的token。因此，通过将它们从<span
class="math inline">\(\mathbb{R}^{n_w \cdot n_h \times
d}\)</span>临时“重复”到<span class="math inline">\(\mathbb{R}^{n_t \cdot
n_h \cdot n_w \times d}\)</span>来初始化位置嵌入。
在初始化时，具有相同空间索引的所有token都具有相同的嵌入，然后进行微调。</p>
<h3 id="embedding-weights-e">Embedding weights, E</h3>
<p>当使用“tubelet embedding”token化方法时，<span
class="math inline">\(\mathbf{E}\)</span>是一个3D张量，与预训练模型中的2D张量<span
class="math inline">\(\mathbf{E}_{\text{image}}\)</span>相比。用于视频分类的从2D滤波器初始化3D卷积滤波器的常用方法是通过沿时间维度复制滤波器并对它们进行平均来“膨胀”它们，如I3D。
<span class="math display">\[
\mathbf{E} = \frac{1}{t}[\mathbf{E}_{\text{image}}, \ldots,
\mathbf{E}_{\text{image}}, \ldots, \mathbf{E}_{\text{image}}].
\]</span> 论文也提出了一种不一样的方式，称为“central frame
initialisation”：除了中间帧，<span
class="math inline">\(\mathbf{E}\)</span>的其他帧都使用0来初始化， <span
class="math display">\[
\mathbf{E} = [\mathbf{0}, \ldots,  \mathbf{E}_{\text{image}}, \ldots,
\mathbf{0}].
\]</span></p>
<h3 id="transformer-weights-for-model-3">Transformer weights for Model
3</h3>
<p>模型3的结构设计，是独立的空间attention和时序attention，空间attention可以直接使用图像模型的pretrain，时序attention初始化为0。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/02/19/Zhang_Temporal_Query_Networks_for_Fine-Grained_Video_Understanding_CVPR_2021_paper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yic-gdut">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yic">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | Yic">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/02/19/Zhang_Temporal_Query_Networks_for_Fine-Grained_Video_Understanding_CVPR_2021_paper/" class="post-title-link" itemprop="url">Temporal Query Networks for Fine-grained Video Understanding</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-02-19 00:00:00" itemprop="dateCreated datePublished" datetime="2023-02-19T00:00:00+08:00">2023-02-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-06-28 09:26:48" itemprop="dateModified" datetime="2023-06-28T09:26:48+08:00">2023-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3/" itemprop="url" rel="index"><span itemprop="name">视频理解</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>CVPR 2021</p>
<h1 id="摘要">摘要</h1>
<p>本文的目标是对未裁剪视频中的动作进行细粒度分类，其中动作可以在时间上扩展，也可以只跨越视频的几帧。将其转换为查询-响应机制，其中每个查询处理特定的问题，并拥有自己的响应标签集。</p>
<p>贡献：</p>
<ol type="1">
<li>提出了一个新的模型—时态查询网络（TQN)—它支持查询-响应功能，以及对细粒度操作的结构理解</li>
<li>提出了一种新的方法-随机特征库更新-在不同长度的视频上训练网络，并使用响应细粒度查询所需的密集采样</li>
<li>将TQN与其他体系结构和文本监督方法进行比较，分析其优缺点</li>
<li>在FineGym和Diving48基准上广泛评估细粒度动作分类的方法并仅使用RGB特征超越最先进的方法</li>
</ol>
<h1 id="temporal-query-networks">Temporal Query Networks</h1>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230218195704209.png"
alt="Temporal Query Network" />
<figcaption aria-hidden="true">Temporal Query Network</figcaption>
</figure>
<p>时间查询网络(TQN)在未修剪的视频中快速识别发生的区分性事件(只跨越几帧)，并且可以在只有弱监督的情况下进行训练，即没有事件的时间位置或持续时间信息。它通过学习一组置换不变的查询向量来实现这一点，这些查询向量对应于关于事件及其属性的预定义查询，使用Transformer[56]解码器层将其转换为响应向量，并从3D卷积网络主干中提取视觉特征。图2给出了模型的概述。视觉骨干和TQN解码器描述如下。</p>
<h2 id="queryattributes-查询属性">Query–Attributes 查询属性</h2>
<p>查询集query set： <span class="math display">\[
\mathcal{Q}=\left\{q_{i}\right\}_{i=1}^{K}
\]</span> 其中每一个查询<span
class="math inline">\(q_i\)</span>都有一个相关的属性集： <span
class="math display">\[
\mathcal{A}_{i}=\left\{a_{1}^{i}, a_{2}^{i}, \ldots, a_{n_{i}-1}^{i},
\varnothing\right\}
\]</span> 由<span
class="math inline">\(q_i\)</span>的响应的可接受值<span
class="math inline">\(a_j^i\)</span>组成</p>
<p>例如，在跳水视频中，查询可以是圈数，属性集为可能的计数{0.5,1.0,2.5};或者在体操中，查询可以是项目类型，属性集为{vault,
floor-exercise, balanced beam}</p>
<h2 id="视觉-backbone">视觉 backbone</h2>
<p>给定一个未修剪的视频，使用3D
ConvNet提取8帧连续不重叠剪辑的第一个视觉特征: <span
class="math display">\[
\boldsymbol{\Phi}=\left(\Phi_{1}, \Phi_{2}, \ldots, \Phi_{t}\right)
\]</span> 其中<span
class="math inline">\(t\)</span>是剪辑片段的总数，<span
class="math inline">\(\Phi_{i} \in
\mathbb{R}^{d}\)</span>是d维剪辑级视觉特征。</p>
<p>在整个视频中密集地提取特征有两个原因：</p>
<ol type="1">
<li>避免了引起时间混叠，也避免了丢失快速事件(只跨越几帧)</li>
<li>从完整视频中选择片段进行分类是次优的，因为这些事件的位置是未知的</li>
</ol>
<h2 id="tqn-decoder">TQN decoder</h2>
<p>给定剪辑级特征和标签查询，TQN解码器为每个查询输出一个响应。具体而言，对于每一个标签查询<span
class="math inline">\(q_i\)</span>，学习一个矢量<span
class="math inline">\(\mathbf{q}_{i} \in
\mathbb{R}^{d_{q}}\)</span>，通过对视觉特征的关注产生一个响应矢量<span
class="math inline">\(\mathbf{r}_{i} \in
\mathbb{R}^{d_{q}}\)</span>。然后将每个响应矢量<span
class="math inline">\(\mathbf{r}_{i}\)</span>独立线性分类到相应的属性集<span
class="math inline">\(\mathcal{A}_{i}\)</span>中。</p>
<h2 id="训练">训练</h2>
<p>通过反向传播将来自视觉编码器和TQN解码器的模型参数与属性分类器<span
class="math inline">\(\Psi_{i}\)</span>进行端到端的联合训练。</p>
<p>这个训练的loss是一个单个分类器损失的多任务组合，是属性集<span
class="math inline">\(\mathcal{A}_{i}\)</span>上对数<span
class="math inline">\(\Psi_{i} \cdot
\mathbf{r}_{i}^{(M)}\)</span>上的Softmax交叉熵损失<span
class="math inline">\(\mathcal{L}_{C E}\)</span> <span
class="math display">\[
\mathcal{L}_{\text {total }}=\sum_{i=1}^{K} \mathcal{L}_{C
E}^{(i)}\left(a^{i}, \Psi_{i} \cdot \mathbf{r}_{i}^{(M)}\right)
\]</span> 其中<span class="math inline">\(a_i\)</span>是标签查询<span
class="math inline">\(q_i\)</span>的groud-truth属性。</p>
<p>本质上，TQN解码器学习建立查询向量和相关视觉特征之间的时间对应关系以生成响应。由于查询向量本身是学习的，它们被优化为“专家”，可以在未修剪的时间特征流中定位相应的事件。</p>
<h2 id="discussion-tqn-and-detr">Discussion: TQN and DETR</h2>
<p>DETR[4]是最近提出的一种基于Transformer的目标检测模型，同样采用非自回归并行解码一次性输出目标检测。然而，有三个关键的区别：</p>
<ol type="1">
<li>DETR对象查询都是等价的-因为它们的输出都指定了相同的“标签空间”(对象类和它们的RoI)，本质上查询是学习位置编码。相比之下，TQN查询具有不同的语义，具有对应事件类型和属性的语义;它们的输出响应向量每个指定一组不同的属性，属性的数量依赖于查询。</li>
<li>由于TQN响应与这些查询绑定，它们可以在直接监督属性标签的情况下进行训练，从而避免了DETR中使用的预测和真实值之间的训练时间
Hungarian Matching[33]。</li>
<li>TQN没有时间上的定位监督，而DETR训练有提供(空间)位置。因此，尽管TQN的任务是(隐式)检测事件，但它是在更弱的监督下完成的。</li>
</ol>
<h1
id="随机更新特征库-stochastically-updated-feature-bank">随机更新特征库
Stochastically Updated Feature Bank</h1>
<p>对整个未修剪的视频输入帧进行密集的时间采样是检测时间位置未知的快速判别事件的关键。但是问题在于GPU显存的限制，无法在每次训练迭代中转发密集采样的帧。作者使用特征内存库来克服这些限制。</p>
<p>记忆库缓存clip级别的3D卷积网络视觉特征。对于给定的视频，clip特征<span
class="math inline">\(\boldsymbol{\Phi}=\left(\Phi_{1}, \Phi_{2},
\ldots,
\Phi_{t}\right)\)</span>可以被相互独立提取。缓存库是由预训练的3D卷积网络中提取的所有训练视频的片段特征初始化的。然后在每次训练迭代中，固定数量的<span
class="math inline">\(n_{online}\)</span>个随机采样clip通过视觉编码器计算得到，而剩下的<span
class="math inline">\((r-n_{online})\)</span>个clip特征则从缓存库中获得。然后将两组视觉特征组合并输入TQN解码器进行最终预测和反向传播以更新模型参数。最后，将在线计算得到的记忆库中的clip特征替换为在线特征。在推理过程中，所有的特征都是在没有缓存库的情况下在线计算的。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230219134722818.png"
alt="image-20230219134722818" />
<figcaption aria-hidden="true">image-20230219134722818</figcaption>
</figure>
<h1 id="将类别分解为属性查询">将类别分解为属性查询</h1>
<p>在本节中，作者演示了通常与细粒度视频识别数据集相关的预定义的N个类别<span
class="math inline">\(\mathcal{C}=\left\{c_{1}, c_{2}, \ldots,
c_{N}\right\}\)</span>集合如何被分解为属性查询。在这些数据集中，类别在微妙的细节上有所不同，例如特定类型、持续时间或特定事件序列的数量。这些事件可能是快速发生(持续时间短)，但时间位置和持续时间未知。</p>
<figure>
<img
src="https://yic-123.oss-cn-guangzhou.aliyuncs.com/img/image-20230219150729217.png"
alt="image-20230219150729217" />
<figcaption aria-hidden="true">image-20230219150729217</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yic-gdut</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
